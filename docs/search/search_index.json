{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started \u00b6 Introduction \u00b6 Sections \u00b6","title":"Getting started"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#introduction","text":"","title":"Introduction"},{"location":"#sections","text":"","title":"Sections"},{"location":"sample/","text":"Specimen \u00b6 Body copy \u00b6 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras arcu libero, mollis sed massa vel, ornare viverra ex . Mauris a ullamcorper lacus. Nullam urna elit, malesuada eget finibus ut, ullamcorper ac tortor. Vestibulum sodales pulvinar nisl, pharetra aliquet est. Quisque volutpat erat ac nisi accumsan tempor. Sed suscipit , orci non pretium pretium, quam mi gravida metus, vel venenatis justo est condimentum diam. Maecenas non ornare justo. Nam a ipsum eros. Nulla aliquam orci sit amet nisl posuere malesuada. Proin aliquet nulla velit, quis ultricies orci feugiat et. Ut tincidunt sollicitudin tincidunt. Aenean ullamcorper sit amet nulla at interdum. Headings \u00b6 The 3rd level \u00b6 The 4th level \u00b6 The 5th level \u00b6 The 6th level \u00b6 Headings with secondary text \u00b6 The 3rd level with secondary text \u00b6 The 4th level with secondary text \u00b6 The 5th level with secondary text \u00b6 The 6th level with secondary text \u00b6 Blockquotes \u00b6 Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Sed molestie imperdiet consectetur. Blockquote nesting \u00b6 Sed aliquet , neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem libero fermentum urna, ut efficitur elit ligula et nunc. Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. Suspendisse rutrum facilisis risus , eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa. Other content blocks \u00b6 Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. js hl_lines=\"8\" var _extends = function(target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { target[key] = source[key]; } } return target; }; Praesent at :::js return target , sodales nibh vel, tempor felis. Fusce vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices. Donec consectetur mauris non neque imperdiet, eget volutpat libero. Lists \u00b6 Unordered lists \u00b6 Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris. Nulla et rhoncus turpis. Mauris ultricies elementum leo. Duis efficitur accumsan nibh eu mattis. Vivamus tempus velit eros, porttitor placerat nibh lacinia sed. Aenean in finibus diam. Ordered lists \u00b6 Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Aliquam ornare feugiat quam et egestas. Nunc id erat et quam pellentesque lacinia eu vel odio. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Mauris dictum mi lacus Ut sit amet placerat ante Suspendisse ac eros arcu Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Sed aliquet, neque at rutrum mollis, neque nisi tincidunt nibh. Pellentesque eget :::js var _extends ornare tellus, ut gravida mi. js hl_lines=\"1\" var _extends = function(target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { target[key] = source[key]; } } return target; }; Vivamus id mi enim. Integer id turpis sapien. Ut condimentum lobortis sagittis. Aliquam purus tellus, faucibus eget urna at, iaculis venenatis nulla. Vivamus a pharetra leo. Definition lists \u00b6 Lorem ipsum dolor sit amet : Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla. 1 2 3 Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Cras arcu libero : Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris. Code blocks \u00b6 Inline \u00b6 Morbi eget dapibus felis . Vivamus venenatis porttitor tortor sit amet rutrum. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Pellentesque aliquet quam enim , eu volutpat urna rutrum a. Nam vehicula nunc :::js return target mauris, a ultricies libero efficitur sed. Sed molestie imperdiet consectetur. Vivamus a pharetra leo. Pellentesque eget ornare tellus, ut gravida mi. Fusce vel lacinia lacus. Listing \u00b6 1 2 3 4 5 6 7 8 9 var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; }; Horizontal rules \u00b6 Aenean in finibus diam. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales. Data tables \u00b6 Sollicitudo / Pellentesi consectetur adipiscing elit arcu sed Vivamus a pharetra yes yes yes yes yes Ornare viverra ex yes yes yes yes yes Mauris a ullamcorper yes yes partial yes yes Nullam urna elit yes yes yes yes yes Malesuada eget finibus yes yes yes yes yes Ullamcorper yes yes yes yes yes Vestibulum sodales yes - yes - yes Pulvinar nisl yes yes yes - - Pharetra aliquet est yes yes yes yes yes Sed suscipit yes yes yes yes yes Orci non pretium yes partial - - - Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Left Center Right Lorem dolor amet ipsum sit Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. Table with colgroups (Pandoc) Lorem ipsum dolor sit amet. Sed sagittis eleifend rutrum. Donec vitae suscipit est.","title":"Sample"},{"location":"sample/#specimen","text":"","title":"Specimen"},{"location":"sample/#body-copy","text":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras arcu libero, mollis sed massa vel, ornare viverra ex . Mauris a ullamcorper lacus. Nullam urna elit, malesuada eget finibus ut, ullamcorper ac tortor. Vestibulum sodales pulvinar nisl, pharetra aliquet est. Quisque volutpat erat ac nisi accumsan tempor. Sed suscipit , orci non pretium pretium, quam mi gravida metus, vel venenatis justo est condimentum diam. Maecenas non ornare justo. Nam a ipsum eros. Nulla aliquam orci sit amet nisl posuere malesuada. Proin aliquet nulla velit, quis ultricies orci feugiat et. Ut tincidunt sollicitudin tincidunt. Aenean ullamcorper sit amet nulla at interdum.","title":"Body copy"},{"location":"sample/#headings","text":"","title":"Headings"},{"location":"sample/#the-3rd-level","text":"","title":"The 3rd level"},{"location":"sample/#the-4th-level","text":"","title":"The 4th level"},{"location":"sample/#the-5th-level","text":"","title":"The 5th level"},{"location":"sample/#the-6th-level","text":"","title":"The 6th level"},{"location":"sample/#headings-with-secondary-text","text":"","title":"Headings with secondary text"},{"location":"sample/#the-3rd-level-with-secondary-text","text":"","title":"The 3rd level with secondary text"},{"location":"sample/#the-4th-level-with-secondary-text","text":"","title":"The 4th level with secondary text"},{"location":"sample/#the-5th-level-with-secondary-text","text":"","title":"The 5th level with secondary text"},{"location":"sample/#the-6th-level-with-secondary-text","text":"","title":"The 6th level with secondary text"},{"location":"sample/#blockquotes","text":"Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Sed molestie imperdiet consectetur.","title":"Blockquotes"},{"location":"sample/#blockquote-nesting","text":"Sed aliquet , neque at rutrum mollis, neque nisi tincidunt nibh, vitae faucibus lacus nunc at lacus. Nunc scelerisque, quam id cursus sodales, lorem libero fermentum urna, ut efficitur elit ligula et nunc. Mauris dictum mi lacus, sit amet pellentesque urna vehicula fringilla. Ut sit amet placerat ante. Proin sed elementum nulla. Nunc vitae sem odio. Suspendisse ac eros arcu. Vivamus orci erat, volutpat a tempor et, rutrum. eu odio. Suspendisse rutrum facilisis risus , eu posuere neque commodo a. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed nec leo bibendum, sodales mauris ut, tincidunt massa.","title":"Blockquote nesting"},{"location":"sample/#other-content-blocks","text":"Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. js hl_lines=\"8\" var _extends = function(target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { target[key] = source[key]; } } return target; }; Praesent at :::js return target , sodales nibh vel, tempor felis. Fusce vel lacinia lacus. Suspendisse rhoncus nunc non nisi iaculis ultrices. Donec consectetur mauris non neque imperdiet, eget volutpat libero.","title":"Other content blocks"},{"location":"sample/#lists","text":"","title":"Lists"},{"location":"sample/#unordered-lists","text":"Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris. Nulla et rhoncus turpis. Mauris ultricies elementum leo. Duis efficitur accumsan nibh eu mattis. Vivamus tempus velit eros, porttitor placerat nibh lacinia sed. Aenean in finibus diam.","title":"Unordered lists"},{"location":"sample/#ordered-lists","text":"Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Aliquam ornare feugiat quam et egestas. Nunc id erat et quam pellentesque lacinia eu vel odio. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Nam vehicula nunc mauris, a ultricies libero efficitur sed. Mauris dictum mi lacus Ut sit amet placerat ante Suspendisse ac eros arcu Morbi eget dapibus felis. Vivamus venenatis porttitor tortor sit amet rutrum. Pellentesque aliquet quam enim, eu volutpat urna rutrum a. Sed aliquet, neque at rutrum mollis, neque nisi tincidunt nibh. Pellentesque eget :::js var _extends ornare tellus, ut gravida mi. js hl_lines=\"1\" var _extends = function(target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { target[key] = source[key]; } } return target; }; Vivamus id mi enim. Integer id turpis sapien. Ut condimentum lobortis sagittis. Aliquam purus tellus, faucibus eget urna at, iaculis venenatis nulla. Vivamus a pharetra leo.","title":"Ordered lists"},{"location":"sample/#definition-lists","text":"Lorem ipsum dolor sit amet : Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla. 1 2 3 Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Cras arcu libero : Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante, fringilla at aliquam ac, aliquet sed mauris.","title":"Definition lists"},{"location":"sample/#code-blocks","text":"","title":"Code blocks"},{"location":"sample/#inline","text":"Morbi eget dapibus felis . Vivamus venenatis porttitor tortor sit amet rutrum. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Pellentesque aliquet quam enim , eu volutpat urna rutrum a. Nam vehicula nunc :::js return target mauris, a ultricies libero efficitur sed. Sed molestie imperdiet consectetur. Vivamus a pharetra leo. Pellentesque eget ornare tellus, ut gravida mi. Fusce vel lacinia lacus.","title":"Inline"},{"location":"sample/#listing","text":"1 2 3 4 5 6 7 8 9 var _extends = function ( target ) { for ( var i = 1 ; i < arguments . length ; i ++ ) { var source = arguments [ i ]; for ( var key in source ) { target [ key ] = source [ key ]; } } return target ; };","title":"Listing"},{"location":"sample/#horizontal-rules","text":"Aenean in finibus diam. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Integer vehicula feugiat magna, a mollis tellus. Nam mollis ex ante, quis elementum eros tempor rutrum. Aenean efficitur lobortis lacinia. Nulla consectetur feugiat sodales.","title":"Horizontal rules"},{"location":"sample/#data-tables","text":"Sollicitudo / Pellentesi consectetur adipiscing elit arcu sed Vivamus a pharetra yes yes yes yes yes Ornare viverra ex yes yes yes yes yes Mauris a ullamcorper yes yes partial yes yes Nullam urna elit yes yes yes yes yes Malesuada eget finibus yes yes yes yes yes Ullamcorper yes yes yes yes yes Vestibulum sodales yes - yes - yes Pulvinar nisl yes yes yes - - Pharetra aliquet est yes yes yes yes yes Sed suscipit yes yes yes yes yes Orci non pretium yes partial - - - Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Nulla tempor lobortis orci, at elementum urna sodales vitae. In in vehicula nulla, quis ornare libero. Left Center Right Lorem dolor amet ipsum sit Vestibulum vitae orci quis ante viverra ultricies ut eget turpis. Sed eu lectus dapibus, eleifend nulla varius, lobortis turpis. In ac hendrerit nisl, sit amet laoreet nibh. Table with colgroups (Pandoc) Lorem ipsum dolor sit amet. Sed sagittis eleifend rutrum. Donec vitae suscipit est.","title":"Data tables"},{"location":"section-0-brief-python-refresher/","text":"A (very) brief Python refresher \u00b6 In this section we will take a quick tour of Python basics and some of the concepts that will be used for the rest of our web scraping workshop. Strings \u00b6 A string can be defined by enclosing it in a single quote(') or a double quote(\") 1 my_str = \"This is a string\" The characters of a string can be accessed by indices and the indices go from 0 to n-1 1 my_str [ 0 ] # [i] where i is the index of element we want access 1 'T' Slice notation [a:b:c] means \"count in increments of c starting at a inclusive, up to b exclusive\". 1 my_str [ 0 : 4 ] 1 'This' A string can be reversed using the following way. The first index corresponds to the start, second to the end and the last one indicates the increment that needs to be done. 1 my_str [:: - 1 ] 1 'gnirts a si sihT' A string can be splitted as well based on a delimmitter. A list is returned after splitting 1 2 my_str = \"one,two,three,four,five\" my_str . split ( ',' ) 1 ['one', 'two', 'three', 'four', 'five'] A string can be stripped as well of extra spaces at the ends. 1 2 3 my_str = \" hello \" print ( my_str ) my_str . strip () 1 2 3 4 5 6 7 hello 'hello' Lists \u00b6 Lists are one of the most useful data structure in Python. They are comparable to arrays from other programming languages such as Java and JavaScript. 1 2 # List can be of a mixed type my_list = [ 'item1' , 'item2' , 100 , 3.14 ] 1 2 # List elements can be accessed by the indices starting from 0 to n-1 my_list [ 2 ] 1 100 1 2 # Function to find the length of the list len ( my_list ) 1 4 1 2 3 # range function to generate a range object num_list = range ( 0 , 10 ) num_list 1 range(0, 10) 1 2 # use list() to get a list out of the range object list ( range ( 0 , 10 )) 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 1 2 3 # Iterate over a list using for loop for num in num_list : print ( num ) 1 2 3 4 5 6 7 8 9 10 0 1 2 3 4 5 6 7 8 9 List comprehension \u00b6 new_list = [expression(item) for item in old_list] 1 2 num_squares = [ num * num for num in num_list ] num_squares 1 [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 1 2 3 # The list can be filtered based on a condition num_evens = [ num for num in num_list if num % 2 == 0 ] num_evens 1 [0, 2, 4, 6, 8] 1 2 3 4 5 6 7 # zip() function to combine 2 lists country_list = [ \"Australia\" , \"France\" , \"USA\" , \"Italy\" ] capital_list = [ \"Canberra\" , \"Paris\" , \"Washington DC\" , \"Rome\" ] pairs = zip ( country_list , capital_list ) for country , capital in pairs : print ( \"The country is {0} and the capital is {1} \" . format ( country , capital )) 1 2 3 4 The country is Australia and the capital is Canberra The country is France and the capital is Paris The country is USA and the capital is Washington DC The country is Italy and the capital is Rome 1 2 3 4 5 # sort() function to sort a list. It stores the sorted list in the original list itself. my_list = [ 954 , 341 , 100 , 3.14 ] my_list . sort () my_list 1 [3.14, 100, 341, 954] Loops \u00b6 1 2 # Loops can be run over list of lists as well languages = [[ 'Spanish' , 'English' , 'French' , 'German' ], [ 'Python' , 'Java' , 'Javascript' , 'C++' ]] 1 2 for lang in languages : print ( lang ) 1 2 ['Spanish', 'English', 'French', 'German'] ['Python', 'Java', 'Javascript', 'C++'] As we see above, in each iteration, we get one list at a time. If each element of the nested list is needed, then a nested loop should be written as below: 1 2 3 4 for lang_list in languages : print ( \"--------------\" ) for lang in lang_list : print ( lang ) 1 2 3 4 5 6 7 8 9 10 -------------- Spanish English French German -------------- Python Java Javascript C++ There are various ways to manipulate the functioning of a loop. continue: This will skip the rest of the statements of that iteration and continue with the next iteration. break: This will break the entire loop and go to the next statement after the loop. 1 2 3 4 5 6 7 for lang_list in languages : print ( \"--------------\" ) for lang in lang_list : if lang == \"German\" : continue print ( lang ) print ( \"End of loops\" ) 1 2 3 4 5 6 7 8 9 10 -------------- Spanish English French -------------- Python Java Javascript C++ End of loops 1 2 3 4 5 6 7 for lang_list in languages : print ( \"--------------\" ) for lang in lang_list : if lang == \"Java\" : break print ( lang ) print ( \"End of loops\" ) 1 2 3 4 5 6 7 8 -------------- Spanish English French German -------------- Python End of loops 1 2 3 4 5 6 7 8 9 10 # another example of continue from math import sqrt number = 0 for i in range ( 10 ): number = i ** 2 if i % 2 == 0 : continue # continue here print ( str ( round ( sqrt ( number ))) + ' squared is equal to ' + str ( number )) 1 2 3 4 5 1 squared is equal to 1 3 squared is equal to 9 5 squared is equal to 25 7 squared is equal to 49 9 squared is equal to 81 Sets \u00b6 Sets is an unordered collections of unique elements. Common uses include membership testing, removing duplicates from a sequence, and computing standard math operations on sets such as intersection, union, difference, and symmetric difference. A set can be created using the '{}' brackets. 1 2 my_set = { 1 , 2 , 3 } print ( my_set ) 1 {1, 2, 3} 1 2 my_list = [ 1 , 2 , 3 , 4 , 2 , 3 ] print ( set ( my_list )) 1 {1, 2, 3, 4} 1 2 3 # A set can be made of mixed types as well. my_set = { 1.0 , \"Hello\" , ( 1 , 2 , 3 )} print ( my_set ) 1 {1.0, 'Hello', (1, 2, 3)} 1 2 3 # Even if duplicate elements are added while initialising, they get remived. my_set = { 1 , 2 , 3 , 4 , 3 , 2 } print ( my_set ) 1 {1, 2, 3, 4} 1 2 3 4 5 #Creating an empty set is a bit tricky. my_set = {} print ( type ( my_set )) my_set = set () print ( type ( my_set )) 1 2 <class 'dict'> <class 'set'> Elements can be added individualy or as a list. 1 2 3 4 5 my_set = { 1 , 2 , 3 } my_set . add ( 4 ) print ( my_set ) my_set . update ([ 6 , 7 , 8 ]) my_set 1 2 3 4 5 6 7 {1, 2, 3, 4} {1, 2, 3, 4, 6, 7, 8} 1 2 3 4 # Union A = { 1 , 2 , 3 , 4 , 5 } B = { 4 , 5 , 6 , 7 , 8 } A | B # or A.union(B) 1 {1, 2, 3, 4, 5, 6, 7, 8} 1 2 3 4 # Intersection A = { 1 , 2 , 3 , 4 , 5 } B = { 4 , 5 , 6 , 7 , 8 } A & B # A.intersection(B) 1 {4, 5} Dictionary \u00b6 Dictionaries are a container that store key-value pairs. They are unordered. Other programming languages might call this a 'hash', 'hashtable' or 'hashmap'. 1 2 dict1 = { 'a' : 1 , 'b' : 2 , 'c' : 3 , 'd' : 4 } dict1 1 {'a': 1, 'b': 2, 'c': 3, 'd': 4} 1 2 3 # Adding a key to the dictionary dict1 [ 'e' ] = 5 dict1 1 {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5} keys() method returns the keys in the dictionary. 1 dict1 . keys () 1 dict_keys(['a', 'b', 'c', 'd', 'e']) 1 2 # items() method can be used to get all the pairs of the dictionary. dict1 . items () 1 dict_items([('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', 5)]) Dictionary comprehension can be used to manipulate the elements of a dictionary. 1 2 double_dict1 = { k : v * 2 for ( k , v ) in dict1 . items ()} double_dict1 1 {'a': 2, 'b': 4, 'c': 6, 'd': 8, 'e': 10} 1 2 dict1_cond = { k : v for ( k , v ) in dict1 . items () if v > 2 } dict1_cond 1 {'c': 3, 'd': 4, 'e': 5} 1 Functions \u00b6 A function is a block of organized, reusable code that is used to perform a single, related action. Functions provide better modularity for your application and a high degree of code reusing. 1 2 3 4 # Function definition def hello (): print ( \"Hello World\" ) return 1 2 # Function calling hello () 1 Hello World Parameters vs arguments: Parameters are a and b. Arguments are 2 and 5. 1 2 3 def plus ( a , b ): return a + b plus ( 2 , 5 ) 1 7 A function can return nothing (null/None) as well. 1 2 3 4 5 def run (): for x in range ( 10 ): if x == 2 : return print ( \"Run!\" ) 1 run () Keyword arguments with default values \u00b6 1 2 3 # Here the value of parameter `b` is 2 by default if the value is not passed. def plus ( a , b = 2 ): return a + b 1 2 # Call `plus()` with only `a` parameter print ( plus ( a = 1 )) 1 3 1 2 # Call `plus()` with `a` and `b` parameters print ( plus ( a = 1 , b = 3 )) 1 4 Anonymous functions: lambda \u00b6 1 2 3 4 5 6 7 8 9 # `sum()` lambda function sum = lambda x , y : x + y ; # Call the `sum()` anonymous function sum ( 4 , 5 ) # \"Translate\" to a UDF # def sum(x, y): # return x+y 1 9 Use of main() \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 def hello (): print ( \"Hello World\" ) return # Define `main()` function def main (): hello () print ( \"This is a main function\" ) main () # As is, if the script is imported, it will execute the main function. 1 2 Hello World This is a main function The following code needs a script mode to show the use. 1 2 3 4 5 6 7 8 # Define `main()` function def main (): hello () print ( \"This is a main function\" ) # Execute `main()` function if __name__ == '__main__' : main () 1 2 Hello World This is a main function Global vs local variables \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Global variable `init` init = 1 # Define `plus()` function to accept a variable number of arguments def plus ( * args ): # Local variable `sum()` total = 0 for i in args : total += i return total # Access the global variable print ( \"this is the initialized value \" + str ( init )) # (Try to) access the local variable print ( \"this is the sum \" + str ( total )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 this is the initialized value 1 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-67-136638963fdb> in <module> 14 15 # (Try to) access the local variable ---> 16 print(\"this is the sum \" + str(total)) NameError: name 'total' is not defined JSON \u00b6 JSON: JavaScript Object Notation. When exchanging data between a browser and a server, the data can only be text. Python has a built-in package called json, which can be used to work with JSON data. 1 import json 1 2 3 4 5 6 7 8 9 10 # Convert from JSON to Python: # some JSON: x = '{ \"name\":\"John\", \"age\":30, \"city\":\"New York\"}' # parse x: y = json . loads ( x ) # the result is a Python dictionary: print ( y [ \"age\" ]) 1 30 1 y 1 {'name': 'John', 'age': 30, 'city': 'New York'} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Convert from Python to JSON # a Python object (dict): x = { \"name\" : \"John\" , \"age\" : 30 , \"city\" : \"New York\" } # convert into JSON: y = json . dumps ( x ) # the result is a JSON string: print ( y ) 1 {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"} 1 2 3 # Indentation y = json . dumps ( x , indent = 4 ) print ( y ) 1 2 3 4 5 { \"name\": \"John\", \"age\": 30, \"city\": \"New York\" } Dataframes \u00b6 One of the most powerful data structures in Python is the Pandas DataFrame . It allows tabular data, including csv (comma seperated values) and tsv (tab seperated values), to be processed and manipulated. People familiar with Excel will no doubt find it intuitive and easy to grasp. Since most csv (or tsv ) has become the de facto standard for sharing datasets both large and small, Pandas dataframe is the way to go. For Pandas documentation : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html 1 2 import pandas as pd # importing the package and using `pd` as the alias print ( 'Pandas version : {} ' . format ( pd . __version__ )) 1 Pandas version : 1.0.1 Suppose we wanted to create a dataframe as follows, name title sam physicist rob economist Let's create a dictionary with the headers as keys and their corresponding values as a list as follows, 1 data = { 'name' : [ 'sam' , 'rob' ], 'title' : [ 'physicist' , 'economist' ]} Converting the same to a dataframe, 1 2 df = pd . DataFrame ( data ) print ( df . to_markdown ()) # converting to markdown for ease of display name title 0 sam physicist 1 rob economist We can also take a quick glance at its contents by using : - df.head() : To display the first 5 rows - df.tail() : To display the last 5 rows In order to read external files we use read_csv() function, 1 pd . read_csv ( filename , sep = ',' ) Similarly, for exporting a Pandas dataframe to a csv file, we can use to_csv() as follows 1 df . to_csv ( index = False ) Regex \u00b6 Regular expressions or regex are a powerful tool to extract key pieces of data from raw text. You can try your regex expressions in : - https://pythex.org/ for a python oriented regex editor - https://regexr.com/ for a more visual explanation behind the expressions (good for getting started) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 html = r ''' <!DOCTYPE html> <html> <head> <title>site</title> </head> <body> <h1>Sam</h1> <h2>Physicist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod.</p> <h1>Rob</h1> <h2>Economist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et.</p> </body> </html> ''' print ( html ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 <!DOCTYPE html> <html> <head> <title>site</title> </head> <body> <h1>Sam</h1> <h2>Physicist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod.</p> <h1>Rob</h1> <h2>Economist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et.</p> </body> </html> Now, if we are only interested in : - names i.e. the data inside the <h1></h1> tags, and - title i.e. the data inside the <h2></h2> tags we can extract the same using regex. First lets import the regex module in python called re 1 import re # regex package in python is named 're' Now lets define the expressions (or patterns) to capture all text between the tags as follows : <h1>(.*?)</h1> : capture all text contained within <h1></h1> tags <h2>(.*?)</h2> : capture all text contained within <h2></h2> tags 1 2 regex_h1 = re . compile ( '<h1>(.*?)</h1>' ) regex_h2 = re . compile ( '<h2>(.*?)</h2>' ) and use findall() to return all the instances that match with our pattern, 1 2 3 4 names = regex_h1 . findall ( html ) titles = regex_h2 . findall ( html ) print ( names , titles ) 1 ['Sam', 'Rob'] ['Physicist', 'Economist'] From a web scraping perspective : \u00b6 JSON and XML are the most widely used formats to carry data all over the internet. To work with CSV s (or TSV s), Pandas DataFrames are the de facto standard. Regexes help us extract key pieces of information (sub-strings) from raw, messy and unstructured text (strings).","title":"A (brief) Python refresher"},{"location":"section-0-brief-python-refresher/#a-very-brief-python-refresher","text":"In this section we will take a quick tour of Python basics and some of the concepts that will be used for the rest of our web scraping workshop.","title":"A (very) brief Python refresher"},{"location":"section-0-brief-python-refresher/#strings","text":"A string can be defined by enclosing it in a single quote(') or a double quote(\") 1 my_str = \"This is a string\" The characters of a string can be accessed by indices and the indices go from 0 to n-1 1 my_str [ 0 ] # [i] where i is the index of element we want access 1 'T' Slice notation [a:b:c] means \"count in increments of c starting at a inclusive, up to b exclusive\". 1 my_str [ 0 : 4 ] 1 'This' A string can be reversed using the following way. The first index corresponds to the start, second to the end and the last one indicates the increment that needs to be done. 1 my_str [:: - 1 ] 1 'gnirts a si sihT' A string can be splitted as well based on a delimmitter. A list is returned after splitting 1 2 my_str = \"one,two,three,four,five\" my_str . split ( ',' ) 1 ['one', 'two', 'three', 'four', 'five'] A string can be stripped as well of extra spaces at the ends. 1 2 3 my_str = \" hello \" print ( my_str ) my_str . strip () 1 2 3 4 5 6 7 hello 'hello'","title":"Strings"},{"location":"section-0-brief-python-refresher/#lists","text":"Lists are one of the most useful data structure in Python. They are comparable to arrays from other programming languages such as Java and JavaScript. 1 2 # List can be of a mixed type my_list = [ 'item1' , 'item2' , 100 , 3.14 ] 1 2 # List elements can be accessed by the indices starting from 0 to n-1 my_list [ 2 ] 1 100 1 2 # Function to find the length of the list len ( my_list ) 1 4 1 2 3 # range function to generate a range object num_list = range ( 0 , 10 ) num_list 1 range(0, 10) 1 2 # use list() to get a list out of the range object list ( range ( 0 , 10 )) 1 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 1 2 3 # Iterate over a list using for loop for num in num_list : print ( num ) 1 2 3 4 5 6 7 8 9 10 0 1 2 3 4 5 6 7 8 9","title":"Lists"},{"location":"section-0-brief-python-refresher/#list-comprehension","text":"new_list = [expression(item) for item in old_list] 1 2 num_squares = [ num * num for num in num_list ] num_squares 1 [0, 1, 4, 9, 16, 25, 36, 49, 64, 81] 1 2 3 # The list can be filtered based on a condition num_evens = [ num for num in num_list if num % 2 == 0 ] num_evens 1 [0, 2, 4, 6, 8] 1 2 3 4 5 6 7 # zip() function to combine 2 lists country_list = [ \"Australia\" , \"France\" , \"USA\" , \"Italy\" ] capital_list = [ \"Canberra\" , \"Paris\" , \"Washington DC\" , \"Rome\" ] pairs = zip ( country_list , capital_list ) for country , capital in pairs : print ( \"The country is {0} and the capital is {1} \" . format ( country , capital )) 1 2 3 4 The country is Australia and the capital is Canberra The country is France and the capital is Paris The country is USA and the capital is Washington DC The country is Italy and the capital is Rome 1 2 3 4 5 # sort() function to sort a list. It stores the sorted list in the original list itself. my_list = [ 954 , 341 , 100 , 3.14 ] my_list . sort () my_list 1 [3.14, 100, 341, 954]","title":"List comprehension"},{"location":"section-0-brief-python-refresher/#loops","text":"1 2 # Loops can be run over list of lists as well languages = [[ 'Spanish' , 'English' , 'French' , 'German' ], [ 'Python' , 'Java' , 'Javascript' , 'C++' ]] 1 2 for lang in languages : print ( lang ) 1 2 ['Spanish', 'English', 'French', 'German'] ['Python', 'Java', 'Javascript', 'C++'] As we see above, in each iteration, we get one list at a time. If each element of the nested list is needed, then a nested loop should be written as below: 1 2 3 4 for lang_list in languages : print ( \"--------------\" ) for lang in lang_list : print ( lang ) 1 2 3 4 5 6 7 8 9 10 -------------- Spanish English French German -------------- Python Java Javascript C++ There are various ways to manipulate the functioning of a loop. continue: This will skip the rest of the statements of that iteration and continue with the next iteration. break: This will break the entire loop and go to the next statement after the loop. 1 2 3 4 5 6 7 for lang_list in languages : print ( \"--------------\" ) for lang in lang_list : if lang == \"German\" : continue print ( lang ) print ( \"End of loops\" ) 1 2 3 4 5 6 7 8 9 10 -------------- Spanish English French -------------- Python Java Javascript C++ End of loops 1 2 3 4 5 6 7 for lang_list in languages : print ( \"--------------\" ) for lang in lang_list : if lang == \"Java\" : break print ( lang ) print ( \"End of loops\" ) 1 2 3 4 5 6 7 8 -------------- Spanish English French German -------------- Python End of loops 1 2 3 4 5 6 7 8 9 10 # another example of continue from math import sqrt number = 0 for i in range ( 10 ): number = i ** 2 if i % 2 == 0 : continue # continue here print ( str ( round ( sqrt ( number ))) + ' squared is equal to ' + str ( number )) 1 2 3 4 5 1 squared is equal to 1 3 squared is equal to 9 5 squared is equal to 25 7 squared is equal to 49 9 squared is equal to 81","title":"Loops"},{"location":"section-0-brief-python-refresher/#sets","text":"Sets is an unordered collections of unique elements. Common uses include membership testing, removing duplicates from a sequence, and computing standard math operations on sets such as intersection, union, difference, and symmetric difference. A set can be created using the '{}' brackets. 1 2 my_set = { 1 , 2 , 3 } print ( my_set ) 1 {1, 2, 3} 1 2 my_list = [ 1 , 2 , 3 , 4 , 2 , 3 ] print ( set ( my_list )) 1 {1, 2, 3, 4} 1 2 3 # A set can be made of mixed types as well. my_set = { 1.0 , \"Hello\" , ( 1 , 2 , 3 )} print ( my_set ) 1 {1.0, 'Hello', (1, 2, 3)} 1 2 3 # Even if duplicate elements are added while initialising, they get remived. my_set = { 1 , 2 , 3 , 4 , 3 , 2 } print ( my_set ) 1 {1, 2, 3, 4} 1 2 3 4 5 #Creating an empty set is a bit tricky. my_set = {} print ( type ( my_set )) my_set = set () print ( type ( my_set )) 1 2 <class 'dict'> <class 'set'> Elements can be added individualy or as a list. 1 2 3 4 5 my_set = { 1 , 2 , 3 } my_set . add ( 4 ) print ( my_set ) my_set . update ([ 6 , 7 , 8 ]) my_set 1 2 3 4 5 6 7 {1, 2, 3, 4} {1, 2, 3, 4, 6, 7, 8} 1 2 3 4 # Union A = { 1 , 2 , 3 , 4 , 5 } B = { 4 , 5 , 6 , 7 , 8 } A | B # or A.union(B) 1 {1, 2, 3, 4, 5, 6, 7, 8} 1 2 3 4 # Intersection A = { 1 , 2 , 3 , 4 , 5 } B = { 4 , 5 , 6 , 7 , 8 } A & B # A.intersection(B) 1 {4, 5}","title":"Sets"},{"location":"section-0-brief-python-refresher/#dictionary","text":"Dictionaries are a container that store key-value pairs. They are unordered. Other programming languages might call this a 'hash', 'hashtable' or 'hashmap'. 1 2 dict1 = { 'a' : 1 , 'b' : 2 , 'c' : 3 , 'd' : 4 } dict1 1 {'a': 1, 'b': 2, 'c': 3, 'd': 4} 1 2 3 # Adding a key to the dictionary dict1 [ 'e' ] = 5 dict1 1 {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5} keys() method returns the keys in the dictionary. 1 dict1 . keys () 1 dict_keys(['a', 'b', 'c', 'd', 'e']) 1 2 # items() method can be used to get all the pairs of the dictionary. dict1 . items () 1 dict_items([('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', 5)]) Dictionary comprehension can be used to manipulate the elements of a dictionary. 1 2 double_dict1 = { k : v * 2 for ( k , v ) in dict1 . items ()} double_dict1 1 {'a': 2, 'b': 4, 'c': 6, 'd': 8, 'e': 10} 1 2 dict1_cond = { k : v for ( k , v ) in dict1 . items () if v > 2 } dict1_cond 1 {'c': 3, 'd': 4, 'e': 5} 1","title":"Dictionary"},{"location":"section-0-brief-python-refresher/#functions","text":"A function is a block of organized, reusable code that is used to perform a single, related action. Functions provide better modularity for your application and a high degree of code reusing. 1 2 3 4 # Function definition def hello (): print ( \"Hello World\" ) return 1 2 # Function calling hello () 1 Hello World Parameters vs arguments: Parameters are a and b. Arguments are 2 and 5. 1 2 3 def plus ( a , b ): return a + b plus ( 2 , 5 ) 1 7 A function can return nothing (null/None) as well. 1 2 3 4 5 def run (): for x in range ( 10 ): if x == 2 : return print ( \"Run!\" ) 1 run ()","title":"Functions"},{"location":"section-0-brief-python-refresher/#keyword-arguments-with-default-values","text":"1 2 3 # Here the value of parameter `b` is 2 by default if the value is not passed. def plus ( a , b = 2 ): return a + b 1 2 # Call `plus()` with only `a` parameter print ( plus ( a = 1 )) 1 3 1 2 # Call `plus()` with `a` and `b` parameters print ( plus ( a = 1 , b = 3 )) 1 4","title":"Keyword arguments with default values"},{"location":"section-0-brief-python-refresher/#anonymous-functions-lambda","text":"1 2 3 4 5 6 7 8 9 # `sum()` lambda function sum = lambda x , y : x + y ; # Call the `sum()` anonymous function sum ( 4 , 5 ) # \"Translate\" to a UDF # def sum(x, y): # return x+y 1 9","title":"Anonymous functions: lambda"},{"location":"section-0-brief-python-refresher/#use-of-main","text":"1 2 3 4 5 6 7 8 9 10 11 12 def hello (): print ( \"Hello World\" ) return # Define `main()` function def main (): hello () print ( \"This is a main function\" ) main () # As is, if the script is imported, it will execute the main function. 1 2 Hello World This is a main function The following code needs a script mode to show the use. 1 2 3 4 5 6 7 8 # Define `main()` function def main (): hello () print ( \"This is a main function\" ) # Execute `main()` function if __name__ == '__main__' : main () 1 2 Hello World This is a main function","title":"Use of main()"},{"location":"section-0-brief-python-refresher/#global-vs-local-variables","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # Global variable `init` init = 1 # Define `plus()` function to accept a variable number of arguments def plus ( * args ): # Local variable `sum()` total = 0 for i in args : total += i return total # Access the global variable print ( \"this is the initialized value \" + str ( init )) # (Try to) access the local variable print ( \"this is the sum \" + str ( total )) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 this is the initialized value 1 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-67-136638963fdb> in <module> 14 15 # (Try to) access the local variable ---> 16 print(\"this is the sum \" + str(total)) NameError: name 'total' is not defined","title":"Global vs local variables"},{"location":"section-0-brief-python-refresher/#json","text":"JSON: JavaScript Object Notation. When exchanging data between a browser and a server, the data can only be text. Python has a built-in package called json, which can be used to work with JSON data. 1 import json 1 2 3 4 5 6 7 8 9 10 # Convert from JSON to Python: # some JSON: x = '{ \"name\":\"John\", \"age\":30, \"city\":\"New York\"}' # parse x: y = json . loads ( x ) # the result is a Python dictionary: print ( y [ \"age\" ]) 1 30 1 y 1 {'name': 'John', 'age': 30, 'city': 'New York'} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Convert from Python to JSON # a Python object (dict): x = { \"name\" : \"John\" , \"age\" : 30 , \"city\" : \"New York\" } # convert into JSON: y = json . dumps ( x ) # the result is a JSON string: print ( y ) 1 {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"} 1 2 3 # Indentation y = json . dumps ( x , indent = 4 ) print ( y ) 1 2 3 4 5 { \"name\": \"John\", \"age\": 30, \"city\": \"New York\" }","title":"JSON"},{"location":"section-0-brief-python-refresher/#dataframes","text":"One of the most powerful data structures in Python is the Pandas DataFrame . It allows tabular data, including csv (comma seperated values) and tsv (tab seperated values), to be processed and manipulated. People familiar with Excel will no doubt find it intuitive and easy to grasp. Since most csv (or tsv ) has become the de facto standard for sharing datasets both large and small, Pandas dataframe is the way to go. For Pandas documentation : https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html 1 2 import pandas as pd # importing the package and using `pd` as the alias print ( 'Pandas version : {} ' . format ( pd . __version__ )) 1 Pandas version : 1.0.1 Suppose we wanted to create a dataframe as follows, name title sam physicist rob economist Let's create a dictionary with the headers as keys and their corresponding values as a list as follows, 1 data = { 'name' : [ 'sam' , 'rob' ], 'title' : [ 'physicist' , 'economist' ]} Converting the same to a dataframe, 1 2 df = pd . DataFrame ( data ) print ( df . to_markdown ()) # converting to markdown for ease of display name title 0 sam physicist 1 rob economist We can also take a quick glance at its contents by using : - df.head() : To display the first 5 rows - df.tail() : To display the last 5 rows In order to read external files we use read_csv() function, 1 pd . read_csv ( filename , sep = ',' ) Similarly, for exporting a Pandas dataframe to a csv file, we can use to_csv() as follows 1 df . to_csv ( index = False )","title":"Dataframes"},{"location":"section-0-brief-python-refresher/#regex","text":"Regular expressions or regex are a powerful tool to extract key pieces of data from raw text. You can try your regex expressions in : - https://pythex.org/ for a python oriented regex editor - https://regexr.com/ for a more visual explanation behind the expressions (good for getting started) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 html = r ''' <!DOCTYPE html> <html> <head> <title>site</title> </head> <body> <h1>Sam</h1> <h2>Physicist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod.</p> <h1>Rob</h1> <h2>Economist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et.</p> </body> </html> ''' print ( html ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 <!DOCTYPE html> <html> <head> <title>site</title> </head> <body> <h1>Sam</h1> <h2>Physicist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod.</p> <h1>Rob</h1> <h2>Economist</h2> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et.</p> </body> </html> Now, if we are only interested in : - names i.e. the data inside the <h1></h1> tags, and - title i.e. the data inside the <h2></h2> tags we can extract the same using regex. First lets import the regex module in python called re 1 import re # regex package in python is named 're' Now lets define the expressions (or patterns) to capture all text between the tags as follows : <h1>(.*?)</h1> : capture all text contained within <h1></h1> tags <h2>(.*?)</h2> : capture all text contained within <h2></h2> tags 1 2 regex_h1 = re . compile ( '<h1>(.*?)</h1>' ) regex_h2 = re . compile ( '<h2>(.*?)</h2>' ) and use findall() to return all the instances that match with our pattern, 1 2 3 4 names = regex_h1 . findall ( html ) titles = regex_h2 . findall ( html ) print ( names , titles ) 1 ['Sam', 'Rob'] ['Physicist', 'Economist']","title":"Regex"},{"location":"section-0-brief-python-refresher/#from-a-web-scraping-perspective","text":"JSON and XML are the most widely used formats to carry data all over the internet. To work with CSV s (or TSV s), Pandas DataFrames are the de facto standard. Regexes help us extract key pieces of information (sub-strings) from raw, messy and unstructured text (strings).","title":"From a web scraping perspective :"},{"location":"section-1-intro-to-web-scraping/","text":"Introduction to Web Scraping \u00b6 What is web scraping? \u00b6 Web scraping is a technique for extracting information from websites. This can be done manually but it is usually faster, more efficient and less error-prone to automate the task. Web scraping allows you to acquire non-tabular or poorly structured data from websites and convert it into a usable, structured format, such as a .csv file or spreadsheet. Scraping is about more than just acquiring data: it can also help you archive data and track changes to data online. It is closely related to the practice of web indexing, which is what search engines like Google do when mass-analysing the Web to build their indices. But contrary to web indexing, which typically parses the entire content of a web page to make it searchable, web scraping targets specific information on the pages visited. For example, online stores will often scour the publicly available pages of their competitors, scrape item prices, and then use this information to adjust their own prices. Another common practice is \u201ccontact scraping\u201d in which personal information like email addresses or phone numbers is collected for marketing purposes. Why do we need it as a skill? \u00b6 Web scraping is increasingly being used by academics and researchers to create data sets for text mining projects; these might be collections of journal articles or digitised texts. The practice of data journalism, in particular, relies on the ability of investigative journalists to harvest data that is not always presented or published in a form that allows analysis. When do we need scraping? \u00b6 As useful as scraping is, there might be better options for the task. Choose the right (i.e. the easiest) tool for the job. Check whether or not you can easily copy and paste data from a site into Excel or Google Sheets. This might be quicker than scraping. Check if the site or service already provides an API to extract structured data. If it does, that will be a much more efficient and effective pathway. Good examples are the Facebook API, the Twitter APIs or the YouTube comments API. For much larger needs, Freedom of information requests can be useful. Be specific about the formats required for the data you want. Structured vs unstructured data \u00b6 When presented with information, human beings are good at quickly categorizing it and extracting the data that they are interested in. For example, when we look at a magazine rack, provided the titles are written in a script that we are able to read, we can rapidly figure out the titles of the magazines, the stories they contain, the language they are written in, etc. and we can probably also easily organize them by topic, recognize those that are aimed at children, or even whether they lean toward a particular end of the political spectrum. Computers have a much harder time making sense of such unstructured data unless we specifically tell them what elements data is made of, for example by adding labels such as this is the title of this magazine or this is a magazine about food. Data in which individual elements are separated and labelled is said to be structured. Refer to the file fortune_500_basic_example.html . We see that this data has been structured for displaying purposes (it is arranged in rows inside a table) but the different elements of information are not clearly labelled. What if we wanted to download this dataset and, for example, compare the revenues of these companies against each other or the industry that they work in? We could try copy-pasting the entire table into a spreadsheet or even manually copy-pasting the names and websites in another document, but this can quickly become impractical when faced with a large set of data. What if we wanted to collect this information for all the companies that are there? Fortunately, there are tools to automate at least part of the process. This technique is called web scraping. Web scraping (web harvesting or web data extraction) is a computer software technique of extracting information from websites.(Source: Wikipedia) Web scraping typically targets one web site at a time to extract unstructured information and put it in a structured form for reuse. In this lesson, we will continue exploring the examples above and try different techniques to extract the information they contain. But before we launch into web scraping proper, we need to look a bit closer at how information is organized within an HTML document and how to build queries to access a specific subset of that information. What is HTML? \u00b6 HTML stands for HyperText Markup Language It is the standard markup language for the webpages which make up the internet. HTML contains a series of elements which make up a webpage which can connect with other webpages altogether forming a website. The HTML elements are represented in tags which tell the web browser how to display the web content. A sample raw HTML file below : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 <!DOCTYPE html> < html > < head > < title > Page Title </ title > </ head > < body > < h1 > My First Heading </ h1 > < p > My first paragraph. </ p > </ body > </ html > Every HTML element corresponds to a display content on the web browser. The following image shows the HTML code and the webpage generated (please refer to `intro_html_example.html). What is XML? \u00b6 XML stands for eXtensible Markup Language XML is a markup language much like HTML XML was designed to store and transport data XML was designed to be self-descriptive 1 2 3 4 5 6 7 <note> <date> 2015-09-01 </date> <hour> 08:30 </hour> <to> Tove </to> <from> Jani </from> <body> Don't forget me this weekend! </body> </note> DOM (Document Object Model) \u00b6 DOM inspector : F12 to the rescue! \u00b6 1 References \u00b6 This image has been taken from https://www.w3schools.com/html/","title":"Section 1 intro to web scraping"},{"location":"section-1-intro-to-web-scraping/#introduction-to-web-scraping","text":"","title":"Introduction to Web Scraping"},{"location":"section-1-intro-to-web-scraping/#what-is-web-scraping","text":"Web scraping is a technique for extracting information from websites. This can be done manually but it is usually faster, more efficient and less error-prone to automate the task. Web scraping allows you to acquire non-tabular or poorly structured data from websites and convert it into a usable, structured format, such as a .csv file or spreadsheet. Scraping is about more than just acquiring data: it can also help you archive data and track changes to data online. It is closely related to the practice of web indexing, which is what search engines like Google do when mass-analysing the Web to build their indices. But contrary to web indexing, which typically parses the entire content of a web page to make it searchable, web scraping targets specific information on the pages visited. For example, online stores will often scour the publicly available pages of their competitors, scrape item prices, and then use this information to adjust their own prices. Another common practice is \u201ccontact scraping\u201d in which personal information like email addresses or phone numbers is collected for marketing purposes.","title":"What is web scraping?"},{"location":"section-1-intro-to-web-scraping/#why-do-we-need-it-as-a-skill","text":"Web scraping is increasingly being used by academics and researchers to create data sets for text mining projects; these might be collections of journal articles or digitised texts. The practice of data journalism, in particular, relies on the ability of investigative journalists to harvest data that is not always presented or published in a form that allows analysis.","title":"Why do we need it as a skill?"},{"location":"section-1-intro-to-web-scraping/#when-do-we-need-scraping","text":"As useful as scraping is, there might be better options for the task. Choose the right (i.e. the easiest) tool for the job. Check whether or not you can easily copy and paste data from a site into Excel or Google Sheets. This might be quicker than scraping. Check if the site or service already provides an API to extract structured data. If it does, that will be a much more efficient and effective pathway. Good examples are the Facebook API, the Twitter APIs or the YouTube comments API. For much larger needs, Freedom of information requests can be useful. Be specific about the formats required for the data you want.","title":"When do we need scraping?"},{"location":"section-1-intro-to-web-scraping/#structured-vs-unstructured-data","text":"When presented with information, human beings are good at quickly categorizing it and extracting the data that they are interested in. For example, when we look at a magazine rack, provided the titles are written in a script that we are able to read, we can rapidly figure out the titles of the magazines, the stories they contain, the language they are written in, etc. and we can probably also easily organize them by topic, recognize those that are aimed at children, or even whether they lean toward a particular end of the political spectrum. Computers have a much harder time making sense of such unstructured data unless we specifically tell them what elements data is made of, for example by adding labels such as this is the title of this magazine or this is a magazine about food. Data in which individual elements are separated and labelled is said to be structured. Refer to the file fortune_500_basic_example.html . We see that this data has been structured for displaying purposes (it is arranged in rows inside a table) but the different elements of information are not clearly labelled. What if we wanted to download this dataset and, for example, compare the revenues of these companies against each other or the industry that they work in? We could try copy-pasting the entire table into a spreadsheet or even manually copy-pasting the names and websites in another document, but this can quickly become impractical when faced with a large set of data. What if we wanted to collect this information for all the companies that are there? Fortunately, there are tools to automate at least part of the process. This technique is called web scraping. Web scraping (web harvesting or web data extraction) is a computer software technique of extracting information from websites.(Source: Wikipedia) Web scraping typically targets one web site at a time to extract unstructured information and put it in a structured form for reuse. In this lesson, we will continue exploring the examples above and try different techniques to extract the information they contain. But before we launch into web scraping proper, we need to look a bit closer at how information is organized within an HTML document and how to build queries to access a specific subset of that information.","title":"Structured vs unstructured data"},{"location":"section-1-intro-to-web-scraping/#what-is-html","text":"HTML stands for HyperText Markup Language It is the standard markup language for the webpages which make up the internet. HTML contains a series of elements which make up a webpage which can connect with other webpages altogether forming a website. The HTML elements are represented in tags which tell the web browser how to display the web content. A sample raw HTML file below : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 <!DOCTYPE html> < html > < head > < title > Page Title </ title > </ head > < body > < h1 > My First Heading </ h1 > < p > My first paragraph. </ p > </ body > </ html > Every HTML element corresponds to a display content on the web browser. The following image shows the HTML code and the webpage generated (please refer to `intro_html_example.html).","title":"What is HTML?"},{"location":"section-1-intro-to-web-scraping/#what-is-xml","text":"XML stands for eXtensible Markup Language XML is a markup language much like HTML XML was designed to store and transport data XML was designed to be self-descriptive 1 2 3 4 5 6 7 <note> <date> 2015-09-01 </date> <hour> 08:30 </hour> <to> Tove </to> <from> Jani </from> <body> Don't forget me this weekend! </body> </note>","title":"What is XML?"},{"location":"section-1-intro-to-web-scraping/#dom-document-object-model","text":"","title":"DOM (Document Object Model)"},{"location":"section-1-intro-to-web-scraping/#dom-inspector-f12-to-the-rescue","text":"1","title":"DOM inspector : F12 to the rescue!"},{"location":"section-1-intro-to-web-scraping/#references","text":"This image has been taken from https://www.w3schools.com/html/","title":"References"},{"location":"section-2-HTML-based-scraping/","text":"URL request and response \u00b6 A URL is Uniform Resource Locator. It acts as a web address to different webpages. Every URL on the internet work on a request-response basis. The browser requests the server for a webpage and the response by the server would be the content of the webpage. This web content is then displayed on the browser. URL Request - Requesting a web server for content to be viewed by the user. This HTTP request is triggered whenever you click on a link or open a webpage. URL Response - A response for the request irrespective of success or failure. For every request to the web server, a mandatory response is provided by the web server and most of the times this would be the respective content requested by the user. Run a simple HTTP request and explain response headers, status codes \u00b6 Instead of the browser requesting for the content of the webpage, Python can be used for the same. A HTTP request to the web server can be sent with the requests library and the response can be examined. Typically every request receives a response with response headers and status code details. Let us request for the web content for the Monash University front webpage with the URL - https://www.monash.edu/. The requests library can be used to work with webpages and web content. A request is made to get the content of the webpage with the get() method. 1 2 3 4 import requests monash_web_url = \"https://www.monash.edu/\" response = requests . get ( monash_web_url ) A response is received from the web server. This response will have response headers and status codes associated to that particular request. Response headers give the detailed information about the request made to the web server. 1 response . headers 1 {'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Vary': 'Accept-Encoding, Accept-Encoding', 'Cache-Control': 'max-age=0, private', 'Pragma': 'cache', 'Server': 'openresty', 'X-Content-Type-Options': 'nosniff', 'X-Cache': 'HIT from squizedge.net', 'Date': 'Fri, 13 Mar 2020 02:14:03 GMT', 'Age': '8', 'Via': '1.1 squizedge.net', 'X-upgrade-enabled': 'off', 'X-Frame-Options': 'SAMEORIGIN', 'Expires': 'Fri, 13 Mar 2020 02:44:03 GMT', 'X-Request-ID': '64ce1751-983a-4590-aa60-1d3f6b933689', 'Content-Encoding': 'gzip'} Every response will have a status code. The status codes indicate whether a specific HTTP request has been successfully completed. Responses are grouped in five classes: Informational responses (100\u2013199) Successful responses (200\u2013299) Redirects (300\u2013399) Client errors (400\u2013499) Server errors (500\u2013599) Let us check the response status code for the HTTP request we placed 1 response . status_code 1 200 The response has a status code of 200. This is a successful response and hence there should be relevant content of the webpage in the obtained response. This can be checked by printing the content. This content received is the HTML source code of the webpage 1 response . content [: 4000 ] 1 b'<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\" prefix=\"content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ og: http://ogp.me/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# schema: http://schema.org/ sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema# \">\\n <head>\\n <meta charset=\"utf-8\" />\\n<script>dataLayer = [];dataLayer.push({\"tag\": \"5914\"});</script>\\n<script>window.dataLayer = window.dataLayer || []; window.dataLayer.push({\"drupalLanguage\":\"en\",\"drupalCountry\":\"IN\",\"siteName\":\"Zyxware Technologies\",\"entityCreated\":\"1562300185\",\"entityLangcode\":\"en\",\"entityStatus\":\"1\",\"entityUid\":\"1\",\"entityUuid\":\"6fdfb477-ce5d-4081-9010-3afd9260cdf7\",\"entityVid\":\"15541\",\"entityName\":\"webmaster\",\"entityType\":\"node\",\"entityBundle\":\"story\",\"entityId\":\"5914\",\"entityTitle\":\"List of Fortune 500 companies and their websites (2018)\",\"entityTaxonomy\":{\"vocabulary_2\":\"Business Insight, Fortune 500, Drupal Insight, Marketing Resources\"},\"userUid\":0});</script>\\n<script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-1488254-2\"></script>\\n<script>window.google_analytics_uacct = \"UA-1488254-2\";window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments)};gtag(\"js\", new Date());window[\\'GoogleAnalyticsObject\\'] = \\'ga\\';\\r\\n window[\\'ga\\'] = window[\\'ga\\'] || function() {\\r\\n (window[\\'ga\\'].q = window[\\'ga\\'].q || []).push(arguments)\\r\\n };\\r\\nga(\"set\", \"dimension2\", window.analytics_manager_node_age);\\r\\nga(\"set\", \"dimension3\", window.analytics_manager_node_author);gtag(\"config\", \"UA-1488254-2\", {\"groups\":\"default\",\"anonymize_ip\":true,\"page_path\":location.pathname + location.search + location.hash,\"link_attribution\":true,\"allow_ad_personalization_signals\":false});</script>\\n<meta name=\"title\" content=\"List of Fortune 500 companies and their websites (2018) | Zyxware Technologies\" />\\n<link rel=\"canonical\" href=\"https://www.zyxware.com/articles/5914/list-of-fortune-500-companies-and-their-websites-2018\" />\\n<meta name=\"description\" content=\"Fortune magazine publishes a list of the largest companies in the US by revenue every year. Here is the list of fortune 500 companies for the year 2018 and their websites. Check out the current list of fortune 500 companies and their websites.\" />\\n<meta name=\"Generator\" content=\"Drupal 8 (https://www.drupal.org)\" />\\n<meta name=\"MobileOptimized\" content=\"width\" />\\n<meta name=\"HandheldFriendly\" content=\"true\" />\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\\n<style>div#sliding-popup, div#sliding-popup .eu-cookie-withdraw-banner, .eu-cookie-withdraw-tab {background: #733ec0} div#sliding-popup.eu-cookie-withdraw-wrapper { background: transparent; } #sliding-popup h1, #sliding-popup h2, #sliding-popup h3, #sliding-popup p, #sliding-popup label, #sliding-popup div, .eu-cookie-compliance-more-button, .eu-cookie-compliance-secondary-button, .eu-cookie-withdraw-tab { color: #ffffff;} .eu-cookie-withdraw-tab { border-color: #ffffff;}</style>\\n<script src=\"https://www.google.com/recaptcha/api.js?hl=en\" async defer></script>\\n<link rel=\"shortcut icon\" href=\"/themes/custom/zyxpro_light/favicon.ico\" type=\"image/vnd.microsoft.icon\" />\\n<link rel=\"revision\" href=\"https://www.zyxware.com/articles/5914/list-of-fortune-500-companies-and-their-websites-2018\" />\\n<script src=\"/sites/default/files/google_tag/google_tag.script.js?q72ub5\"></script>\\n<script>window.a2a_config=window.a2a_config||{};a2a_config.callbacks=[];a2a_config.overlays=[];a2a_config.templates={};</script>\\n\\n <title>List of Fortune 500 companies and their websites (2018) | Zyxware Technologies</title>\\n <link rel=\"stylesheet\" media=\"all\" href=\"/sites/default/files/css/css_20KuxgA9EGPA1Yt5CdQmKTq6xZJpEUDALYwFLBKAYns.css?q72ub5\" />\\n<link rel=\"stylesheet\" media=\"all\" href=\"https://fonts.googleapis.com/css?family=Montserrat:300,400,500,600,700\" />\\n<link rel=\"stylesheet\" media=\"al' GET and POST calls to retrieve response objects - using urllib2, requests, JSON etc \u00b6 There are mainly two types of requests which can be made to the web server. A GET request/call or a POST request/call. GET call - GET is used to request data from a specified source. They are one of the most common HTTP requests. They are usually used to only receive content from the web server. An example would be to receive the content of the complete webpage. POST call - POST is used to send data to either update details or request specific content from the web server. In a POST call, data is sent and then a response can be expected from the web server. An example would be to request content from a web server based on a particular selection from a drop-down menu. The selection option is upadted while also respective content is sent back. Let us scrape a list of the fotune 500 compaies for the year 2018. The website from which the data is to be scraped is https://www.zyxware.com/articles/5914/list-of-fortune-500-companies-and-their-websites-2018. It can be seen on this website that the list contains the rank, company name and the website of the company. The whole content of this website can be received as a response when requested with the request library in Python 1 2 3 4 5 6 7 8 9 10 import requests import pandas as pd from bs4 import BeautifulSoup web_url = 'https://www.zyxware.com/articles/5914/list-of-fortune-500-companies-and-their-websites-2018' response = requests . get ( web_url ) print ( 'Status code \\n ' , response . status_code ) print ( ' \\n -- \\n ' ) print ( 'Content of the website \\n ' , response . content [: 4000 ]) 1 2 3 4 5 6 7 Status code 200 -- Content of the website b'<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\" prefix=\"content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ og: http://ogp.me/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# schema: http://schema.org/ sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema# \">\\n <head>\\n <meta charset=\"utf-8\" />\\n<script>dataLayer = [];dataLayer.push({\"tag\": \"5914\"});</script>\\n<script>window.dataLayer = window.dataLayer || []; window.dataLayer.push({\"drupalLanguage\":\"en\",\"drupalCountry\":\"IN\",\"siteName\":\"Zyxware Technologies\",\"entityCreated\":\"1562300185\",\"entityLangcode\":\"en\",\"entityStatus\":\"1\",\"entityUid\":\"1\",\"entityUuid\":\"6fdfb477-ce5d-4081-9010-3afd9260cdf7\",\"entityVid\":\"15541\",\"entityName\":\"webmaster\",\"entityType\":\"node\",\"entityBundle\":\"story\",\"entityId\":\"5914\",\"entityTitle\":\"List of Fortune 500 companies and their websites (2018)\",\"entityTaxonomy\":{\"vocabulary_2\":\"Business Insight, Fortune 500, Drupal Insight, Marketing Resources\"},\"userUid\":0});</script>\\n<script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-1488254-2\"></script>\\n<script>window.google_analytics_uacct = \"UA-1488254-2\";window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments)};gtag(\"js\", new Date());window[\\'GoogleAnalyticsObject\\'] = \\'ga\\';\\r\\n window[\\'ga\\'] = window[\\'ga\\'] || function() {\\r\\n (window[\\'ga\\'].q = window[\\'ga\\'].q || []).push(arguments)\\r\\n };\\r\\nga(\"set\", \"dimension2\", window.analytics_manager_node_age);\\r\\nga(\"set\", \"dimension3\", window.analytics_manager_node_author);gtag(\"config\", \"UA-1488254-2\", {\"groups\":\"default\",\"anonymize_ip\":true,\"page_path\":location.pathname + location.search + location.hash,\"link_attribution\":true,\"allow_ad_personalization_signals\":false});</script>\\n<meta name=\"title\" content=\"List of Fortune 500 companies and their websites (2018) | Zyxware Technologies\" />\\n<link rel=\"canonical\" href=\"https://www.zyxware.com/articles/5914/list-of-fortune-500-companies-and-their-websites-2018\" />\\n<meta name=\"description\" content=\"Fortune magazine publishes a list of the largest companies in the US by revenue every year. Here is the list of fortune 500 companies for the year 2018 and their websites. Check out the current list of fortune 500 companies and their websites.\" />\\n<meta name=\"Generator\" content=\"Drupal 8 (https://www.drupal.org)\" />\\n<meta name=\"MobileOptimized\" content=\"width\" />\\n<meta name=\"HandheldFriendly\" content=\"true\" />\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\\n<style>div#sliding-popup, div#sliding-popup .eu-cookie-withdraw-banner, .eu-cookie-withdraw-tab {background: #733ec0} div#sliding-popup.eu-cookie-withdraw-wrapper { background: transparent; } #sliding-popup h1, #sliding-popup h2, #sliding-popup h3, #sliding-popup p, #sliding-popup label, #sliding-popup div, .eu-cookie-compliance-more-button, .eu-cookie-compliance-secondary-button, .eu-cookie-withdraw-tab { color: #ffffff;} .eu-cookie-withdraw-tab { border-color: #ffffff;}</style>\\n<script src=\"https://www.google.com/recaptcha/api.js?hl=en\" async defer></script>\\n<link rel=\"shortcut icon\" href=\"/themes/custom/zyxpro_light/favicon.ico\" type=\"image/vnd.microsoft.icon\" />\\n<link rel=\"revision\" href=\"https://www.zyxware.com/articles/5914/list-of-fortune-500-companies-and-their-websites-2018\" />\\n<script src=\"/sites/default/files/google_tag/google_tag.script.js?q72ub5\"></script>\\n<script>window.a2a_config=window.a2a_config||{};a2a_config.callbacks=[];a2a_config.overlays=[];a2a_config.templates={};</script>\\n\\n <title>List of Fortune 500 companies and their websites (2018) | Zyxware Technologies</title>\\n <link rel=\"stylesheet\" media=\"all\" href=\"/sites/default/files/css/css_20KuxgA9EGPA1Yt5CdQmKTq6xZJpEUDALYwFLBKAYns.css?q72ub5\" />\\n<link rel=\"stylesheet\" media=\"all\" href=\"https://fonts.googleapis.com/css?family=Montserrat:300,400,500,600,700\" />\\n<link rel=\"stylesheet\" media=\"al' Using bs4 (and lxml) to parse the structure and access different elements within a HTML or XML \u00b6 bs4 is a Python library which parses through HTML content and understands the complete structure of the content. The response content can be passed to a BeautifulSoup method to obtain a soup object which looks very structured. 1 2 3 4 soup_object = BeautifulSoup ( response . content ) # Uncomment the below line and look into the contents of soup_object # soup_object Manipulate it into a tabular structure - explore the schema \u00b6 To be able to accurately extract relevant data from the webpage, it is important to explore the schema and understand the structure of the webpage. A good way to do this is to inspect the webpage directly on a web browser. To do this, - Open the webpage on a browser - Right click on the data content to be extracted - Click on 'Inspect' or 'Inspect element' option This will open a console window which shows the real time HTML code corresponding to the web content. Now identify the type of HTML tag which contains all the data along with any id names or class names associated to that HTML tag. In our case, the data is enclosed in the 'table' HTML tag with the class name 'data-table'. This information can be used to search for the web content directly in our soup object with the find_all() method. This will return a soup object. 1 2 3 4 data_table = soup_object . find_all ( 'table' , 'data-table' )[ 0 ] # Uncomment the below line and look into the contents of data_table # data_table It can be seen that relevant block of data has been extracted but further extracted needs to be done to individually extract the rank, company name and the company website data. On further analysis, it can be seen that every row of data is enclosed under a 'tr' HTML tag which means 'table row'. All these row values can be extracted into a list of values by finding the 'tr' values from our newly created soup object 'data_table' 1 2 all_values = data_table . find_all ( 'tr' ) all_values [: 10 ] # Prints the first 10 captured tag elements 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 [<tr><th>Rank</th> <th>Company</th> <th>Website</th> </tr>, <tr><td>1</td> <td>Walmart</td> <td><a href=\"http://www.stock.walmart.com\">http://www.stock.walmart.com</a></td> </tr>, <tr><td>2</td> <td>Exxon Mobil</td> <td><a href=\"http://www.exxonmobil.com\">http://www.exxonmobil.com</a></td> </tr>, <tr><td>3</td> <td>Berkshire Hathaway</td> <td><a href=\"http://www.berkshirehathaway.com\">http://www.berkshirehathaway.com</a></td> </tr>, <tr><td>4</td> <td>Apple</td> <td><a href=\"http://www.apple.com\">http://www.apple.com</a></td> </tr>, <tr><td>5</td> <td>UnitedHealth Group</td> <td><a href=\"http://www.unitedhealthgroup.com\">http://www.unitedhealthgroup.com</a></td> </tr>, <tr><td>6</td> <td>McKesson</td> <td><a href=\"http://www.mckesson.com\">http://www.mckesson.com</a></td> </tr>, <tr><td>7</td> <td>CVS Health</td> <td><a href=\"http://www.cvshealth.com\">http://www.cvshealth.com</a></td> </tr>, <tr><td>8</td> <td>Amazon.com</td> <td><a href=\"http://www.amazon.com\">http://www.amazon.com</a></td> </tr>, <tr><td>9</td> <td>AT&amp;T</td> <td><a href=\"http://www.att.com\">http://www.att.com</a></td> </tr>] 1 2 3 4 5 print ( all_values [ 0 ]) print ( '--' ) print ( all_values [ 1 ]) print ( '--' ) print ( all_values [ 2 ]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 <tr><th>Rank</th> <th>Company</th> <th>Website</th> </tr> -- <tr><td>1</td> <td>Walmart</td> <td><a href=\"http://www.stock.walmart.com\">http://www.stock.walmart.com</a></td> </tr> -- <tr><td>2</td> <td>Exxon Mobil</td> <td><a href=\"http://www.exxonmobil.com\">http://www.exxonmobil.com</a></td> </tr> The first element of the list contains the column names 'Rank, Company and Website'. The next elements of the list contain soup objects which contain the company data including the rank. This data can be extracted in a loop since the structure for all the list elements is the same. An empty dataframe fortune_500_df is created with the column names 'rank', 'company_name' and 'company_website' The index is initiated to zero A for loop is designed to go through all the elements of the list in order and extract the rank, company name and company website from the list element which are enclosed in the 'td' HTML tag. A find_all() will return a list of td tags. The '.text' attribute can be used to just pick the text part from the tag. In our case this is the rank, company name and the compnay website These values are then put into the dataframe and the index value is incremented 1 2 3 4 5 6 7 8 9 10 11 12 13 fortune_500_df = pd . DataFrame ( columns = [ 'rank' , 'company_name' , 'company_website' ]) ix = 0 for row in all_values [ 1 :]: values = row . find_all ( 'td' ) rank = values [ 0 ] . text company = values [ 1 ] . text website = values [ 2 ] . text fortune_500_df . loc [ ix ] = [ rank , company , website ] ix += 1 fortune_500_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rank company_name company_website 0 1 Walmart http://www.stock.walmart.com 1 2 Exxon Mobil http://www.exxonmobil.com 2 3 Berkshire Hathaway http://www.berkshirehathaway.com 3 4 Apple http://www.apple.com 4 5 UnitedHealth Group http://www.unitedhealthgroup.com Store it in the appropriate format - CSV, TSV and export the results \u00b6 The dataframe can now be stored as a csv file. Pandas has a 'to_csv' method which can be used to save the data into the file ' 1 fortune_500_df . to_csv ( './fortune_500_companies.csv' , index = False ) 1","title":"HTML based scraping"},{"location":"section-2-HTML-based-scraping/#url-request-and-response","text":"A URL is Uniform Resource Locator. It acts as a web address to different webpages. Every URL on the internet work on a request-response basis. The browser requests the server for a webpage and the response by the server would be the content of the webpage. This web content is then displayed on the browser. URL Request - Requesting a web server for content to be viewed by the user. This HTTP request is triggered whenever you click on a link or open a webpage. URL Response - A response for the request irrespective of success or failure. For every request to the web server, a mandatory response is provided by the web server and most of the times this would be the respective content requested by the user.","title":"URL request and response"},{"location":"section-2-HTML-based-scraping/#run-a-simple-http-request-and-explain-response-headers-status-codes","text":"Instead of the browser requesting for the content of the webpage, Python can be used for the same. A HTTP request to the web server can be sent with the requests library and the response can be examined. Typically every request receives a response with response headers and status code details. Let us request for the web content for the Monash University front webpage with the URL - https://www.monash.edu/. The requests library can be used to work with webpages and web content. A request is made to get the content of the webpage with the get() method. 1 2 3 4 import requests monash_web_url = \"https://www.monash.edu/\" response = requests . get ( monash_web_url ) A response is received from the web server. This response will have response headers and status codes associated to that particular request. Response headers give the detailed information about the request made to the web server. 1 response . headers 1 {'Content-Type': 'text/html; charset=utf-8', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Vary': 'Accept-Encoding, Accept-Encoding', 'Cache-Control': 'max-age=0, private', 'Pragma': 'cache', 'Server': 'openresty', 'X-Content-Type-Options': 'nosniff', 'X-Cache': 'HIT from squizedge.net', 'Date': 'Fri, 13 Mar 2020 02:14:03 GMT', 'Age': '8', 'Via': '1.1 squizedge.net', 'X-upgrade-enabled': 'off', 'X-Frame-Options': 'SAMEORIGIN', 'Expires': 'Fri, 13 Mar 2020 02:44:03 GMT', 'X-Request-ID': '64ce1751-983a-4590-aa60-1d3f6b933689', 'Content-Encoding': 'gzip'} Every response will have a status code. The status codes indicate whether a specific HTTP request has been successfully completed. Responses are grouped in five classes: Informational responses (100\u2013199) Successful responses (200\u2013299) Redirects (300\u2013399) Client errors (400\u2013499) Server errors (500\u2013599) Let us check the response status code for the HTTP request we placed 1 response . status_code 1 200 The response has a status code of 200. This is a successful response and hence there should be relevant content of the webpage in the obtained response. This can be checked by printing the content. This content received is the HTML source code of the webpage 1 response . content [: 4000 ] 1 b'<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\" prefix=\"content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ og: http://ogp.me/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# schema: http://schema.org/ sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema# \">\\n <head>\\n <meta charset=\"utf-8\" />\\n<script>dataLayer = [];dataLayer.push({\"tag\": \"5914\"});</script>\\n<script>window.dataLayer = window.dataLayer || []; window.dataLayer.push({\"drupalLanguage\":\"en\",\"drupalCountry\":\"IN\",\"siteName\":\"Zyxware Technologies\",\"entityCreated\":\"1562300185\",\"entityLangcode\":\"en\",\"entityStatus\":\"1\",\"entityUid\":\"1\",\"entityUuid\":\"6fdfb477-ce5d-4081-9010-3afd9260cdf7\",\"entityVid\":\"15541\",\"entityName\":\"webmaster\",\"entityType\":\"node\",\"entityBundle\":\"story\",\"entityId\":\"5914\",\"entityTitle\":\"List of Fortune 500 companies and their websites (2018)\",\"entityTaxonomy\":{\"vocabulary_2\":\"Business Insight, Fortune 500, Drupal Insight, Marketing Resources\"},\"userUid\":0});</script>\\n<script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-1488254-2\"></script>\\n<script>window.google_analytics_uacct = \"UA-1488254-2\";window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments)};gtag(\"js\", new Date());window[\\'GoogleAnalyticsObject\\'] = \\'ga\\';\\r\\n window[\\'ga\\'] = window[\\'ga\\'] || function() {\\r\\n (window[\\'ga\\'].q = window[\\'ga\\'].q || []).push(arguments)\\r\\n };\\r\\nga(\"set\", \"dimension2\", window.analytics_manager_node_age);\\r\\nga(\"set\", \"dimension3\", window.analytics_manager_node_author);gtag(\"config\", \"UA-1488254-2\", {\"groups\":\"default\",\"anonymize_ip\":true,\"page_path\":location.pathname + location.search + location.hash,\"link_attribution\":true,\"allow_ad_personalization_signals\":false});</script>\\n<meta name=\"title\" content=\"List of Fortune 500 companies and their websites (2018) | Zyxware Technologies\" />\\n<link rel=\"canonical\" href=\"https://www.zyxware.com/articles/5914/list-of-fortune-500-companies-and-their-websites-2018\" />\\n<meta name=\"description\" content=\"Fortune magazine publishes a list of the largest companies in the US by revenue every year. Here is the list of fortune 500 companies for the year 2018 and their websites. Check out the current list of fortune 500 companies and their websites.\" />\\n<meta name=\"Generator\" content=\"Drupal 8 (https://www.drupal.org)\" />\\n<meta name=\"MobileOptimized\" content=\"width\" />\\n<meta name=\"HandheldFriendly\" content=\"true\" />\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\\n<style>div#sliding-popup, div#sliding-popup .eu-cookie-withdraw-banner, .eu-cookie-withdraw-tab {background: #733ec0} div#sliding-popup.eu-cookie-withdraw-wrapper { background: transparent; } #sliding-popup h1, #sliding-popup h2, #sliding-popup h3, #sliding-popup p, #sliding-popup label, #sliding-popup div, .eu-cookie-compliance-more-button, .eu-cookie-compliance-secondary-button, .eu-cookie-withdraw-tab { color: #ffffff;} .eu-cookie-withdraw-tab { border-color: #ffffff;}</style>\\n<script src=\"https://www.google.com/recaptcha/api.js?hl=en\" async defer></script>\\n<link rel=\"shortcut icon\" href=\"/themes/custom/zyxpro_light/favicon.ico\" type=\"image/vnd.microsoft.icon\" />\\n<link rel=\"revision\" href=\"https://www.zyxware.com/articles/5914/list-of-fortune-500-companies-and-their-websites-2018\" />\\n<script src=\"/sites/default/files/google_tag/google_tag.script.js?q72ub5\"></script>\\n<script>window.a2a_config=window.a2a_config||{};a2a_config.callbacks=[];a2a_config.overlays=[];a2a_config.templates={};</script>\\n\\n <title>List of Fortune 500 companies and their websites (2018) | Zyxware Technologies</title>\\n <link rel=\"stylesheet\" media=\"all\" href=\"/sites/default/files/css/css_20KuxgA9EGPA1Yt5CdQmKTq6xZJpEUDALYwFLBKAYns.css?q72ub5\" />\\n<link rel=\"stylesheet\" media=\"all\" href=\"https://fonts.googleapis.com/css?family=Montserrat:300,400,500,600,700\" />\\n<link rel=\"stylesheet\" media=\"al'","title":"Run a simple HTTP request and explain response headers, status codes"},{"location":"section-2-HTML-based-scraping/#get-and-post-calls-to-retrieve-response-objects-using-urllib2-requests-json-etc","text":"There are mainly two types of requests which can be made to the web server. A GET request/call or a POST request/call. GET call - GET is used to request data from a specified source. They are one of the most common HTTP requests. They are usually used to only receive content from the web server. An example would be to receive the content of the complete webpage. POST call - POST is used to send data to either update details or request specific content from the web server. In a POST call, data is sent and then a response can be expected from the web server. An example would be to request content from a web server based on a particular selection from a drop-down menu. The selection option is upadted while also respective content is sent back. Let us scrape a list of the fotune 500 compaies for the year 2018. The website from which the data is to be scraped is https://www.zyxware.com/articles/5914/list-of-fortune-500-companies-and-their-websites-2018. It can be seen on this website that the list contains the rank, company name and the website of the company. The whole content of this website can be received as a response when requested with the request library in Python 1 2 3 4 5 6 7 8 9 10 import requests import pandas as pd from bs4 import BeautifulSoup web_url = 'https://www.zyxware.com/articles/5914/list-of-fortune-500-companies-and-their-websites-2018' response = requests . get ( web_url ) print ( 'Status code \\n ' , response . status_code ) print ( ' \\n -- \\n ' ) print ( 'Content of the website \\n ' , response . content [: 4000 ]) 1 2 3 4 5 6 7 Status code 200 -- Content of the website b'<!DOCTYPE html>\\n<html lang=\"en\" dir=\"ltr\" prefix=\"content: http://purl.org/rss/1.0/modules/content/ dc: http://purl.org/dc/terms/ foaf: http://xmlns.com/foaf/0.1/ og: http://ogp.me/ns# rdfs: http://www.w3.org/2000/01/rdf-schema# schema: http://schema.org/ sioc: http://rdfs.org/sioc/ns# sioct: http://rdfs.org/sioc/types# skos: http://www.w3.org/2004/02/skos/core# xsd: http://www.w3.org/2001/XMLSchema# \">\\n <head>\\n <meta charset=\"utf-8\" />\\n<script>dataLayer = [];dataLayer.push({\"tag\": \"5914\"});</script>\\n<script>window.dataLayer = window.dataLayer || []; window.dataLayer.push({\"drupalLanguage\":\"en\",\"drupalCountry\":\"IN\",\"siteName\":\"Zyxware Technologies\",\"entityCreated\":\"1562300185\",\"entityLangcode\":\"en\",\"entityStatus\":\"1\",\"entityUid\":\"1\",\"entityUuid\":\"6fdfb477-ce5d-4081-9010-3afd9260cdf7\",\"entityVid\":\"15541\",\"entityName\":\"webmaster\",\"entityType\":\"node\",\"entityBundle\":\"story\",\"entityId\":\"5914\",\"entityTitle\":\"List of Fortune 500 companies and their websites (2018)\",\"entityTaxonomy\":{\"vocabulary_2\":\"Business Insight, Fortune 500, Drupal Insight, Marketing Resources\"},\"userUid\":0});</script>\\n<script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-1488254-2\"></script>\\n<script>window.google_analytics_uacct = \"UA-1488254-2\";window.dataLayer = window.dataLayer || [];function gtag(){dataLayer.push(arguments)};gtag(\"js\", new Date());window[\\'GoogleAnalyticsObject\\'] = \\'ga\\';\\r\\n window[\\'ga\\'] = window[\\'ga\\'] || function() {\\r\\n (window[\\'ga\\'].q = window[\\'ga\\'].q || []).push(arguments)\\r\\n };\\r\\nga(\"set\", \"dimension2\", window.analytics_manager_node_age);\\r\\nga(\"set\", \"dimension3\", window.analytics_manager_node_author);gtag(\"config\", \"UA-1488254-2\", {\"groups\":\"default\",\"anonymize_ip\":true,\"page_path\":location.pathname + location.search + location.hash,\"link_attribution\":true,\"allow_ad_personalization_signals\":false});</script>\\n<meta name=\"title\" content=\"List of Fortune 500 companies and their websites (2018) | Zyxware Technologies\" />\\n<link rel=\"canonical\" href=\"https://www.zyxware.com/articles/5914/list-of-fortune-500-companies-and-their-websites-2018\" />\\n<meta name=\"description\" content=\"Fortune magazine publishes a list of the largest companies in the US by revenue every year. Here is the list of fortune 500 companies for the year 2018 and their websites. Check out the current list of fortune 500 companies and their websites.\" />\\n<meta name=\"Generator\" content=\"Drupal 8 (https://www.drupal.org)\" />\\n<meta name=\"MobileOptimized\" content=\"width\" />\\n<meta name=\"HandheldFriendly\" content=\"true\" />\\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\\n<style>div#sliding-popup, div#sliding-popup .eu-cookie-withdraw-banner, .eu-cookie-withdraw-tab {background: #733ec0} div#sliding-popup.eu-cookie-withdraw-wrapper { background: transparent; } #sliding-popup h1, #sliding-popup h2, #sliding-popup h3, #sliding-popup p, #sliding-popup label, #sliding-popup div, .eu-cookie-compliance-more-button, .eu-cookie-compliance-secondary-button, .eu-cookie-withdraw-tab { color: #ffffff;} .eu-cookie-withdraw-tab { border-color: #ffffff;}</style>\\n<script src=\"https://www.google.com/recaptcha/api.js?hl=en\" async defer></script>\\n<link rel=\"shortcut icon\" href=\"/themes/custom/zyxpro_light/favicon.ico\" type=\"image/vnd.microsoft.icon\" />\\n<link rel=\"revision\" href=\"https://www.zyxware.com/articles/5914/list-of-fortune-500-companies-and-their-websites-2018\" />\\n<script src=\"/sites/default/files/google_tag/google_tag.script.js?q72ub5\"></script>\\n<script>window.a2a_config=window.a2a_config||{};a2a_config.callbacks=[];a2a_config.overlays=[];a2a_config.templates={};</script>\\n\\n <title>List of Fortune 500 companies and their websites (2018) | Zyxware Technologies</title>\\n <link rel=\"stylesheet\" media=\"all\" href=\"/sites/default/files/css/css_20KuxgA9EGPA1Yt5CdQmKTq6xZJpEUDALYwFLBKAYns.css?q72ub5\" />\\n<link rel=\"stylesheet\" media=\"all\" href=\"https://fonts.googleapis.com/css?family=Montserrat:300,400,500,600,700\" />\\n<link rel=\"stylesheet\" media=\"al'","title":"GET and POST calls to retrieve response objects - using urllib2, requests, JSON etc"},{"location":"section-2-HTML-based-scraping/#using-bs4-and-lxml-to-parse-the-structure-and-access-different-elements-within-a-html-or-xml","text":"bs4 is a Python library which parses through HTML content and understands the complete structure of the content. The response content can be passed to a BeautifulSoup method to obtain a soup object which looks very structured. 1 2 3 4 soup_object = BeautifulSoup ( response . content ) # Uncomment the below line and look into the contents of soup_object # soup_object","title":"Using bs4 (and lxml) to parse the structure and access different elements within a HTML or XML"},{"location":"section-2-HTML-based-scraping/#manipulate-it-into-a-tabular-structure-explore-the-schema","text":"To be able to accurately extract relevant data from the webpage, it is important to explore the schema and understand the structure of the webpage. A good way to do this is to inspect the webpage directly on a web browser. To do this, - Open the webpage on a browser - Right click on the data content to be extracted - Click on 'Inspect' or 'Inspect element' option This will open a console window which shows the real time HTML code corresponding to the web content. Now identify the type of HTML tag which contains all the data along with any id names or class names associated to that HTML tag. In our case, the data is enclosed in the 'table' HTML tag with the class name 'data-table'. This information can be used to search for the web content directly in our soup object with the find_all() method. This will return a soup object. 1 2 3 4 data_table = soup_object . find_all ( 'table' , 'data-table' )[ 0 ] # Uncomment the below line and look into the contents of data_table # data_table It can be seen that relevant block of data has been extracted but further extracted needs to be done to individually extract the rank, company name and the company website data. On further analysis, it can be seen that every row of data is enclosed under a 'tr' HTML tag which means 'table row'. All these row values can be extracted into a list of values by finding the 'tr' values from our newly created soup object 'data_table' 1 2 all_values = data_table . find_all ( 'tr' ) all_values [: 10 ] # Prints the first 10 captured tag elements 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 [<tr><th>Rank</th> <th>Company</th> <th>Website</th> </tr>, <tr><td>1</td> <td>Walmart</td> <td><a href=\"http://www.stock.walmart.com\">http://www.stock.walmart.com</a></td> </tr>, <tr><td>2</td> <td>Exxon Mobil</td> <td><a href=\"http://www.exxonmobil.com\">http://www.exxonmobil.com</a></td> </tr>, <tr><td>3</td> <td>Berkshire Hathaway</td> <td><a href=\"http://www.berkshirehathaway.com\">http://www.berkshirehathaway.com</a></td> </tr>, <tr><td>4</td> <td>Apple</td> <td><a href=\"http://www.apple.com\">http://www.apple.com</a></td> </tr>, <tr><td>5</td> <td>UnitedHealth Group</td> <td><a href=\"http://www.unitedhealthgroup.com\">http://www.unitedhealthgroup.com</a></td> </tr>, <tr><td>6</td> <td>McKesson</td> <td><a href=\"http://www.mckesson.com\">http://www.mckesson.com</a></td> </tr>, <tr><td>7</td> <td>CVS Health</td> <td><a href=\"http://www.cvshealth.com\">http://www.cvshealth.com</a></td> </tr>, <tr><td>8</td> <td>Amazon.com</td> <td><a href=\"http://www.amazon.com\">http://www.amazon.com</a></td> </tr>, <tr><td>9</td> <td>AT&amp;T</td> <td><a href=\"http://www.att.com\">http://www.att.com</a></td> </tr>] 1 2 3 4 5 print ( all_values [ 0 ]) print ( '--' ) print ( all_values [ 1 ]) print ( '--' ) print ( all_values [ 2 ]) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 <tr><th>Rank</th> <th>Company</th> <th>Website</th> </tr> -- <tr><td>1</td> <td>Walmart</td> <td><a href=\"http://www.stock.walmart.com\">http://www.stock.walmart.com</a></td> </tr> -- <tr><td>2</td> <td>Exxon Mobil</td> <td><a href=\"http://www.exxonmobil.com\">http://www.exxonmobil.com</a></td> </tr> The first element of the list contains the column names 'Rank, Company and Website'. The next elements of the list contain soup objects which contain the company data including the rank. This data can be extracted in a loop since the structure for all the list elements is the same. An empty dataframe fortune_500_df is created with the column names 'rank', 'company_name' and 'company_website' The index is initiated to zero A for loop is designed to go through all the elements of the list in order and extract the rank, company name and company website from the list element which are enclosed in the 'td' HTML tag. A find_all() will return a list of td tags. The '.text' attribute can be used to just pick the text part from the tag. In our case this is the rank, company name and the compnay website These values are then put into the dataframe and the index value is incremented 1 2 3 4 5 6 7 8 9 10 11 12 13 fortune_500_df = pd . DataFrame ( columns = [ 'rank' , 'company_name' , 'company_website' ]) ix = 0 for row in all_values [ 1 :]: values = row . find_all ( 'td' ) rank = values [ 0 ] . text company = values [ 1 ] . text website = values [ 2 ] . text fortune_500_df . loc [ ix ] = [ rank , company , website ] ix += 1 fortune_500_df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rank company_name company_website 0 1 Walmart http://www.stock.walmart.com 1 2 Exxon Mobil http://www.exxonmobil.com 2 3 Berkshire Hathaway http://www.berkshirehathaway.com 3 4 Apple http://www.apple.com 4 5 UnitedHealth Group http://www.unitedhealthgroup.com","title":"Manipulate it into a tabular structure - explore the schema"},{"location":"section-2-HTML-based-scraping/#store-it-in-the-appropriate-format-csv-tsv-and-export-the-results","text":"The dataframe can now be stored as a csv file. Pandas has a 'to_csv' method which can be used to save the data into the file ' 1 fortune_500_df . to_csv ( './fortune_500_companies.csv' , index = False ) 1","title":"Store it in the appropriate format - CSV, TSV and export the results"},{"location":"section-3-API-based-scraping/","text":"Web API based scraping \u00b6 A brief introduction to APIs \u00b6 In this section, we will take a look at an alternative way to gather data than the previous pattern based, HTML scraping. Sometimes websites offer an API (or Application Programming Interface) as a service which provides a high level interface to directly retrieve data from their repositories or databases at the backend. From wikipedia, An API is typically defined as a set of specifications, such as Hypertext Transfer Protocol (HTTP) request messages, along with a definition of the structure of response messages, usually in an Extensible Markup Language (XML) or JavaScript Object Notation (JSON) format. They typically tend to be URL endpoints (to be fired as requests) that need to be modified based on our requirements (what we desire in the response body) which then returns some a payload (data) within the response, formatted as either JSON, XML or HTML. A popular web architecture style called REST (or representational state transfer) allows users to interact with web services via GET and POST calls (two most commonly used). An API in the context of web scraping would be : - Requests (through Hypertext Transfer Protocol HTTP - Headers talk more here! E.g. For example, Twitter's REST API allows developers to access core Twitter data and the Search API provides methods for developers to interact with Twitter Search and trends data. https://en.wikipedia.org/w/api.php There are primarily two ways to use APIs : - Through the command terminal using URL endpoints, or - Through programming language specific wrappers For e.g. Tweepy is a famous python wrapper for Twitter API whereas twurl is a command line interface (CLI) tool but both can achieve the same outcomes. Here we focus on the latter approach and will use a Python library (a wrapper) called wptools based around the MediaWiki API. One advantage of using official APIs is that they are usually compliant of the terms of service (ToS) of a particular service that researchers are looking to gather data from. However, third-party libraries or packages which claim to provide more throughput than the official APIs (rate limits, number of requests/sec) generally operate in a gray area as they tend to violate ToS. Wikipedia API \u00b6 Let's say we want to gather some additional data about the Fortune 500 companies and since wikipedia is a rich source for data we decide to use the MediaWiki API to scrape this data. One very good place to start would be to look at the infoboxes (as wikipedia defines them) of articles corresponsing to each company on the list. They essentially contain a wealth of metadata about a particular entity the article belongs to which in our case is a company. For e.g. consider the wikipedia article for walmart (https://en.wikipedia.org/wiki/Walmart) which includes the following infobox : As we can see from above, the infoboxes could provide us with a lot of valuable information such as : - Year of founding - Industry - Founder(s) - Products - Services - Operating income - Net income - Total assets - Total equity - Number of employees etc Although we expect this data to be fairly organized, it would require some post-processing which we will tackle in our next section. We pick a subset of our data and focus only on the top 20 of the Fortune 500 from the full list. Let's begin by installing some of libraries we will use for this excercise as follows, 1 2 3 4 5 # sudo apt install libcurl4-openssl-dev libssl-dev ! pip install wptools ! pip install wikipedia # pip install pandas ! pip install wordcloud Importing the same, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import json import wptools import itertools import wikipedia import pandas as pd from pathlib import Path from wordcloud import WordCloud import matplotlib.pyplot as plt from IPython.display import Image % matplotlib inline plt . style . use ( 'ggplot' ) # setting the style to ggplot print ( wptools . __version__ ) # checking the installed version 1 0.4.17 Now let's load the data which we scrapped in the previous section as follows, 1 2 3 4 fname = 'fortune_500_companies.csv' # filename path = Path ( '../data/' ) # path to the csv file df = pd . read_csv ( path / fname ) # reading the csv file as a pandas df df . head () # displaying the first 5 rows .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rank company_name company_website 0 1 Walmart http://www.stock.walmart.com 1 2 Exxon Mobil http://www.exxonmobil.com 2 3 Berkshire Hathaway http://www.berkshirehathaway.com 3 4 Apple http://www.apple.com 4 5 UnitedHealth Group http://www.unitedhealthgroup.com Let's focus and select only the top 20 companies from the list as follows, 1 2 3 no_of_companies = 20 # no of companies we are interested df_sub = df . iloc [: no_of_companies , :] . copy () # only selecting the top 20 companies companies = df_sub [ 'company_name' ] . tolist () # converting the column to a list Now let's take a brief look as follows, 1 2 for i , j in enumerate ( companies ): # looping through the list of 20 company print ( ' {} . {} ' . format ( i + 1 , j )) # printing out the same 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 1. Walmart 2. Exxon Mobil 3. Berkshire Hathaway 4. Apple 5. UnitedHealth Group 6. McKesson 7. CVS Health 8. Amazon.com 9. AT&T 10. General Motors 11. Ford Motor 12. AmerisourceBergen 13. Chevron 14. Cardinal Health 15. Costco 16. Verizon 17. Kroger 18. General Electric 19. Walgreens Boots Alliance 20. JPMorgan Chase Getting article names from wiki \u00b6 Right off the bat, as you might have guessed, a tricky issue with matching the top 20 Fortune 500 companies to their wikipedia article names is that both of them would not be exactly the same i.e. they match character for character. To overcome this problem and ensure that we have all the company names and its corresponding wikipedia article, we will use (https://wikipedia.readthedocs.io/en/latest/code.html) to get suggestions for the company names and their equivalent in wikipedia. 1 wiki_search = [{ company : wikipedia . search ( company )} for company in companies ] 1 2 3 4 for idx , company in enumerate ( wiki_search ): for i , j in company . items (): print ( ' {} . {} : \\n {} ' . format ( idx + 1 , i , ', ' . join ( j ))) print ( ' \\n ' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 1. Walmart : Walmart, Criticism of Walmart, History of Walmart, Walmarting, Walmart Canada, Walmart Labs, People of Walmart, List of Walmart brands, Walmart (disambiguation), Walmart Watch 2. Exxon Mobil : ExxonMobil, Exxon, ExxonMobil climate change controversy, Mobil, ExxonMobil Building, 2020 Qatar ExxonMobil Open, Darren Woods, ExxonMobil Tower, Exxon Valdez oil spill, Exxon Valdez 3. Berkshire Hathaway : Berkshire Hathaway, List of assets owned by Berkshire Hathaway, Berkshire Hathaway Energy, Berkshire Hathaway Assurance, Berkshire Hathaway GUARD Insurance Companies, List of Berkshire Hathaway publications, Warren Buffett, Ajit Jain, Berkshire Hathaway Travel Protection, The World's Billionaires 4. Apple : Apple, Apple Inc., Apple (disambiguation), IPhone, Apple Music, Apple A13, Apple TV, Apple ID, Apple Watch, Apple Records 5. UnitedHealth Group : UnitedHealth Group, Pharmacy benefit management, Optum, List of largest companies in the United States by revenue, William W. McGuire, Golden Rule Insurance Company, Stephen J. Hemsley, Amelia Warren Tyagi, PacifiCare Health Systems, Gail Koziara Boudreaux 6. McKesson : McKesson Corporation, DeRay Mckesson, Malcolm McKesson, McKesson & Robbins scandal (1938), Celesio, McKesson Plaza, John Hammergren, McKesson (disambiguation), Rexall Pharmacy Group, Coindre Hall 7. CVS Health : CVS Health, CVS Pharmacy, CVS Caremark, CVS, Pharmacy benefit management, Larry Merlo, CVS Health Charity Classic, Helena Foulkes, MinuteClinic, List of largest companies by revenue 8. Amazon.com : Amazon (company), History of Amazon, List of Amazon products and services, Amazon Web Services, List of original programs distributed by Amazon, .amazon, Criticism of Amazon, Amazon.ae, Amazon S3, List of mergers and acquisitions by Amazon 9. AT&T : AT&T, AT&T Mobility, AT&T Corporation, AT&T TV, T, T & T Supermarket, AT&T Stadium, AT&T Communications, T-54/T-55, AT&T Mexico 10. General Motors : General Motors, History of General Motors, List of General Motors factories, General Motors India, General Motors EV1, General Motors Canada, General Motors Vortec engine, GMC (automobile), General Motors Chapter 11 reorganization, General Motors Firebird 11. Ford Motor : Ford Motor Company, History of Ford Motor Company, Henry Ford, Lincoln Motor Company, Ford Trimotor, Edsel Ford, Ford of Britain, Ford Germany, Henry Ford II, Ford Motor Argentina 12. AmerisourceBergen : AmerisourceBergen, Steven H. Collis, List of largest companies by revenue, List of largest companies in the United States by revenue, Family Pharmacy, Ornella Barra, Good Neighbor Pharmacy, Cardinal Health, Michael DiCandilo, PharMerica 13. Chevron : Chevron Corporation, Chevron, Chevron (insignia), Chevron Cars, Philip Chevron, Wound Chevron, Chevron (geology), Chevron Engineering, Chevron Cars Ltd, Chevron U.S.A., Inc. v. Natural Resources Defense Council, Inc. 14. Cardinal Health : Cardinal Health, Cardinal, Catalent, Cordis (medical), Robert D. Walter, List of largest companies by revenue, George S. Barrett, Pyxis Corporation, List of largest Central Ohio employers, List of largest companies in the United States by revenue 15. Costco : Costco, Costco bear, Warehouse club, Price Club, Coca-Cola, American Express, Rotisserie chicken, James Sinegal, Most-Favoured-Customer Clause, Sol Price 16. Verizon : Verizon Communications, Verizon Wireless, Verizon Fios, Verizon Media, Verizon Delaware, Verizon Business, Verizon Center, Verizon Building, Verizon Pennsylvania, Verizon Hum 17. Kroger : Kroger, Murder Kroger, Bernard Kroger, Michael Kroger, John Kroger, Jeffersontown Kroger shooting, Uwe Kr\u00f6ger, Chad Kroeger, Smith's Food and Drug, Kroger (disambiguation) 18. General Electric : General Electric, General Electric GEnx, General Electric Building, General Electric CF6, General Electric Theater, General Electric GE9X, General Electric Company, General Electric GE90, General Electric LM6000, General Electric CF34 19. Walgreens Boots Alliance : Walgreens Boots Alliance, Alliance Boots, Walgreens, Boots (company), Alliance Healthcare, Stefano Pessina, Boots Opticians, Ornella Barra, Gregory Wasson, James A. Skinner 20. JPMorgan Chase : JPMorgan Chase, Chase Bank, 2012 JPMorgan Chase trading loss, JPMorgan Chase Tower (Houston), 2014 JPMorgan Chase data breach, JPMorgan Chase Building (San Francisco), JPMorgan Corporate Challenge, Chase Tower (Dallas), 270 Park Avenue, Chase Paymentech 1 2 most_probable = [( company , wiki_search [ i ][ company ][ 0 ]) for i , company in enumerate ( companies )] most_probable 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [('Walmart', 'Walmart'), ('Exxon Mobil', 'ExxonMobil'), ('Berkshire Hathaway', 'Berkshire Hathaway'), ('Apple', 'Apple'), ('UnitedHealth Group', 'UnitedHealth Group'), ('McKesson', 'McKesson Corporation'), ('CVS Health', 'CVS Health'), ('Amazon.com', 'Amazon (company)'), ('AT&T', 'AT&T'), ('General Motors', 'General Motors'), ('Ford Motor', 'Ford Motor Company'), ('AmerisourceBergen', 'AmerisourceBergen'), ('Chevron', 'Chevron Corporation'), ('Cardinal Health', 'Cardinal Health'), ('Costco', 'Costco'), ('Verizon', 'Verizon Communications'), ('Kroger', 'Kroger'), ('General Electric', 'General Electric'), ('Walgreens Boots Alliance', 'Walgreens Boots Alliance'), ('JPMorgan Chase', 'JPMorgan Chase')] 1 2 companies = [ x [ 1 ] for x in most_probable ] companies 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ['Walmart', 'ExxonMobil', 'Berkshire Hathaway', 'Apple', 'UnitedHealth Group', 'McKesson Corporation', 'CVS Health', 'Amazon (company)', 'AT&T', 'General Motors', 'Ford Motor Company', 'AmerisourceBergen', 'Chevron Corporation', 'Cardinal Health', 'Costco', 'Verizon Communications', 'Kroger', 'General Electric', 'Walgreens Boots Alliance', 'JPMorgan Chase'] For Apple , lets manually replace it with Apple Inc. as follows, 1 2 companies [ companies . index ( 'Apple' )] = 'Apple Inc.' print ( companies ) 1 ['Walmart', 'ExxonMobil', 'Berkshire Hathaway', 'Apple Inc.', 'UnitedHealth Group', 'McKesson Corporation', 'CVS Health', 'Amazon (company)', 'AT&T', 'General Motors', 'Ford Motor Company', 'AmerisourceBergen', 'Chevron Corporation', 'Cardinal Health', 'Costco', 'Verizon Communications', 'Kroger', 'General Electric', 'Walgreens Boots Alliance', 'JPMorgan Chase'] Note : Wiki data dump link (last updated 2015) : https://old.datahub.io/dataset/wikidata wptools \u00b6 https://github.com/siznax/wptools/wiki/Data-captured 1 2 3 page = wptools . page ( 'Walmart' ) page . get_parse () page . get_wikidata () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 en.wikipedia.org (parse) Walmart en.wikipedia.org (imageinfo) File:Walmart store exterior 5266815680.jpg Walmart (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Walmart s... infobox: <dict(30)> name, logo, logo_caption, image, image_size,... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:W... pageid: 33589 parsetree: <str(346504)> <root><template><title>about</title><pa... requests: <list(2)> parse, imageinfo title: Walmart wikibase: Q483551 wikidata_url: https://www.wikidata.org/wiki/Q483551 wikitext: <str(274081)> {{about|the retail chain|other uses}}{{p... } www.wikidata.org (wikidata) Q483551 www.wikidata.org (labels) Q180816|Q219635|P18|Q478758|Q10382887|Q... www.wikidata.org (labels) P740|Q54862513|P966|P3500|Q6383259|Q694... www.wikidata.org (labels) Q818364|P6160|P1278|P3347|Q17343056|P37... en.wikipedia.org (imageinfo) File:Walmart Home Office.jpg Walmart (en) data { aliases: <list(5)> Wal-Mart, Wal Mart, Wal-Mart Stores, Inc., Wa... claims: <dict(63)> P112, P946, P373, P31, P856, P910, P159, P414... description: U.S. discount retailer based in Arkansas image: <list(2)> {'kind': 'parse-image', 'file': 'File:Walmart s... infobox: <dict(30)> name, logo, logo_caption, image, image_size,... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:W... label: Walmart labels: <dict(116)> Q180816, Q219635, P18, Q478758, Q10382887, Q... modified: <dict(1)> wikidata pageid: 33589 parsetree: <str(346504)> <root><template><title>about</title><pa... requests: <list(7)> parse, imageinfo, wikidata, labels, labels, ... title: Walmart what: retail chain wikibase: Q483551 wikidata: <dict(63)> founded by (P112), ISIN (P946), Commons cat... wikidata_pageid: 455133 wikidata_url: https://www.wikidata.org/wiki/Q483551 wikitext: <str(274081)> {{about|the retail chain|other uses}}{{p... } <wptools.page.WPToolsPage at 0x7f1fc48660f0> 1 page . data . keys () 1 dict_keys(['requests', 'iwlinks', 'pageid', 'wikitext', 'parsetree', 'infobox', 'title', 'wikibase', 'wikidata_url', 'image', 'labels', 'wikidata', 'wikidata_pageid', 'aliases', 'modified', 'description', 'label', 'claims', 'what']) Alternatively, 1 page . data [ 'wikidata' ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 {'founded by (P112)': 'Sam Walton (Q497827)', 'ISIN (P946)': 'US9311421039', 'Commons category (P373)': 'Walmart', 'instance of (P31)': ['retail chain (Q507619)', 'enterprise (Q6881511)'], 'official website (P856)': 'https://www.walmart.com', \"topic's main category (P910)\": 'Category:Walmart (Q6383259)', 'headquarters location (P159)': ['Bentonville (Q818364)', 'Arkansas (Q1612)'], 'stock exchange (P414)': 'New York Stock Exchange (Q13677)', 'subsidiary (P355)': [\"Sam's Club (Q1972120)\", 'Massmart (Q3297791)', 'Walmart Canada (Q1645718)', 'Walmart Chile (Q5283104)', 'Walmart de M\u00e9xico y Centroam\u00e9rica (Q1064887)', 'Seiyu Group (Q3108542)', 'Asda (Q297410)', 'Walmart Labs (Q3816562)', 'Walmart (Q30338489)', 'M\u00e1s Club (Q6949810)', 'L\u00edder (Q6711261)', 'Hypermart USA (Q16845747)', 'Amigo Supermarkets (Q4746234)', 'Walmart Neighborhood Market (Q7963529)', 'Asda Mobile (Q4804093)', 'Marketside (Q6770960)', 'Vudu (Q5371838)', 'Walmart Nicaragua (Q22121904)'], 'owned by (P127)': ['Walton Enterprises (Q17343056)', 'State Street Corporation (Q2037125)', 'The Vanguard Group (Q849363)', 'BlackRock (Q219635)'], 'VIAF ID (P214)': '128951275', 'Freebase ID (P646)': '/m/0841v', 'inception (P571)': '+1962-07-02T00:00:00Z', 'industry (P452)': ['retail (Q126793)', 'retail chain (Q507619)', 'discount store (Q261428)'], 'chairperson (P488)': ['Doug McMillon (Q16196595)', 'Greg Penner (Q20177269)'], 'motto text (P1451)': 'Save money. Live better.', 'Facebook ID (P2013)': 'walmart', 'Twitter username (P2002)': 'Walmart', 'part of (P361)': ['S&P 500 (Q242345)', 'Dow Jones Industrial Average (Q180816)'], 'image (P18)': 'Walmart Home Office.jpg', 'location of formation (P740)': 'Rogers (Q79497)', 'country (P17)': 'United States of America (Q30)', 'legal form (P1454)': 'public company (Q891723)', 'named after (P138)': 'Sam Walton (Q497827)', 'total revenue (P2139)': [{'amount': '+482130000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+482130500000', 'lowerBound': '+482129500000'}, {'amount': '+485651000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+485651500000', 'lowerBound': '+485650500000'}, {'amount': '+476294000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+476294500000', 'lowerBound': '+476293500000'}, {'amount': '+468651000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+468651500000', 'lowerBound': '+468650500000'}, {'amount': '+446509000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+446509500000', 'lowerBound': '+446508500000'}, {'amount': '+485873000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}], 'net profit (P2295)': [{'amount': '+14694000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+14694500000', 'lowerBound': '+14693500000'}, {'amount': '+16182000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+16182500000', 'lowerBound': '+16181500000'}, {'amount': '+15918000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+15918500000', 'lowerBound': '+15917500000'}, {'amount': '+16963000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+16963500000', 'lowerBound': '+16962500000'}, {'amount': '+15734000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+15734500000', 'lowerBound': '+15733500000'}, {'amount': '+13643000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}], 'total assets (P2403)': [{'amount': '+199581000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+199581500000', 'lowerBound': '+199580500000'}, {'amount': '+203490000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+203490500000', 'lowerBound': '+203489500000'}, {'amount': '+204541000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+204541500000', 'lowerBound': '+204540500000'}, {'amount': '+202910000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+202910500000', 'lowerBound': '+202909500000'}, {'amount': '+193120000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+193120500000', 'lowerBound': '+193119500000'}, {'amount': '+198825000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}], 'employees (P1128)': {'amount': '+2300000', 'unit': '1'}, 'Legal Entity Identifier (P1278)': 'Y87794H0US1R65VBXU25', 'Instagram username (P2003)': 'walmart', 'Google+ ID (P2847)': '111852759168797891317', 'chief executive officer (P169)': ['Doug McMillon (Q16196595)', 'Mike Duke (Q1933118)', 'Lee Scott (Q478758)', 'David Glass (Q5234167)', 'Sam Walton (Q497827)'], 'Quora topic ID (P3417)': 'Walmart-company', 'Justia Patents company ID (P3875)': 'wal-mart', 'logo image (P154)': 'Walmart logo.svg', 'IPv4 routing prefix (P3761)': '156.94.0.0/16', 'ISNI (P213)': '0000 0004 0616 1876', 'operating income (P3362)': [{'amount': '+22764000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}, {'amount': '+24105000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}, {'amount': '+27147000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}], 'website account on (P553)': 'WeChat (Q283233)', 'PermID (P3347)': '4295905298', 'Encyclop\u00e6dia Britannica Online ID (P1417)': 'topic/Wal-Mart', 'GRID ID (P2427)': 'grid.480455.8', 'award received (P166)': 'Public Eye Labour Law Award (Q54862513)', 'Central Index Key (P5531)': '0000104169', 'IdRef ID (P269)': '050771116', 'owner of (P1830)': ['Asda (Q297410)', \"Sam's Club (Q1972120)\", 'Seiyu Group (Q3108542)', 'Bodega Aurrer\u00e1 (Q3365858)', 'Asda Mobile (Q4804093)', 'Bompre\u00e7o (Q4940907)', 'Hayneedle (Q5687056)', 'Mercadorama (Q10328812)', None, 'Jet.com (Q22079907)', '.george (Q26911051)', '.samsclub (Q26972795)', 'Shoes.com (Q46438789)'], 'NKCR AUT ID (P691)': 'osa2010597558', 'Microsoft Academic ID (P6366)': '1330693074', 'market capitalization (P2226)': {'amount': '+239000000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}, 'member of (P463)': 'Linux Foundation (Q858851)', 'Library of Congress authority ID (P244)': 'n90648829', 'MusicBrainz label ID (P966)': 'b3a104e8-eed0-4a3e-aae8-676c6e7ab016', 'ROR ID (P6782)': '04j0gge90', 'Ringgold ID (P3500)': '48990', 'BoardGameGeek game publisher ID (P6160)': '29995', 'total equity (P2137)': {'amount': '+80535000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}, 'DR topic ID (P6849)': 'walmart', 'BBC News topic ID (P6200)': 'ce1qrvlex0et', 'Gran Enciclop\u00e8dia Catalana ID (P1296)': '0256072', 'Downdetector ID (P7306)': 'wal-mart', 'LittleSis organisation ID (P3393)': '1-Walmart', 'WeChat ID (P7650)': 'Walmart_Hyper', 'Pinterest username (P3836)': 'walmart'} 1 2 3 4 wiki_data = [] # attributes of interest contained within the wiki infoboxes features = [ 'founder' , 'location_country' , 'revenue' , 'operating_income' , 'net_income' , 'assets' , 'equity' , 'type' , 'industry' , 'products' , 'num_employees' ] Now lets fetch results for all the companies as follows, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 for company in companies : page = wptools . page ( company ) try : page . get_parse () if page . data [ 'infobox' ] != None : infobox = page . data [ 'infobox' ] data = { feature : infobox [ feature ] if feature in infobox else '' for feature in features } else : data = { feature : '' for feature in features } data [ 'company_name' ] = company wiki_data . append ( data ) except KeyError : pass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 en.wikipedia.org (parse) Walmart en.wikipedia.org (imageinfo) File:Walmart store exterior 5266815680.jpg Walmart (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Walmart s... infobox: <dict(30)> name, logo, logo_caption, image, image_size,... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:W... pageid: 33589 parsetree: <str(346504)> <root><template><title>about</title><pa... requests: <list(2)> parse, imageinfo title: Walmart wikibase: Q483551 wikidata_url: https://www.wikidata.org/wiki/Q483551 wikitext: <str(274081)> {{about|the retail chain|other uses}}{{p... } en.wikipedia.org (parse) ExxonMobil en.wikipedia.org (imageinfo) File:ExxonMobilBuilding.JPG ExxonMobil (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:ExxonMobi... infobox: <dict(29)> name, logo, image, image_caption, type, trad... iwlinks: <list(3)> https://commons.wikimedia.org/wiki/Category:E... pageid: 18848197 parsetree: <str(187433)> <root><template><title>About</title><pa... requests: <list(2)> parse, imageinfo title: ExxonMobil wikibase: Q156238 wikidata_url: https://www.wikidata.org/wiki/Q156238 wikitext: <str(152792)> {{About|Exxon Mobil Corp|the company's s... } en.wikipedia.org (parse) Berkshire Hathaway Berkshire Hathaway (en) data { image: <list(0)> infobox: <dict(24)> name, former_name, logo, image, image_captio... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:B... pageid: 314333 parsetree: <str(101434)> <root><template><title>short descriptio... requests: <list(1)> parse title: Berkshire Hathaway wikibase: Q217583 wikidata_url: https://www.wikidata.org/wiki/Q217583 wikitext: <str(86730)> {{short description|American multinationa... } en.wikipedia.org (parse) Apple Inc. en.wikipedia.org (imageinfo) File:Apple park cupertino 2019.jpg Apple Inc. (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Apple par... infobox: <dict(36)> name, logo, logo_size, image, image_size, im... iwlinks: <list(8)> https://commons.wikimedia.org/wiki/Special:Se... pageid: 856 parsetree: <str(403082)> <root><template><title>Redirect</title>... requests: <list(2)> parse, imageinfo title: Apple Inc. wikibase: Q312 wikidata_url: https://www.wikidata.org/wiki/Q312 wikitext: <str(321377)> {{Redirect|Apple (company)|other compani... } en.wikipedia.org (parse) UnitedHealth Group UnitedHealth Group (en) data { infobox: <dict(17)> name, logo, type, traded_as, founder, key_pe... pageid: 1845551 parsetree: <str(86142)> <root><template><title>Redirect</title><... requests: <list(1)> parse title: UnitedHealth Group wikibase: Q2103926 wikidata_url: https://www.wikidata.org/wiki/Q2103926 wikitext: <str(73971)> {{Redirect|UnitedHealthcare|the cycling t... } en.wikipedia.org (parse) McKesson Corporation McKesson Corporation (en) data { infobox: <dict(19)> name, logo, type, traded_as, founder, locati... pageid: 1041603 parsetree: <str(38152)> <root><template><title>Redirect</title><... requests: <list(1)> parse title: McKesson Corporation wikibase: Q570473 wikidata_url: https://www.wikidata.org/wiki/Q570473 wikitext: <str(30274)> {{Redirect|McKesson}}{{short description|... } en.wikipedia.org (parse) CVS Health CVS Health (en) data { infobox: <dict(28)> name, logo, logo_size, former_name, type, tr... pageid: 10377597 parsetree: <str(69373)> <root><template><title>short description... requests: <list(1)> parse title: CVS Health wikibase: Q624375 wikidata_url: https://www.wikidata.org/wiki/Q624375 wikitext: <str(54045)> {{short description|American healthcare c... } en.wikipedia.org (parse) Amazon (company) en.wikipedia.org (imageinfo) File:Seattle Spheres on May 10, 2018.jpg Amazon (company) (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Seattle S... infobox: <dict(33)> name, logo, logo_size, image, image_size, im... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:A... pageid: 90451 parsetree: <str(153139)> <root><template><title>pp</title><part>... requests: <list(2)> parse, imageinfo title: Amazon (company) wikibase: Q3884 wikidata_url: https://www.wikidata.org/wiki/Q3884 wikitext: <str(116580)> {{pp|small=yes}}{{short description|Amer... } en.wikipedia.org (parse) AT&T en.wikipedia.org (imageinfo) File:AT&THQDallas.jpg AT&T (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:AT&THQDal... infobox: <dict(27)> name, logo, logo_size, image, image_size, im... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:AT%26T pageid: 17555269 parsetree: <str(130159)> <root><template><title>about</title><pa... requests: <list(2)> parse, imageinfo title: AT&T wikibase: Q35476 wikidata_url: https://www.wikidata.org/wiki/Q35476 wikitext: <str(104995)> {{about|the company known as AT&T since ... } en.wikipedia.org (parse) General Motors en.wikipedia.org (imageinfo) File:RenCen.JPG General Motors (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:RenCen.JP... infobox: <dict(29)> name, former_name, logo, logo_size, image, i... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:G... pageid: 12102 parsetree: <str(190450)> <root><template><title>short descriptio... requests: <list(2)> parse, imageinfo title: General Motors wikibase: Q81965 wikidata_url: https://www.wikidata.org/wiki/Q81965 wikitext: <str(150285)> {{short description|American automotive ... } en.wikipedia.org (parse) Ford Motor Company en.wikipedia.org (imageinfo) File:FordGlassHouse.jpg Ford Motor Company (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:FordGlass... infobox: <dict(27)> name, logo, image, image_size, image_caption... iwlinks: <list(8)> https://commons.wikimedia.org/wiki/Category:F... pageid: 30433662 parsetree: <str(193764)> <root><template><title>Redirect</title>... requests: <list(2)> parse, imageinfo title: Ford Motor Company wikibase: Q44294 wikidata_url: https://www.wikidata.org/wiki/Q44294 wikitext: <str(157653)> {{Redirect|Ford}}{{pp-semi-indef}}{{pp-m... } en.wikipedia.org (parse) AmerisourceBergen AmerisourceBergen (en) data { infobox: <dict(17)> name, logo, type, traded_as, foundation, loc... pageid: 1445945 parsetree: <str(16501)> <root><template><title>short description... requests: <list(1)> parse title: AmerisourceBergen wikibase: Q470156 wikidata_url: https://www.wikidata.org/wiki/Q470156 wikitext: <str(11755)> {{short description|American healthcare c... } en.wikipedia.org (parse) Chevron Corporation Chevron Corporation (en) data { image: <list(0)> infobox: <dict(24)> name, logo, logo_size, logo_caption, image, ... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:C... pageid: 284749 parsetree: <str(120598)> <root><template><title>short descriptio... requests: <list(1)> parse title: Chevron Corporation wikibase: Q319642 wikidata_url: https://www.wikidata.org/wiki/Q319642 wikitext: <str(97793)> {{short description|American multinationa... } en.wikipedia.org (parse) Cardinal Health Cardinal Health (en) data { infobox: <dict(17)> name, logo, type, traded_as, industry, found... pageid: 1041632 parsetree: <str(32814)> <root><template><title>Infobox company</... requests: <list(1)> parse title: Cardinal Health wikibase: Q902397 wikidata_url: https://www.wikidata.org/wiki/Q902397 wikitext: <str(25715)> {{Infobox company| name = Cardinal Health... } en.wikipedia.org (parse) Costco en.wikipedia.org (imageinfo) File:Costcoheadquarters.jpg Costco (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Costcohea... infobox: <dict(35)> name, logo, logo_caption, image, image_size,... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:Costco pageid: 446056 parsetree: <str(97750)> <root><template><title>Distinguish</titl... requests: <list(2)> parse, imageinfo title: Costco wikibase: Q715583 wikidata_url: https://www.wikidata.org/wiki/Q715583 wikitext: <str(71853)> {{Distinguish|COSCO|Cosco (India) Limited... } en.wikipedia.org (parse) Verizon Communications en.wikipedia.org (imageinfo) File:Verizon Building (8156005279).jpg Verizon Communications (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Verizon B... infobox: <dict(30)> name, logo, image, image_caption, former_nam... iwlinks: <list(3)> https://commons.wikimedia.org/wiki/Category:T... pageid: 18619278 parsetree: <str(147152)> <root><template><title>short descriptio... requests: <list(2)> parse, imageinfo title: Verizon Communications wikibase: Q467752 wikidata_url: https://www.wikidata.org/wiki/Q467752 wikitext: <str(124812)> {{short description|American communicati... } en.wikipedia.org (parse) Kroger en.wikipedia.org (imageinfo) File:Cincinnati-kroger-building.jpg Kroger (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Cincinnat... infobox: <dict(24)> name, logo, image, image_caption, type, trad... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:Kroger pageid: 367762 parsetree: <str(121519)> <root><template><title>Use American Eng... requests: <list(2)> parse, imageinfo title: Kroger wikibase: Q153417 wikidata_url: https://www.wikidata.org/wiki/Q153417 wikitext: <str(102176)> {{Use American English|date = August 201... } en.wikipedia.org (parse) General Electric General Electric (en) data { infobox: <dict(20)> name, logo, type, traded_as, ISIN, industry,... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:G... pageid: 12730 parsetree: <str(162543)> <root><template><title>redirect</title>... requests: <list(1)> parse title: General Electric wikibase: Q54173 wikidata_url: https://www.wikidata.org/wiki/Q54173 wikitext: <str(137546)> {{redirect|GE}}{{distinguish|text=the fo... } en.wikipedia.org (parse) Walgreens Boots Alliance Walgreens Boots Alliance (en) data { infobox: <dict(29)> name, logo, logo_size, type, traded_as, pred... pageid: 44732533 parsetree: <str(32631)> <root><template><title>Use mdy dates</ti... requests: <list(1)> parse title: Walgreens Boots Alliance wikibase: Q18712620 wikidata_url: https://www.wikidata.org/wiki/Q18712620 wikitext: <str(25099)> {{Use mdy dates|date=October 2019}}{{shor... } en.wikipedia.org (parse) JPMorgan Chase en.wikipedia.org (imageinfo) File:383 Madison Ave Bear Stearns C ... JPMorgan Chase (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:383 Madis... infobox: <dict(31)> name, logo, image, image_caption, type, trad... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:J... pageid: 231001 parsetree: <str(137921)> <root><template><title>About</title><pa... requests: <list(2)> parse, imageinfo title: JPMorgan Chase wikibase: Q192314 wikidata_url: https://www.wikidata.org/wiki/Q192314 wikitext: <str(112397)> {{About|JPMorgan Chase & Co|its main sub... } 1 wiki_data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 [{'founder': '[[Sam Walton]]', 'location_country': 'U.S.', 'revenue': '{{increase}} {{US$|514.405 billion|link|=|yes}} (2019)', 'operating_income': '{{increase}} {{US$|21.957 billion}} (2019)', 'net_income': '{{decrease}} {{US$|6.67 billion}} (2019)', 'assets': '{{increase}} {{US$|219.295 billion}} (2019)', 'equity': '{{decrease}} {{US$|79.634 billion}} (2019)', 'type': '[[Public company|Public]]', 'industry': '[[Retail]]', 'products': '{{hlist|Electronics|Movies and music|Home and furniture|Home improvement|Clothing|Footwear|Jewelry|Toys|Health and beauty|Pet supplies|Sporting goods and fitness|Auto|Photo finishing|Craft supplies|Party supplies|Grocery}}', 'num_employees': '{{plainlist|\\n* 2.2|nbsp|million, Worldwide (2018)|ref| name=\"xbrlus_1\" |\\n* 1.5|nbsp|million, U.S. (2017)|ref| name=\"Walmart\"|{{cite web |url = http://corporate.walmart.com/our-story/locations/united-states |title = Walmart Locations Around the World \u2013 United States |publisher = |url-status=live |archiveurl = https://web.archive.org/web/20150926012456/http://corporate.walmart.com/our-story/locations/united-states |archivedate = September 26, 2015 |df = mdy-all }}|</ref>|\\n* 700,000, International}} {{nbsp}} million, Worldwide (2018) * 1.5 {{nbsp}} million, U.S. (2017) * 700,000, International', 'company_name': 'Walmart'}, {'founder': '', 'location_country': '', 'revenue': '{{Nowrap|Increase| |US$|279.3 billion|link|=|yes|ref| name=\"201310K\"|[https://corporate.exxonmobil.com/-/media/global/files/annual-report/2018-financial-and-operating-review.pdf EXXON MOBIL CORPORATION Form 10-K] {{Webarchive|url=https://web.archive.org/web/20190404044022/https://corporate.exxonmobil.com/-/media/global/files/annual-report/2018-financial-and-operating-review.pdf |date=April 4, 2019 }}, \\'\\'Google Finance\\'\\', March 21, 2019|</ref>}} {{Increase}} {{US$|279.3 billion|link|=|yes}}', 'operating_income': '{{Nowrap|Increase| |US$|21.53 billion|ref| name=\"201310K\"}} {{Increase}} {{US$|21.53 billion}}', 'net_income': '{{Nowrap|Increase| |US$|20.84 billion|ref| name=\"201310K\"}} {{Increase}} {{US$|20.84 billion}}', 'assets': '{{Nowrap|Decrease| |US$|346.2 billion|ref| name=\"201310K\"}} {{Decrease}} {{US$|346.2 billion}}', 'equity': '{{Nowrap|Increase| |US$|191.8 billion|ref| name=\"201310K\"}} {{Increase}} {{US$|191.8 billion}}', 'type': '[[Public company|Public]]', 'industry': '[[Energy industry|Energy]]: [[Oil and gas industry|Oil and gas]]', 'products': '{{Unbulleted list\\n | [[Crude oil]]\\n | [[Oil products]]\\n | [[Natural gas]]\\n | [[Petrochemical]]s\\n | [[Power generation]]}}', 'num_employees': '71,000', 'company_name': 'ExxonMobil'}, {'founder': '[[Oliver Chace]]<br>[[Warren Buffett]] (Modern era)', 'location_country': '', 'revenue': '{{increase}} US$247.5 billion (2018)', 'operating_income': '{{Decrease}} US$10.02 billion (2018)', 'net_income': '{{Decrease}} US$4.02 billion (2018)', 'assets': '{{increase}} US$707.8 billion (2018)', 'equity': '{{increase}} US$348.7 billion (2018)', 'type': '[[Public company|Public]]', 'industry': '[[Conglomerate (company)|Conglomerate]]', 'products': '[[Investment|Diversified investments]], [[Insurance#Types|Property & casualty insurance]], [[Public utility|Utilities]], [[Restaurants]], [[Food processing]], [[Aerospace]], [[Toys]], [[Mass media|Media]], [[Automotive industry|Automotive]], [[Sports equipment|Sporting goods]], [[Final good|Consumer products]], [[Internet]], [[Real estate]]', 'num_employees': '{{nowrap|389,373 (2018)}}', 'company_name': 'Berkshire Hathaway'}, {'founder': '', 'location_country': '', 'revenue': '{{Decrease}} {{US$|260.174&nbsp;billion|link|=|yes}}', 'operating_income': '{{Decrease}} {{US$|63.930&nbsp;billion}}', 'net_income': '{{Decrease}} {{US$|55.256&nbsp;billion}}', 'assets': '{{Decrease}} {{US$|338.516&nbsp;billion}}', 'equity': '{{Decrease}} {{US$|90.488&nbsp;billion}}', 'type': '[[Public company|Public]]', 'industry': '{{Unbulleted list | [[Computer hardware]] | [[Computer software]] | [[Consumer electronics]] | [[Cloud computing]] | [[Digital distribution]] | [[Fabless manufacturing|Fabless silicon design]] | [[Semiconductors]] | [[Financial technology]] | [[Artificial intelligence]]}}', 'products': '{{Flatlist|\\n* [[Macintosh]]\\n* [[iPod]]\\n* [[iPhone]]\\n* [[iPad]]\\n* [[Apple Watch]]\\n* [[Apple TV]]\\n* [[HomePod]]\\n* [[macOS]]\\n* [[iOS]]\\n* [[iPadOS]]\\n* [[watchOS]]\\n* [[tvOS]]\\n* [[iLife]]\\n* [[iWork]]\\n* [[Final Cut Pro]]\\n* [[Logic Pro]]\\n* [[GarageBand]]\\n* [[Shazam (application)|Shazam]]\\n* [[Siri]]}}', 'num_employees': '137,000', 'company_name': 'Apple Inc.'}, {'founder': 'Richard T. Burke', 'location_country': '', 'revenue': '{{increase}} $242.155 billion (2019)', 'operating_income': '{{increase}} $17.981 billion (2019)', 'net_income': '{{increase}} $14.239 billion (2019)', 'assets': '{{increase}} $173.889 billion (2019)', 'equity': '{{increase}} $60.436 billion (2019)', 'type': '[[Public company]]', 'industry': '[[Managed health care]]', 'products': '[[Uniprise]], [[Health Care]] [[Service (economics)|Services]], Specialized Care Services, and [[Ingenix]]', 'num_employees': '300,000 (2019)', 'company_name': 'UnitedHealth Group'}, {'founder': 'John McKesson<br>Charles Olcott', 'location_country': '', 'revenue': '{{increase}} {{US$|208.4 billion}} {{small|(2018)}}', 'operating_income': '{{increase}} {{US$|2.921 billion}} {{small|(2018)}}', 'net_income': '{{increase}} {{US$|67 million}} {{small|(2018)}}', 'assets': '{{nowrap|increase| |US$|60.381 billion| |small|(2018)|ref| name=FY}} {{increase}} {{US$|60.381 billion}} {{small|(2018)}}', 'equity': '{{decrease}} {{US$|10.057 billion}} {{small|(2018)}}', 'type': '[[Public company|Public]]', 'industry': '[[Healthcare]]', 'products': '[[Pharmaceuticals]]<br>[[Medical technology]]<br>[[Health care services]]', 'num_employees': '~78,000 {{small|(2018)}}', 'company_name': 'McKesson Corporation'}, {'founder': '', 'location_country': '', 'revenue': '{{ublist|class|=|nowrap|increase| |US$|194.579 billion| (2018)|US$|184.786 billion| (2017)}} {{increase}} {{US$|194.579 billion}} (2018) {{US$|184.786 billion}} (2017)', 'operating_income': '{{ublist|class|=|nowrap|decrease| |US$|4.021 billion| (2018)|US$|9.538 billion| (2017)}} {{decrease}} {{US$|4.021 billion}} (2018) {{US$|9.538 billion}} (2017)', 'net_income': '{{ublist|class|=|nowrap|decrease| |US$|-596 million| (2018)|US$|6.623 billion| (2017)}} {{decrease}} {{US$|-596 million}} (2018) {{US$|6.623 billion}} (2017)', 'assets': '{{increase}} {{US$|196.456 billion}}', 'equity': '{{increase}} {{US$|58.225 billion}}', 'type': '[[Public company|Public]]', 'industry': '{{flat list|\\n* [[Retail]]\\n* [[health care]]}}', 'products': '', 'num_employees': '295,000', 'company_name': 'CVS Health'}, {'founder': '[[Jeff Bezos]]', 'location_country': '', 'revenue': '{{increase}} {{US$|232.887 billion|link|=|yes}}', 'operating_income': '{{increase}} {{US$|12.421 billion}}', 'net_income': '{{increase}} {{US$|10.073 billion}}', 'assets': '{{decrease}} {{US$|162.648 billion}}', 'equity': '{{decrease}} {{US$|43.549 billion}}', 'type': '[[Public company|Public]]', 'industry': '{{plainlist|\\n* [[Cloud computing]]\\n* [[E-commerce]]\\n* [[Artificial intelligence]]\\n* [[Consumer electronics]]\\n* [[Digital distribution]]\\n* [[Grocery stores]]}}', 'products': '{{Hlist|[[Amazon Echo]]|[[Amazon Fire tablet|Amazon Fire]]|[[Amazon Fire TV]]|[[Fire OS|Amazon Fire OS]]|[[Amazon Kindle]]}}', 'num_employees': '{{increase}} 750,000 (2019)', 'company_name': 'Amazon (company)'}, {'founder': '', 'location_country': '', 'revenue': '{{increase}} {{US$|link|=|yes}} 170.756 billion (2018)', 'operating_income': '{{increase}} {{US$|link|=|yes}} 26.096 billion (2018)', 'net_income': '{{increase}} {{US$|link|=|yes}} 19.953 billion (2018)', 'assets': '{{increase}} {{US$|link|=|yes}} 531 billion (2018)', 'equity': '{{increase}} {{US$|link|=|yes}} 193.884 billion (2018)', 'type': '[[Public company|Public]]', 'industry': '{{Unbulleted list|[[Telecommunications industry|Telecommunications]]|[[Technology company|Technology]]|[[Mass media]]|[[Entertainment]]}}', 'products': '{{Hlist|[[Satellite television]]|[[Landline|Fixed-line telephones]]|[[Mobile phone|Mobile telephones]]|[[Internet service provider|Internet services]]|[[Broadband]]|[[Digital television]]|[[Home security]]|[[IPTV]]|[[Over-the-top media services|OTT services]]|[[Network security]]|[[Filmmaking|Film production]]|[[Television production]]|[[Cable television]]|[[Pay television]]|[[Publishing]]|[[Podcast]]s|[[Sports management]]|[[News agency]]|[[Video game]]s}}', 'num_employees': '251,840 (2019)', 'company_name': 'AT&T'}, {'founder': '[[William C. Durant]]', 'location_country': 'US', 'revenue': '{{decrease}} [[United States dollar|US$]]147.049 billion {{small|(2018)}}', 'operating_income': '{{decrease}} US$11.783 billion {{small|(2018)}}', 'net_income': '{{increase}} US$8.014 billion {{small|(2018)}}', 'assets': '{{increase}} US$227.339 billion {{small|(2018)}}', 'equity': '{{increase}} US$42.777 billion {{small|(2018)}}', 'type': '[[Public company|Public]]', 'industry': '[[Automotive industry|Automotive]]', 'products': '[[Car|Automobiles]]<br />Automobile parts<br />[[Commercial vehicle]]s', 'num_employees': '170,483 {{small|(December 2018)}}', 'company_name': 'General Motors'}, {'founder': '[[Henry Ford]]', 'location_country': 'U.S.', 'revenue': '{{increase}} {{US$|160.33 billion|link|=|yes}} {{small|(2018)}}', 'operating_income': '{{decrease}} {{US$|3.27 billion}} {{small|(2018)}}', 'net_income': '{{decrease}} {{US$|3.67 billion}} {{small|(2018)}}', 'assets': '{{decrease}} {{US$|256.54 billion}} {{small|(2018)}}', 'equity': '{{decrease}} {{US$|35.93 billion}} {{small|(2018)}}', 'type': '[[Public company|Public]]', 'industry': '[[Automotive industry|Automotive]]', 'products': '{{unbulleted list\\n | [[Car|Automobiles]]\\n | [[Luxury Car|Luxury Vehicles]]\\n | [[Commercial vehicle|Commercial Vehicles]]\\n | [[List of auto parts|Automotive parts]]\\n | [[Pickup trucks]]\\n | [[SUVs]]}}', 'num_employees': '199,000 {{small|(December 2018)}}', 'company_name': 'Ford Motor Company'}, {'founder': '', 'location_country': '', 'revenue': '{{increase}} {{US$|167.93 billion|link|=|yes}} (2018)', 'operating_income': '{{increase}} {{US$|1.44 billion}} (2018)', 'net_income': '{{increase}} {{US$|1.65 billion}} (2018)', 'assets': '{{increase}} {{US$|37.66 billion}} (2018)', 'equity': '{{increase}} {{US$|2.93 billion}} (2018)', 'type': '[[Public company|Public]]', 'industry': '[[Pharmaceutical]]', 'products': '[[Pharmaceutical]]s and [[pharmacy]] services', 'num_employees': '20,000 (2018)', 'company_name': 'AmerisourceBergen'}, {'founder': '', 'location_country': '', 'revenue': '{{increase}} {{US$|158.9 billion|link|=|yes}} {{small|(2018)}}', 'operating_income': '{{increase}} {{US$|15.45 billion}} {{small|(2018)}}', 'net_income': '{{increase}} {{US$|14.82 billion}} {{small|(2018)}}', 'assets': '{{decrease}} {{US$|253.9 billion}} {{small|(2018)}}', 'equity': '{{increase}} {{US$|154.5 billion}} {{small|(2018)}}', 'type': '[[Public company|Public]]', 'industry': '[[Oil and gas industry|Oil and gas]]', 'products': \"[[Petroleum]], [[natural gas]] and other [[petrochemical]]s, ''[[#Marketing brands|See Chevron products]]''\", 'num_employees': '~51,900 {{small|(December 2018)}}', 'company_name': 'Chevron Corporation'}, {'founder': '', 'location_country': '', 'revenue': '{{increase}} [[US$]]136.80 billion {{small|(2018)}}', 'operating_income': '{{decrease}} US$126 million {{small|(2018)}}', 'net_income': '{{decrease}} US$256 million {{small|(2018)}}', 'assets': '{{increase}} US$39.95 billion {{small|(2018)}}', 'equity': '{{increase}} US$6.05 billion {{small|(2018)}}', 'type': '[[Public company|Public]]', 'industry': '[[Pharmaceuticals]]', 'products': 'Medical and pharmaceutical products and services', 'num_employees': '~50,000 {{small|(2018)}}', 'company_name': 'Cardinal Health'}, {'founder': '', 'location_country': 'United States', 'revenue': '{{increase}} {{US$|152.7 billion}}', 'operating_income': '{{increase}} US$4.74 billion (2018)', 'net_income': '{{increase}} US$3.66 billion', 'assets': '{{increase}} US$45.40 billion', 'equity': '{{increase}} US$15.24 billion', 'type': '[[Public company|Public]]', 'industry': '[[Retail]]', 'products': '', 'num_employees': '{{increase}} 254,000', 'company_name': 'Costco'}, {'founder': '', 'location_country': '', 'revenue': '{{increase}} {{US$|130.86 [[1,000,000,000|billion]]|link|=|yes}}', 'operating_income': '{{decrease}} {{US$|22.27 billion}}', 'net_income': '{{decrease}} {{US$|15.52 billion}}', 'assets': '{{increase}} {{US$|264.82 billion}}', 'equity': '{{increase}} {{US$|53.14 billion}}', 'type': '[[Public company|Public]]', 'industry': '{{Plainlist|\\n*[[Telecommunications industry|Telecommunications]]\\n*[[Mass media]]}}', 'products': '{{Plainlist|\\n*[[Cable television]]\\n*[[Landline]]\\n*[[Mobile phone]]\\n*[[Broadband]]\\n*[[Digital television]]\\n*[[IPTV]]\\n*[[Digital Media]]\\n*[[Internet of things|Internet]]\\n*[[Telematics]]}}', 'num_employees': '135,400 (2020)', 'company_name': 'Verizon Communications'}, {'founder': '[[Bernard Kroger]]', 'location_country': 'U.S.', 'revenue': '{{increase}} {{US$|121.16 billion|link|=|yes}} (2019)', 'operating_income': '{{increase}} {{US$|2.67 billion}} (2019)', 'net_income': '{{increase}} {{US$|3.11 billion}} (2019)', 'assets': '{{increase}} {{US$|38.11 billion}} (2019)', 'equity': '{{increase}} {{US$|7.88 billion}} (2019)', 'type': '[[Public company|Public]]', 'industry': '[[Retail]]', 'products': '[[Supercenter]]/[[superstore]],<br>Other specialty, [[supermarket]]', 'num_employees': '453,000 (2019)', 'company_name': 'Kroger'}, {'founder': '', 'location_country': '', 'revenue': '{{nowrap|Increase| [[US$]] 121.615 billion |small|(2018)}} {{Increase}} [[US$]] 121.615 billion {{small|(2018)}}', 'operating_income': '{{nowrap|Decrease| US$ |color|red|&minus;20.717| billion |small|(2018)}} {{Decrease}} US$ {{color|red|&minus;20.717}} billion {{small|(2018)}}', 'net_income': '{{nowrap|Decrease| US$ |color|red|&minus;22.355| billion |small|(2018)}} {{Decrease}} US$ {{color|red|&minus;22.355}} billion {{small|(2018)}}', 'assets': '{{nowrap|Decrease| US$ 309.129 billion |small|(2018)}} {{Decrease}} US$ 309.129 billion {{small|(2018)}}', 'equity': '{{nowrap|Decrease| US$ 30.981 billion |small|(2018)}} {{Decrease}} US$ 30.981 billion {{small|(2018)}}', 'type': '[[Public company|Public]]', 'industry': '[[Conglomerate (company)|Conglomerate]]', 'products': '{{hlist|[[Aircraft engine]]s|[[Electric power distribution|Electrical distribution]]|[[Electric motor]]s|[[Energy]]|[[Finance]]|[[Health care]]|[[Lighting]]|[[Software]]|[[Wind turbine]]s}}', 'num_employees': '283,000 {{small|(2018)}}', 'company_name': 'General Electric'}, {'founder': '', 'location_country': '', 'revenue': \"{{nowrap|increase| |US$|136.9 billion|link|=|yes|ref| name='10-K'|{{cite web|url=https://sec.report/Document/0001618921-19-000069/ |title=Walgreens Boots Alliance Annual Report (Form 10-K) |publisher=[[U.S. Securities and Exchange Commission]]}}|</ref>}} {{increase}} {{US$|136.9 billion|link|=|yes}}\", 'operating_income': '{{decrease}} {{US$|4.9 billion}}', 'net_income': '{{decrease}} {{US$|3.9 billion}}', 'assets': '{{decrease}} {{US$|67.59 billion}}', 'equity': '{{decrease}} {{US$|24.15 billion}}', 'type': '[[Public company|Public]]', 'industry': '[[Pharmaceutical]]<br>[[Retail]]', 'products': '[[Drug store]]<br>[[Pharmacy]]', 'num_employees': '440,000', 'company_name': 'Walgreens Boots Alliance'}, {'founder': '[[Aaron Burr]] (Bank of the Manhattan Company)<br>[[J. P. Morgan|John Pierpont Morgan]]', 'location_country': '', 'revenue': '{{increase}} [[United States dollar|US$]]115.627 [[Billion (short scale)|billion]]', 'operating_income': '{{increase}} [[United States dollar|US$]]44.545 billion', 'net_income': '{{increase}} [[United States dollar|US$]]36.431 billion', 'assets': '{{increase}} [[United States dollar|US$]]2.687 [[trillion]]', 'equity': '{{increase}} [[United States dollar|US$]]261.330 billion', 'type': '[[Public company|Public]]', 'industry': '[[Bank]]ing<br>[[Financial services]]', 'products': '[[Alternative financial service]]s, [[American depositary receipt]]s, [[asset allocation]], [[asset management]], [[Bond (finance)|bond]] trading, [[broker]] services, [[capital market]] services, [[collateralized debt obligation]]s, [[commercial banking]], [[commodity market|commodities]] trading, [[commercial bank]]ing, [[credit card]]s, [[credit default swap]], [[credit derivative]] trading, [[currency exchange]], [[custodian bank]]ing, [[debt settlement]], [[digital banking]], [[estate planning]], [[exchange-traded fund]]s, [[financial analysis]], [[financial market]]s, [[foreign exchange market]], [[futures exchange]], [[hedge fund]]s, [[index fund]]s, [[information processing]], [[institutional investor|institutional investing]], [[insurance]], [[investment bank]]ing, [[Financial capital|investment capital]], [[investment management]], investment [[Portfolio (finance)|portfolios]], [[loan servicing]], [[merchant services]], [[mobile banking]], [[money market]] trading, [[mortgage brokers|mortgage broker]]ing, [[mortgage loan]]s, [[Mortgage-backed security|mortgage\u2013backed securities]], [[mutual fund]]s, [[pension fund]]s, [[prime brokerage]], [[private banking]], [[private equity]], [[remittance]], [[retail banking]], retail [[broker]]age, [[risk management]], [[securities lending]], [[Security (finance)|security]] services, [[stock trader|stock trading]], [[subprime lending]], [[treasury services]], [[trustee]] services, [[underwriting]], [[venture capital]], [[wealth management]], [[wholesale funding]], [[Wholesale mortgage lenders|wholesale mortgage lending]], [[wire transfer]]s', 'num_employees': '{{increase}} 256,981', 'company_name': 'JPMorgan Chase'}] Finally, let's export all the scapped infoboxes as a single JSON file to a convenient location as follows, 1 2 with open ( '../data/infoboxes.json' , 'w' ) as file : json . dump ( wiki_data , file ) Import : 1 2 with open ( '../data/infoboxes.json' , 'r' ) as file : wiki_data = json . load ( file ) References \u00b6 https://phpenthusiast.com/blog/what-is-rest-api 1","title":"API based scraping"},{"location":"section-3-API-based-scraping/#web-api-based-scraping","text":"","title":"Web API based scraping"},{"location":"section-3-API-based-scraping/#a-brief-introduction-to-apis","text":"In this section, we will take a look at an alternative way to gather data than the previous pattern based, HTML scraping. Sometimes websites offer an API (or Application Programming Interface) as a service which provides a high level interface to directly retrieve data from their repositories or databases at the backend. From wikipedia, An API is typically defined as a set of specifications, such as Hypertext Transfer Protocol (HTTP) request messages, along with a definition of the structure of response messages, usually in an Extensible Markup Language (XML) or JavaScript Object Notation (JSON) format. They typically tend to be URL endpoints (to be fired as requests) that need to be modified based on our requirements (what we desire in the response body) which then returns some a payload (data) within the response, formatted as either JSON, XML or HTML. A popular web architecture style called REST (or representational state transfer) allows users to interact with web services via GET and POST calls (two most commonly used). An API in the context of web scraping would be : - Requests (through Hypertext Transfer Protocol HTTP - Headers talk more here! E.g. For example, Twitter's REST API allows developers to access core Twitter data and the Search API provides methods for developers to interact with Twitter Search and trends data. https://en.wikipedia.org/w/api.php There are primarily two ways to use APIs : - Through the command terminal using URL endpoints, or - Through programming language specific wrappers For e.g. Tweepy is a famous python wrapper for Twitter API whereas twurl is a command line interface (CLI) tool but both can achieve the same outcomes. Here we focus on the latter approach and will use a Python library (a wrapper) called wptools based around the MediaWiki API. One advantage of using official APIs is that they are usually compliant of the terms of service (ToS) of a particular service that researchers are looking to gather data from. However, third-party libraries or packages which claim to provide more throughput than the official APIs (rate limits, number of requests/sec) generally operate in a gray area as they tend to violate ToS.","title":"A brief introduction to APIs"},{"location":"section-3-API-based-scraping/#wikipedia-api","text":"Let's say we want to gather some additional data about the Fortune 500 companies and since wikipedia is a rich source for data we decide to use the MediaWiki API to scrape this data. One very good place to start would be to look at the infoboxes (as wikipedia defines them) of articles corresponsing to each company on the list. They essentially contain a wealth of metadata about a particular entity the article belongs to which in our case is a company. For e.g. consider the wikipedia article for walmart (https://en.wikipedia.org/wiki/Walmart) which includes the following infobox : As we can see from above, the infoboxes could provide us with a lot of valuable information such as : - Year of founding - Industry - Founder(s) - Products - Services - Operating income - Net income - Total assets - Total equity - Number of employees etc Although we expect this data to be fairly organized, it would require some post-processing which we will tackle in our next section. We pick a subset of our data and focus only on the top 20 of the Fortune 500 from the full list. Let's begin by installing some of libraries we will use for this excercise as follows, 1 2 3 4 5 # sudo apt install libcurl4-openssl-dev libssl-dev ! pip install wptools ! pip install wikipedia # pip install pandas ! pip install wordcloud Importing the same, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import json import wptools import itertools import wikipedia import pandas as pd from pathlib import Path from wordcloud import WordCloud import matplotlib.pyplot as plt from IPython.display import Image % matplotlib inline plt . style . use ( 'ggplot' ) # setting the style to ggplot print ( wptools . __version__ ) # checking the installed version 1 0.4.17 Now let's load the data which we scrapped in the previous section as follows, 1 2 3 4 fname = 'fortune_500_companies.csv' # filename path = Path ( '../data/' ) # path to the csv file df = pd . read_csv ( path / fname ) # reading the csv file as a pandas df df . head () # displaying the first 5 rows .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } rank company_name company_website 0 1 Walmart http://www.stock.walmart.com 1 2 Exxon Mobil http://www.exxonmobil.com 2 3 Berkshire Hathaway http://www.berkshirehathaway.com 3 4 Apple http://www.apple.com 4 5 UnitedHealth Group http://www.unitedhealthgroup.com Let's focus and select only the top 20 companies from the list as follows, 1 2 3 no_of_companies = 20 # no of companies we are interested df_sub = df . iloc [: no_of_companies , :] . copy () # only selecting the top 20 companies companies = df_sub [ 'company_name' ] . tolist () # converting the column to a list Now let's take a brief look as follows, 1 2 for i , j in enumerate ( companies ): # looping through the list of 20 company print ( ' {} . {} ' . format ( i + 1 , j )) # printing out the same 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 1. Walmart 2. Exxon Mobil 3. Berkshire Hathaway 4. Apple 5. UnitedHealth Group 6. McKesson 7. CVS Health 8. Amazon.com 9. AT&T 10. General Motors 11. Ford Motor 12. AmerisourceBergen 13. Chevron 14. Cardinal Health 15. Costco 16. Verizon 17. Kroger 18. General Electric 19. Walgreens Boots Alliance 20. JPMorgan Chase","title":"Wikipedia API"},{"location":"section-3-API-based-scraping/#getting-article-names-from-wiki","text":"Right off the bat, as you might have guessed, a tricky issue with matching the top 20 Fortune 500 companies to their wikipedia article names is that both of them would not be exactly the same i.e. they match character for character. To overcome this problem and ensure that we have all the company names and its corresponding wikipedia article, we will use (https://wikipedia.readthedocs.io/en/latest/code.html) to get suggestions for the company names and their equivalent in wikipedia. 1 wiki_search = [{ company : wikipedia . search ( company )} for company in companies ] 1 2 3 4 for idx , company in enumerate ( wiki_search ): for i , j in company . items (): print ( ' {} . {} : \\n {} ' . format ( idx + 1 , i , ', ' . join ( j ))) print ( ' \\n ' ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 1. Walmart : Walmart, Criticism of Walmart, History of Walmart, Walmarting, Walmart Canada, Walmart Labs, People of Walmart, List of Walmart brands, Walmart (disambiguation), Walmart Watch 2. Exxon Mobil : ExxonMobil, Exxon, ExxonMobil climate change controversy, Mobil, ExxonMobil Building, 2020 Qatar ExxonMobil Open, Darren Woods, ExxonMobil Tower, Exxon Valdez oil spill, Exxon Valdez 3. Berkshire Hathaway : Berkshire Hathaway, List of assets owned by Berkshire Hathaway, Berkshire Hathaway Energy, Berkshire Hathaway Assurance, Berkshire Hathaway GUARD Insurance Companies, List of Berkshire Hathaway publications, Warren Buffett, Ajit Jain, Berkshire Hathaway Travel Protection, The World's Billionaires 4. Apple : Apple, Apple Inc., Apple (disambiguation), IPhone, Apple Music, Apple A13, Apple TV, Apple ID, Apple Watch, Apple Records 5. UnitedHealth Group : UnitedHealth Group, Pharmacy benefit management, Optum, List of largest companies in the United States by revenue, William W. McGuire, Golden Rule Insurance Company, Stephen J. Hemsley, Amelia Warren Tyagi, PacifiCare Health Systems, Gail Koziara Boudreaux 6. McKesson : McKesson Corporation, DeRay Mckesson, Malcolm McKesson, McKesson & Robbins scandal (1938), Celesio, McKesson Plaza, John Hammergren, McKesson (disambiguation), Rexall Pharmacy Group, Coindre Hall 7. CVS Health : CVS Health, CVS Pharmacy, CVS Caremark, CVS, Pharmacy benefit management, Larry Merlo, CVS Health Charity Classic, Helena Foulkes, MinuteClinic, List of largest companies by revenue 8. Amazon.com : Amazon (company), History of Amazon, List of Amazon products and services, Amazon Web Services, List of original programs distributed by Amazon, .amazon, Criticism of Amazon, Amazon.ae, Amazon S3, List of mergers and acquisitions by Amazon 9. AT&T : AT&T, AT&T Mobility, AT&T Corporation, AT&T TV, T, T & T Supermarket, AT&T Stadium, AT&T Communications, T-54/T-55, AT&T Mexico 10. General Motors : General Motors, History of General Motors, List of General Motors factories, General Motors India, General Motors EV1, General Motors Canada, General Motors Vortec engine, GMC (automobile), General Motors Chapter 11 reorganization, General Motors Firebird 11. Ford Motor : Ford Motor Company, History of Ford Motor Company, Henry Ford, Lincoln Motor Company, Ford Trimotor, Edsel Ford, Ford of Britain, Ford Germany, Henry Ford II, Ford Motor Argentina 12. AmerisourceBergen : AmerisourceBergen, Steven H. Collis, List of largest companies by revenue, List of largest companies in the United States by revenue, Family Pharmacy, Ornella Barra, Good Neighbor Pharmacy, Cardinal Health, Michael DiCandilo, PharMerica 13. Chevron : Chevron Corporation, Chevron, Chevron (insignia), Chevron Cars, Philip Chevron, Wound Chevron, Chevron (geology), Chevron Engineering, Chevron Cars Ltd, Chevron U.S.A., Inc. v. Natural Resources Defense Council, Inc. 14. Cardinal Health : Cardinal Health, Cardinal, Catalent, Cordis (medical), Robert D. Walter, List of largest companies by revenue, George S. Barrett, Pyxis Corporation, List of largest Central Ohio employers, List of largest companies in the United States by revenue 15. Costco : Costco, Costco bear, Warehouse club, Price Club, Coca-Cola, American Express, Rotisserie chicken, James Sinegal, Most-Favoured-Customer Clause, Sol Price 16. Verizon : Verizon Communications, Verizon Wireless, Verizon Fios, Verizon Media, Verizon Delaware, Verizon Business, Verizon Center, Verizon Building, Verizon Pennsylvania, Verizon Hum 17. Kroger : Kroger, Murder Kroger, Bernard Kroger, Michael Kroger, John Kroger, Jeffersontown Kroger shooting, Uwe Kr\u00f6ger, Chad Kroeger, Smith's Food and Drug, Kroger (disambiguation) 18. General Electric : General Electric, General Electric GEnx, General Electric Building, General Electric CF6, General Electric Theater, General Electric GE9X, General Electric Company, General Electric GE90, General Electric LM6000, General Electric CF34 19. Walgreens Boots Alliance : Walgreens Boots Alliance, Alliance Boots, Walgreens, Boots (company), Alliance Healthcare, Stefano Pessina, Boots Opticians, Ornella Barra, Gregory Wasson, James A. Skinner 20. JPMorgan Chase : JPMorgan Chase, Chase Bank, 2012 JPMorgan Chase trading loss, JPMorgan Chase Tower (Houston), 2014 JPMorgan Chase data breach, JPMorgan Chase Building (San Francisco), JPMorgan Corporate Challenge, Chase Tower (Dallas), 270 Park Avenue, Chase Paymentech 1 2 most_probable = [( company , wiki_search [ i ][ company ][ 0 ]) for i , company in enumerate ( companies )] most_probable 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [('Walmart', 'Walmart'), ('Exxon Mobil', 'ExxonMobil'), ('Berkshire Hathaway', 'Berkshire Hathaway'), ('Apple', 'Apple'), ('UnitedHealth Group', 'UnitedHealth Group'), ('McKesson', 'McKesson Corporation'), ('CVS Health', 'CVS Health'), ('Amazon.com', 'Amazon (company)'), ('AT&T', 'AT&T'), ('General Motors', 'General Motors'), ('Ford Motor', 'Ford Motor Company'), ('AmerisourceBergen', 'AmerisourceBergen'), ('Chevron', 'Chevron Corporation'), ('Cardinal Health', 'Cardinal Health'), ('Costco', 'Costco'), ('Verizon', 'Verizon Communications'), ('Kroger', 'Kroger'), ('General Electric', 'General Electric'), ('Walgreens Boots Alliance', 'Walgreens Boots Alliance'), ('JPMorgan Chase', 'JPMorgan Chase')] 1 2 companies = [ x [ 1 ] for x in most_probable ] companies 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ['Walmart', 'ExxonMobil', 'Berkshire Hathaway', 'Apple', 'UnitedHealth Group', 'McKesson Corporation', 'CVS Health', 'Amazon (company)', 'AT&T', 'General Motors', 'Ford Motor Company', 'AmerisourceBergen', 'Chevron Corporation', 'Cardinal Health', 'Costco', 'Verizon Communications', 'Kroger', 'General Electric', 'Walgreens Boots Alliance', 'JPMorgan Chase'] For Apple , lets manually replace it with Apple Inc. as follows, 1 2 companies [ companies . index ( 'Apple' )] = 'Apple Inc.' print ( companies ) 1 ['Walmart', 'ExxonMobil', 'Berkshire Hathaway', 'Apple Inc.', 'UnitedHealth Group', 'McKesson Corporation', 'CVS Health', 'Amazon (company)', 'AT&T', 'General Motors', 'Ford Motor Company', 'AmerisourceBergen', 'Chevron Corporation', 'Cardinal Health', 'Costco', 'Verizon Communications', 'Kroger', 'General Electric', 'Walgreens Boots Alliance', 'JPMorgan Chase'] Note : Wiki data dump link (last updated 2015) : https://old.datahub.io/dataset/wikidata","title":"Getting article names from wiki"},{"location":"section-3-API-based-scraping/#wptools","text":"https://github.com/siznax/wptools/wiki/Data-captured 1 2 3 page = wptools . page ( 'Walmart' ) page . get_parse () page . get_wikidata () 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 en.wikipedia.org (parse) Walmart en.wikipedia.org (imageinfo) File:Walmart store exterior 5266815680.jpg Walmart (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Walmart s... infobox: <dict(30)> name, logo, logo_caption, image, image_size,... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:W... pageid: 33589 parsetree: <str(346504)> <root><template><title>about</title><pa... requests: <list(2)> parse, imageinfo title: Walmart wikibase: Q483551 wikidata_url: https://www.wikidata.org/wiki/Q483551 wikitext: <str(274081)> {{about|the retail chain|other uses}}{{p... } www.wikidata.org (wikidata) Q483551 www.wikidata.org (labels) Q180816|Q219635|P18|Q478758|Q10382887|Q... www.wikidata.org (labels) P740|Q54862513|P966|P3500|Q6383259|Q694... www.wikidata.org (labels) Q818364|P6160|P1278|P3347|Q17343056|P37... en.wikipedia.org (imageinfo) File:Walmart Home Office.jpg Walmart (en) data { aliases: <list(5)> Wal-Mart, Wal Mart, Wal-Mart Stores, Inc., Wa... claims: <dict(63)> P112, P946, P373, P31, P856, P910, P159, P414... description: U.S. discount retailer based in Arkansas image: <list(2)> {'kind': 'parse-image', 'file': 'File:Walmart s... infobox: <dict(30)> name, logo, logo_caption, image, image_size,... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:W... label: Walmart labels: <dict(116)> Q180816, Q219635, P18, Q478758, Q10382887, Q... modified: <dict(1)> wikidata pageid: 33589 parsetree: <str(346504)> <root><template><title>about</title><pa... requests: <list(7)> parse, imageinfo, wikidata, labels, labels, ... title: Walmart what: retail chain wikibase: Q483551 wikidata: <dict(63)> founded by (P112), ISIN (P946), Commons cat... wikidata_pageid: 455133 wikidata_url: https://www.wikidata.org/wiki/Q483551 wikitext: <str(274081)> {{about|the retail chain|other uses}}{{p... } <wptools.page.WPToolsPage at 0x7f1fc48660f0> 1 page . data . keys () 1 dict_keys(['requests', 'iwlinks', 'pageid', 'wikitext', 'parsetree', 'infobox', 'title', 'wikibase', 'wikidata_url', 'image', 'labels', 'wikidata', 'wikidata_pageid', 'aliases', 'modified', 'description', 'label', 'claims', 'what']) Alternatively, 1 page . data [ 'wikidata' ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 {'founded by (P112)': 'Sam Walton (Q497827)', 'ISIN (P946)': 'US9311421039', 'Commons category (P373)': 'Walmart', 'instance of (P31)': ['retail chain (Q507619)', 'enterprise (Q6881511)'], 'official website (P856)': 'https://www.walmart.com', \"topic's main category (P910)\": 'Category:Walmart (Q6383259)', 'headquarters location (P159)': ['Bentonville (Q818364)', 'Arkansas (Q1612)'], 'stock exchange (P414)': 'New York Stock Exchange (Q13677)', 'subsidiary (P355)': [\"Sam's Club (Q1972120)\", 'Massmart (Q3297791)', 'Walmart Canada (Q1645718)', 'Walmart Chile (Q5283104)', 'Walmart de M\u00e9xico y Centroam\u00e9rica (Q1064887)', 'Seiyu Group (Q3108542)', 'Asda (Q297410)', 'Walmart Labs (Q3816562)', 'Walmart (Q30338489)', 'M\u00e1s Club (Q6949810)', 'L\u00edder (Q6711261)', 'Hypermart USA (Q16845747)', 'Amigo Supermarkets (Q4746234)', 'Walmart Neighborhood Market (Q7963529)', 'Asda Mobile (Q4804093)', 'Marketside (Q6770960)', 'Vudu (Q5371838)', 'Walmart Nicaragua (Q22121904)'], 'owned by (P127)': ['Walton Enterprises (Q17343056)', 'State Street Corporation (Q2037125)', 'The Vanguard Group (Q849363)', 'BlackRock (Q219635)'], 'VIAF ID (P214)': '128951275', 'Freebase ID (P646)': '/m/0841v', 'inception (P571)': '+1962-07-02T00:00:00Z', 'industry (P452)': ['retail (Q126793)', 'retail chain (Q507619)', 'discount store (Q261428)'], 'chairperson (P488)': ['Doug McMillon (Q16196595)', 'Greg Penner (Q20177269)'], 'motto text (P1451)': 'Save money. Live better.', 'Facebook ID (P2013)': 'walmart', 'Twitter username (P2002)': 'Walmart', 'part of (P361)': ['S&P 500 (Q242345)', 'Dow Jones Industrial Average (Q180816)'], 'image (P18)': 'Walmart Home Office.jpg', 'location of formation (P740)': 'Rogers (Q79497)', 'country (P17)': 'United States of America (Q30)', 'legal form (P1454)': 'public company (Q891723)', 'named after (P138)': 'Sam Walton (Q497827)', 'total revenue (P2139)': [{'amount': '+482130000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+482130500000', 'lowerBound': '+482129500000'}, {'amount': '+485651000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+485651500000', 'lowerBound': '+485650500000'}, {'amount': '+476294000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+476294500000', 'lowerBound': '+476293500000'}, {'amount': '+468651000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+468651500000', 'lowerBound': '+468650500000'}, {'amount': '+446509000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+446509500000', 'lowerBound': '+446508500000'}, {'amount': '+485873000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}], 'net profit (P2295)': [{'amount': '+14694000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+14694500000', 'lowerBound': '+14693500000'}, {'amount': '+16182000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+16182500000', 'lowerBound': '+16181500000'}, {'amount': '+15918000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+15918500000', 'lowerBound': '+15917500000'}, {'amount': '+16963000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+16963500000', 'lowerBound': '+16962500000'}, {'amount': '+15734000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+15734500000', 'lowerBound': '+15733500000'}, {'amount': '+13643000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}], 'total assets (P2403)': [{'amount': '+199581000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+199581500000', 'lowerBound': '+199580500000'}, {'amount': '+203490000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+203490500000', 'lowerBound': '+203489500000'}, {'amount': '+204541000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+204541500000', 'lowerBound': '+204540500000'}, {'amount': '+202910000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+202910500000', 'lowerBound': '+202909500000'}, {'amount': '+193120000000', 'unit': 'http://www.wikidata.org/entity/Q4917', 'upperBound': '+193120500000', 'lowerBound': '+193119500000'}, {'amount': '+198825000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}], 'employees (P1128)': {'amount': '+2300000', 'unit': '1'}, 'Legal Entity Identifier (P1278)': 'Y87794H0US1R65VBXU25', 'Instagram username (P2003)': 'walmart', 'Google+ ID (P2847)': '111852759168797891317', 'chief executive officer (P169)': ['Doug McMillon (Q16196595)', 'Mike Duke (Q1933118)', 'Lee Scott (Q478758)', 'David Glass (Q5234167)', 'Sam Walton (Q497827)'], 'Quora topic ID (P3417)': 'Walmart-company', 'Justia Patents company ID (P3875)': 'wal-mart', 'logo image (P154)': 'Walmart logo.svg', 'IPv4 routing prefix (P3761)': '156.94.0.0/16', 'ISNI (P213)': '0000 0004 0616 1876', 'operating income (P3362)': [{'amount': '+22764000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}, {'amount': '+24105000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}, {'amount': '+27147000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}], 'website account on (P553)': 'WeChat (Q283233)', 'PermID (P3347)': '4295905298', 'Encyclop\u00e6dia Britannica Online ID (P1417)': 'topic/Wal-Mart', 'GRID ID (P2427)': 'grid.480455.8', 'award received (P166)': 'Public Eye Labour Law Award (Q54862513)', 'Central Index Key (P5531)': '0000104169', 'IdRef ID (P269)': '050771116', 'owner of (P1830)': ['Asda (Q297410)', \"Sam's Club (Q1972120)\", 'Seiyu Group (Q3108542)', 'Bodega Aurrer\u00e1 (Q3365858)', 'Asda Mobile (Q4804093)', 'Bompre\u00e7o (Q4940907)', 'Hayneedle (Q5687056)', 'Mercadorama (Q10328812)', None, 'Jet.com (Q22079907)', '.george (Q26911051)', '.samsclub (Q26972795)', 'Shoes.com (Q46438789)'], 'NKCR AUT ID (P691)': 'osa2010597558', 'Microsoft Academic ID (P6366)': '1330693074', 'market capitalization (P2226)': {'amount': '+239000000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}, 'member of (P463)': 'Linux Foundation (Q858851)', 'Library of Congress authority ID (P244)': 'n90648829', 'MusicBrainz label ID (P966)': 'b3a104e8-eed0-4a3e-aae8-676c6e7ab016', 'ROR ID (P6782)': '04j0gge90', 'Ringgold ID (P3500)': '48990', 'BoardGameGeek game publisher ID (P6160)': '29995', 'total equity (P2137)': {'amount': '+80535000000', 'unit': 'http://www.wikidata.org/entity/Q4917'}, 'DR topic ID (P6849)': 'walmart', 'BBC News topic ID (P6200)': 'ce1qrvlex0et', 'Gran Enciclop\u00e8dia Catalana ID (P1296)': '0256072', 'Downdetector ID (P7306)': 'wal-mart', 'LittleSis organisation ID (P3393)': '1-Walmart', 'WeChat ID (P7650)': 'Walmart_Hyper', 'Pinterest username (P3836)': 'walmart'} 1 2 3 4 wiki_data = [] # attributes of interest contained within the wiki infoboxes features = [ 'founder' , 'location_country' , 'revenue' , 'operating_income' , 'net_income' , 'assets' , 'equity' , 'type' , 'industry' , 'products' , 'num_employees' ] Now lets fetch results for all the companies as follows, 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 for company in companies : page = wptools . page ( company ) try : page . get_parse () if page . data [ 'infobox' ] != None : infobox = page . data [ 'infobox' ] data = { feature : infobox [ feature ] if feature in infobox else '' for feature in features } else : data = { feature : '' for feature in features } data [ 'company_name' ] = company wiki_data . append ( data ) except KeyError : pass 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 en.wikipedia.org (parse) Walmart en.wikipedia.org (imageinfo) File:Walmart store exterior 5266815680.jpg Walmart (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Walmart s... infobox: <dict(30)> name, logo, logo_caption, image, image_size,... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:W... pageid: 33589 parsetree: <str(346504)> <root><template><title>about</title><pa... requests: <list(2)> parse, imageinfo title: Walmart wikibase: Q483551 wikidata_url: https://www.wikidata.org/wiki/Q483551 wikitext: <str(274081)> {{about|the retail chain|other uses}}{{p... } en.wikipedia.org (parse) ExxonMobil en.wikipedia.org (imageinfo) File:ExxonMobilBuilding.JPG ExxonMobil (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:ExxonMobi... infobox: <dict(29)> name, logo, image, image_caption, type, trad... iwlinks: <list(3)> https://commons.wikimedia.org/wiki/Category:E... pageid: 18848197 parsetree: <str(187433)> <root><template><title>About</title><pa... requests: <list(2)> parse, imageinfo title: ExxonMobil wikibase: Q156238 wikidata_url: https://www.wikidata.org/wiki/Q156238 wikitext: <str(152792)> {{About|Exxon Mobil Corp|the company's s... } en.wikipedia.org (parse) Berkshire Hathaway Berkshire Hathaway (en) data { image: <list(0)> infobox: <dict(24)> name, former_name, logo, image, image_captio... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:B... pageid: 314333 parsetree: <str(101434)> <root><template><title>short descriptio... requests: <list(1)> parse title: Berkshire Hathaway wikibase: Q217583 wikidata_url: https://www.wikidata.org/wiki/Q217583 wikitext: <str(86730)> {{short description|American multinationa... } en.wikipedia.org (parse) Apple Inc. en.wikipedia.org (imageinfo) File:Apple park cupertino 2019.jpg Apple Inc. (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Apple par... infobox: <dict(36)> name, logo, logo_size, image, image_size, im... iwlinks: <list(8)> https://commons.wikimedia.org/wiki/Special:Se... pageid: 856 parsetree: <str(403082)> <root><template><title>Redirect</title>... requests: <list(2)> parse, imageinfo title: Apple Inc. wikibase: Q312 wikidata_url: https://www.wikidata.org/wiki/Q312 wikitext: <str(321377)> {{Redirect|Apple (company)|other compani... } en.wikipedia.org (parse) UnitedHealth Group UnitedHealth Group (en) data { infobox: <dict(17)> name, logo, type, traded_as, founder, key_pe... pageid: 1845551 parsetree: <str(86142)> <root><template><title>Redirect</title><... requests: <list(1)> parse title: UnitedHealth Group wikibase: Q2103926 wikidata_url: https://www.wikidata.org/wiki/Q2103926 wikitext: <str(73971)> {{Redirect|UnitedHealthcare|the cycling t... } en.wikipedia.org (parse) McKesson Corporation McKesson Corporation (en) data { infobox: <dict(19)> name, logo, type, traded_as, founder, locati... pageid: 1041603 parsetree: <str(38152)> <root><template><title>Redirect</title><... requests: <list(1)> parse title: McKesson Corporation wikibase: Q570473 wikidata_url: https://www.wikidata.org/wiki/Q570473 wikitext: <str(30274)> {{Redirect|McKesson}}{{short description|... } en.wikipedia.org (parse) CVS Health CVS Health (en) data { infobox: <dict(28)> name, logo, logo_size, former_name, type, tr... pageid: 10377597 parsetree: <str(69373)> <root><template><title>short description... requests: <list(1)> parse title: CVS Health wikibase: Q624375 wikidata_url: https://www.wikidata.org/wiki/Q624375 wikitext: <str(54045)> {{short description|American healthcare c... } en.wikipedia.org (parse) Amazon (company) en.wikipedia.org (imageinfo) File:Seattle Spheres on May 10, 2018.jpg Amazon (company) (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Seattle S... infobox: <dict(33)> name, logo, logo_size, image, image_size, im... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:A... pageid: 90451 parsetree: <str(153139)> <root><template><title>pp</title><part>... requests: <list(2)> parse, imageinfo title: Amazon (company) wikibase: Q3884 wikidata_url: https://www.wikidata.org/wiki/Q3884 wikitext: <str(116580)> {{pp|small=yes}}{{short description|Amer... } en.wikipedia.org (parse) AT&T en.wikipedia.org (imageinfo) File:AT&THQDallas.jpg AT&T (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:AT&THQDal... infobox: <dict(27)> name, logo, logo_size, image, image_size, im... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:AT%26T pageid: 17555269 parsetree: <str(130159)> <root><template><title>about</title><pa... requests: <list(2)> parse, imageinfo title: AT&T wikibase: Q35476 wikidata_url: https://www.wikidata.org/wiki/Q35476 wikitext: <str(104995)> {{about|the company known as AT&T since ... } en.wikipedia.org (parse) General Motors en.wikipedia.org (imageinfo) File:RenCen.JPG General Motors (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:RenCen.JP... infobox: <dict(29)> name, former_name, logo, logo_size, image, i... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:G... pageid: 12102 parsetree: <str(190450)> <root><template><title>short descriptio... requests: <list(2)> parse, imageinfo title: General Motors wikibase: Q81965 wikidata_url: https://www.wikidata.org/wiki/Q81965 wikitext: <str(150285)> {{short description|American automotive ... } en.wikipedia.org (parse) Ford Motor Company en.wikipedia.org (imageinfo) File:FordGlassHouse.jpg Ford Motor Company (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:FordGlass... infobox: <dict(27)> name, logo, image, image_size, image_caption... iwlinks: <list(8)> https://commons.wikimedia.org/wiki/Category:F... pageid: 30433662 parsetree: <str(193764)> <root><template><title>Redirect</title>... requests: <list(2)> parse, imageinfo title: Ford Motor Company wikibase: Q44294 wikidata_url: https://www.wikidata.org/wiki/Q44294 wikitext: <str(157653)> {{Redirect|Ford}}{{pp-semi-indef}}{{pp-m... } en.wikipedia.org (parse) AmerisourceBergen AmerisourceBergen (en) data { infobox: <dict(17)> name, logo, type, traded_as, foundation, loc... pageid: 1445945 parsetree: <str(16501)> <root><template><title>short description... requests: <list(1)> parse title: AmerisourceBergen wikibase: Q470156 wikidata_url: https://www.wikidata.org/wiki/Q470156 wikitext: <str(11755)> {{short description|American healthcare c... } en.wikipedia.org (parse) Chevron Corporation Chevron Corporation (en) data { image: <list(0)> infobox: <dict(24)> name, logo, logo_size, logo_caption, image, ... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:C... pageid: 284749 parsetree: <str(120598)> <root><template><title>short descriptio... requests: <list(1)> parse title: Chevron Corporation wikibase: Q319642 wikidata_url: https://www.wikidata.org/wiki/Q319642 wikitext: <str(97793)> {{short description|American multinationa... } en.wikipedia.org (parse) Cardinal Health Cardinal Health (en) data { infobox: <dict(17)> name, logo, type, traded_as, industry, found... pageid: 1041632 parsetree: <str(32814)> <root><template><title>Infobox company</... requests: <list(1)> parse title: Cardinal Health wikibase: Q902397 wikidata_url: https://www.wikidata.org/wiki/Q902397 wikitext: <str(25715)> {{Infobox company| name = Cardinal Health... } en.wikipedia.org (parse) Costco en.wikipedia.org (imageinfo) File:Costcoheadquarters.jpg Costco (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Costcohea... infobox: <dict(35)> name, logo, logo_caption, image, image_size,... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:Costco pageid: 446056 parsetree: <str(97750)> <root><template><title>Distinguish</titl... requests: <list(2)> parse, imageinfo title: Costco wikibase: Q715583 wikidata_url: https://www.wikidata.org/wiki/Q715583 wikitext: <str(71853)> {{Distinguish|COSCO|Cosco (India) Limited... } en.wikipedia.org (parse) Verizon Communications en.wikipedia.org (imageinfo) File:Verizon Building (8156005279).jpg Verizon Communications (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Verizon B... infobox: <dict(30)> name, logo, image, image_caption, former_nam... iwlinks: <list(3)> https://commons.wikimedia.org/wiki/Category:T... pageid: 18619278 parsetree: <str(147152)> <root><template><title>short descriptio... requests: <list(2)> parse, imageinfo title: Verizon Communications wikibase: Q467752 wikidata_url: https://www.wikidata.org/wiki/Q467752 wikitext: <str(124812)> {{short description|American communicati... } en.wikipedia.org (parse) Kroger en.wikipedia.org (imageinfo) File:Cincinnati-kroger-building.jpg Kroger (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:Cincinnat... infobox: <dict(24)> name, logo, image, image_caption, type, trad... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:Kroger pageid: 367762 parsetree: <str(121519)> <root><template><title>Use American Eng... requests: <list(2)> parse, imageinfo title: Kroger wikibase: Q153417 wikidata_url: https://www.wikidata.org/wiki/Q153417 wikitext: <str(102176)> {{Use American English|date = August 201... } en.wikipedia.org (parse) General Electric General Electric (en) data { infobox: <dict(20)> name, logo, type, traded_as, ISIN, industry,... iwlinks: <list(1)> https://commons.wikimedia.org/wiki/Category:G... pageid: 12730 parsetree: <str(162543)> <root><template><title>redirect</title>... requests: <list(1)> parse title: General Electric wikibase: Q54173 wikidata_url: https://www.wikidata.org/wiki/Q54173 wikitext: <str(137546)> {{redirect|GE}}{{distinguish|text=the fo... } en.wikipedia.org (parse) Walgreens Boots Alliance Walgreens Boots Alliance (en) data { infobox: <dict(29)> name, logo, logo_size, type, traded_as, pred... pageid: 44732533 parsetree: <str(32631)> <root><template><title>Use mdy dates</ti... requests: <list(1)> parse title: Walgreens Boots Alliance wikibase: Q18712620 wikidata_url: https://www.wikidata.org/wiki/Q18712620 wikitext: <str(25099)> {{Use mdy dates|date=October 2019}}{{shor... } en.wikipedia.org (parse) JPMorgan Chase en.wikipedia.org (imageinfo) File:383 Madison Ave Bear Stearns C ... JPMorgan Chase (en) data { image: <list(1)> {'kind': 'parse-image', 'file': 'File:383 Madis... infobox: <dict(31)> name, logo, image, image_caption, type, trad... iwlinks: <list(2)> https://commons.wikimedia.org/wiki/Category:J... pageid: 231001 parsetree: <str(137921)> <root><template><title>About</title><pa... requests: <list(2)> parse, imageinfo title: JPMorgan Chase wikibase: Q192314 wikidata_url: https://www.wikidata.org/wiki/Q192314 wikitext: <str(112397)> {{About|JPMorgan Chase & Co|its main sub... } 1 wiki_data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 [{'founder': '[[Sam Walton]]', 'location_country': 'U.S.', 'revenue': '{{increase}} {{US$|514.405 billion|link|=|yes}} (2019)', 'operating_income': '{{increase}} {{US$|21.957 billion}} (2019)', 'net_income': '{{decrease}} {{US$|6.67 billion}} (2019)', 'assets': '{{increase}} {{US$|219.295 billion}} (2019)', 'equity': '{{decrease}} {{US$|79.634 billion}} (2019)', 'type': '[[Public company|Public]]', 'industry': '[[Retail]]', 'products': '{{hlist|Electronics|Movies and music|Home and furniture|Home improvement|Clothing|Footwear|Jewelry|Toys|Health and beauty|Pet supplies|Sporting goods and fitness|Auto|Photo finishing|Craft supplies|Party supplies|Grocery}}', 'num_employees': '{{plainlist|\\n* 2.2|nbsp|million, Worldwide (2018)|ref| name=\"xbrlus_1\" |\\n* 1.5|nbsp|million, U.S. (2017)|ref| name=\"Walmart\"|{{cite web |url = http://corporate.walmart.com/our-story/locations/united-states |title = Walmart Locations Around the World \u2013 United States |publisher = |url-status=live |archiveurl = https://web.archive.org/web/20150926012456/http://corporate.walmart.com/our-story/locations/united-states |archivedate = September 26, 2015 |df = mdy-all }}|</ref>|\\n* 700,000, International}} {{nbsp}} million, Worldwide (2018) * 1.5 {{nbsp}} million, U.S. (2017) * 700,000, International', 'company_name': 'Walmart'}, {'founder': '', 'location_country': '', 'revenue': '{{Nowrap|Increase| |US$|279.3 billion|link|=|yes|ref| name=\"201310K\"|[https://corporate.exxonmobil.com/-/media/global/files/annual-report/2018-financial-and-operating-review.pdf EXXON MOBIL CORPORATION Form 10-K] {{Webarchive|url=https://web.archive.org/web/20190404044022/https://corporate.exxonmobil.com/-/media/global/files/annual-report/2018-financial-and-operating-review.pdf |date=April 4, 2019 }}, \\'\\'Google Finance\\'\\', March 21, 2019|</ref>}} {{Increase}} {{US$|279.3 billion|link|=|yes}}', 'operating_income': '{{Nowrap|Increase| |US$|21.53 billion|ref| name=\"201310K\"}} {{Increase}} {{US$|21.53 billion}}', 'net_income': '{{Nowrap|Increase| |US$|20.84 billion|ref| name=\"201310K\"}} {{Increase}} {{US$|20.84 billion}}', 'assets': '{{Nowrap|Decrease| |US$|346.2 billion|ref| name=\"201310K\"}} {{Decrease}} {{US$|346.2 billion}}', 'equity': '{{Nowrap|Increase| |US$|191.8 billion|ref| name=\"201310K\"}} {{Increase}} {{US$|191.8 billion}}', 'type': '[[Public company|Public]]', 'industry': '[[Energy industry|Energy]]: [[Oil and gas industry|Oil and gas]]', 'products': '{{Unbulleted list\\n | [[Crude oil]]\\n | [[Oil products]]\\n | [[Natural gas]]\\n | [[Petrochemical]]s\\n | [[Power generation]]}}', 'num_employees': '71,000', 'company_name': 'ExxonMobil'}, {'founder': '[[Oliver Chace]]<br>[[Warren Buffett]] (Modern era)', 'location_country': '', 'revenue': '{{increase}} US$247.5 billion (2018)', 'operating_income': '{{Decrease}} US$10.02 billion (2018)', 'net_income': '{{Decrease}} US$4.02 billion (2018)', 'assets': '{{increase}} US$707.8 billion (2018)', 'equity': '{{increase}} US$348.7 billion (2018)', 'type': '[[Public company|Public]]', 'industry': '[[Conglomerate (company)|Conglomerate]]', 'products': '[[Investment|Diversified investments]], [[Insurance#Types|Property & casualty insurance]], [[Public utility|Utilities]], [[Restaurants]], [[Food processing]], [[Aerospace]], [[Toys]], [[Mass media|Media]], [[Automotive industry|Automotive]], [[Sports equipment|Sporting goods]], [[Final good|Consumer products]], [[Internet]], [[Real estate]]', 'num_employees': '{{nowrap|389,373 (2018)}}', 'company_name': 'Berkshire Hathaway'}, {'founder': '', 'location_country': '', 'revenue': '{{Decrease}} {{US$|260.174&nbsp;billion|link|=|yes}}', 'operating_income': '{{Decrease}} {{US$|63.930&nbsp;billion}}', 'net_income': '{{Decrease}} {{US$|55.256&nbsp;billion}}', 'assets': '{{Decrease}} {{US$|338.516&nbsp;billion}}', 'equity': '{{Decrease}} {{US$|90.488&nbsp;billion}}', 'type': '[[Public company|Public]]', 'industry': '{{Unbulleted list | [[Computer hardware]] | [[Computer software]] | [[Consumer electronics]] | [[Cloud computing]] | [[Digital distribution]] | [[Fabless manufacturing|Fabless silicon design]] | [[Semiconductors]] | [[Financial technology]] | [[Artificial intelligence]]}}', 'products': '{{Flatlist|\\n* [[Macintosh]]\\n* [[iPod]]\\n* [[iPhone]]\\n* [[iPad]]\\n* [[Apple Watch]]\\n* [[Apple TV]]\\n* [[HomePod]]\\n* [[macOS]]\\n* [[iOS]]\\n* [[iPadOS]]\\n* [[watchOS]]\\n* [[tvOS]]\\n* [[iLife]]\\n* [[iWork]]\\n* [[Final Cut Pro]]\\n* [[Logic Pro]]\\n* [[GarageBand]]\\n* [[Shazam (application)|Shazam]]\\n* [[Siri]]}}', 'num_employees': '137,000', 'company_name': 'Apple Inc.'}, {'founder': 'Richard T. Burke', 'location_country': '', 'revenue': '{{increase}} $242.155 billion (2019)', 'operating_income': '{{increase}} $17.981 billion (2019)', 'net_income': '{{increase}} $14.239 billion (2019)', 'assets': '{{increase}} $173.889 billion (2019)', 'equity': '{{increase}} $60.436 billion (2019)', 'type': '[[Public company]]', 'industry': '[[Managed health care]]', 'products': '[[Uniprise]], [[Health Care]] [[Service (economics)|Services]], Specialized Care Services, and [[Ingenix]]', 'num_employees': '300,000 (2019)', 'company_name': 'UnitedHealth Group'}, {'founder': 'John McKesson<br>Charles Olcott', 'location_country': '', 'revenue': '{{increase}} {{US$|208.4 billion}} {{small|(2018)}}', 'operating_income': '{{increase}} {{US$|2.921 billion}} {{small|(2018)}}', 'net_income': '{{increase}} {{US$|67 million}} {{small|(2018)}}', 'assets': '{{nowrap|increase| |US$|60.381 billion| |small|(2018)|ref| name=FY}} {{increase}} {{US$|60.381 billion}} {{small|(2018)}}', 'equity': '{{decrease}} {{US$|10.057 billion}} {{small|(2018)}}', 'type': '[[Public company|Public]]', 'industry': '[[Healthcare]]', 'products': '[[Pharmaceuticals]]<br>[[Medical technology]]<br>[[Health care services]]', 'num_employees': '~78,000 {{small|(2018)}}', 'company_name': 'McKesson Corporation'}, {'founder': '', 'location_country': '', 'revenue': '{{ublist|class|=|nowrap|increase| |US$|194.579 billion| (2018)|US$|184.786 billion| (2017)}} {{increase}} {{US$|194.579 billion}} (2018) {{US$|184.786 billion}} (2017)', 'operating_income': '{{ublist|class|=|nowrap|decrease| |US$|4.021 billion| (2018)|US$|9.538 billion| (2017)}} {{decrease}} {{US$|4.021 billion}} (2018) {{US$|9.538 billion}} (2017)', 'net_income': '{{ublist|class|=|nowrap|decrease| |US$|-596 million| (2018)|US$|6.623 billion| (2017)}} {{decrease}} {{US$|-596 million}} (2018) {{US$|6.623 billion}} (2017)', 'assets': '{{increase}} {{US$|196.456 billion}}', 'equity': '{{increase}} {{US$|58.225 billion}}', 'type': '[[Public company|Public]]', 'industry': '{{flat list|\\n* [[Retail]]\\n* [[health care]]}}', 'products': '', 'num_employees': '295,000', 'company_name': 'CVS Health'}, {'founder': '[[Jeff Bezos]]', 'location_country': '', 'revenue': '{{increase}} {{US$|232.887 billion|link|=|yes}}', 'operating_income': '{{increase}} {{US$|12.421 billion}}', 'net_income': '{{increase}} {{US$|10.073 billion}}', 'assets': '{{decrease}} {{US$|162.648 billion}}', 'equity': '{{decrease}} {{US$|43.549 billion}}', 'type': '[[Public company|Public]]', 'industry': '{{plainlist|\\n* [[Cloud computing]]\\n* [[E-commerce]]\\n* [[Artificial intelligence]]\\n* [[Consumer electronics]]\\n* [[Digital distribution]]\\n* [[Grocery stores]]}}', 'products': '{{Hlist|[[Amazon Echo]]|[[Amazon Fire tablet|Amazon Fire]]|[[Amazon Fire TV]]|[[Fire OS|Amazon Fire OS]]|[[Amazon Kindle]]}}', 'num_employees': '{{increase}} 750,000 (2019)', 'company_name': 'Amazon (company)'}, {'founder': '', 'location_country': '', 'revenue': '{{increase}} {{US$|link|=|yes}} 170.756 billion (2018)', 'operating_income': '{{increase}} {{US$|link|=|yes}} 26.096 billion (2018)', 'net_income': '{{increase}} {{US$|link|=|yes}} 19.953 billion (2018)', 'assets': '{{increase}} {{US$|link|=|yes}} 531 billion (2018)', 'equity': '{{increase}} {{US$|link|=|yes}} 193.884 billion (2018)', 'type': '[[Public company|Public]]', 'industry': '{{Unbulleted list|[[Telecommunications industry|Telecommunications]]|[[Technology company|Technology]]|[[Mass media]]|[[Entertainment]]}}', 'products': '{{Hlist|[[Satellite television]]|[[Landline|Fixed-line telephones]]|[[Mobile phone|Mobile telephones]]|[[Internet service provider|Internet services]]|[[Broadband]]|[[Digital television]]|[[Home security]]|[[IPTV]]|[[Over-the-top media services|OTT services]]|[[Network security]]|[[Filmmaking|Film production]]|[[Television production]]|[[Cable television]]|[[Pay television]]|[[Publishing]]|[[Podcast]]s|[[Sports management]]|[[News agency]]|[[Video game]]s}}', 'num_employees': '251,840 (2019)', 'company_name': 'AT&T'}, {'founder': '[[William C. Durant]]', 'location_country': 'US', 'revenue': '{{decrease}} [[United States dollar|US$]]147.049 billion {{small|(2018)}}', 'operating_income': '{{decrease}} US$11.783 billion {{small|(2018)}}', 'net_income': '{{increase}} US$8.014 billion {{small|(2018)}}', 'assets': '{{increase}} US$227.339 billion {{small|(2018)}}', 'equity': '{{increase}} US$42.777 billion {{small|(2018)}}', 'type': '[[Public company|Public]]', 'industry': '[[Automotive industry|Automotive]]', 'products': '[[Car|Automobiles]]<br />Automobile parts<br />[[Commercial vehicle]]s', 'num_employees': '170,483 {{small|(December 2018)}}', 'company_name': 'General Motors'}, {'founder': '[[Henry Ford]]', 'location_country': 'U.S.', 'revenue': '{{increase}} {{US$|160.33 billion|link|=|yes}} {{small|(2018)}}', 'operating_income': '{{decrease}} {{US$|3.27 billion}} {{small|(2018)}}', 'net_income': '{{decrease}} {{US$|3.67 billion}} {{small|(2018)}}', 'assets': '{{decrease}} {{US$|256.54 billion}} {{small|(2018)}}', 'equity': '{{decrease}} {{US$|35.93 billion}} {{small|(2018)}}', 'type': '[[Public company|Public]]', 'industry': '[[Automotive industry|Automotive]]', 'products': '{{unbulleted list\\n | [[Car|Automobiles]]\\n | [[Luxury Car|Luxury Vehicles]]\\n | [[Commercial vehicle|Commercial Vehicles]]\\n | [[List of auto parts|Automotive parts]]\\n | [[Pickup trucks]]\\n | [[SUVs]]}}', 'num_employees': '199,000 {{small|(December 2018)}}', 'company_name': 'Ford Motor Company'}, {'founder': '', 'location_country': '', 'revenue': '{{increase}} {{US$|167.93 billion|link|=|yes}} (2018)', 'operating_income': '{{increase}} {{US$|1.44 billion}} (2018)', 'net_income': '{{increase}} {{US$|1.65 billion}} (2018)', 'assets': '{{increase}} {{US$|37.66 billion}} (2018)', 'equity': '{{increase}} {{US$|2.93 billion}} (2018)', 'type': '[[Public company|Public]]', 'industry': '[[Pharmaceutical]]', 'products': '[[Pharmaceutical]]s and [[pharmacy]] services', 'num_employees': '20,000 (2018)', 'company_name': 'AmerisourceBergen'}, {'founder': '', 'location_country': '', 'revenue': '{{increase}} {{US$|158.9 billion|link|=|yes}} {{small|(2018)}}', 'operating_income': '{{increase}} {{US$|15.45 billion}} {{small|(2018)}}', 'net_income': '{{increase}} {{US$|14.82 billion}} {{small|(2018)}}', 'assets': '{{decrease}} {{US$|253.9 billion}} {{small|(2018)}}', 'equity': '{{increase}} {{US$|154.5 billion}} {{small|(2018)}}', 'type': '[[Public company|Public]]', 'industry': '[[Oil and gas industry|Oil and gas]]', 'products': \"[[Petroleum]], [[natural gas]] and other [[petrochemical]]s, ''[[#Marketing brands|See Chevron products]]''\", 'num_employees': '~51,900 {{small|(December 2018)}}', 'company_name': 'Chevron Corporation'}, {'founder': '', 'location_country': '', 'revenue': '{{increase}} [[US$]]136.80 billion {{small|(2018)}}', 'operating_income': '{{decrease}} US$126 million {{small|(2018)}}', 'net_income': '{{decrease}} US$256 million {{small|(2018)}}', 'assets': '{{increase}} US$39.95 billion {{small|(2018)}}', 'equity': '{{increase}} US$6.05 billion {{small|(2018)}}', 'type': '[[Public company|Public]]', 'industry': '[[Pharmaceuticals]]', 'products': 'Medical and pharmaceutical products and services', 'num_employees': '~50,000 {{small|(2018)}}', 'company_name': 'Cardinal Health'}, {'founder': '', 'location_country': 'United States', 'revenue': '{{increase}} {{US$|152.7 billion}}', 'operating_income': '{{increase}} US$4.74 billion (2018)', 'net_income': '{{increase}} US$3.66 billion', 'assets': '{{increase}} US$45.40 billion', 'equity': '{{increase}} US$15.24 billion', 'type': '[[Public company|Public]]', 'industry': '[[Retail]]', 'products': '', 'num_employees': '{{increase}} 254,000', 'company_name': 'Costco'}, {'founder': '', 'location_country': '', 'revenue': '{{increase}} {{US$|130.86 [[1,000,000,000|billion]]|link|=|yes}}', 'operating_income': '{{decrease}} {{US$|22.27 billion}}', 'net_income': '{{decrease}} {{US$|15.52 billion}}', 'assets': '{{increase}} {{US$|264.82 billion}}', 'equity': '{{increase}} {{US$|53.14 billion}}', 'type': '[[Public company|Public]]', 'industry': '{{Plainlist|\\n*[[Telecommunications industry|Telecommunications]]\\n*[[Mass media]]}}', 'products': '{{Plainlist|\\n*[[Cable television]]\\n*[[Landline]]\\n*[[Mobile phone]]\\n*[[Broadband]]\\n*[[Digital television]]\\n*[[IPTV]]\\n*[[Digital Media]]\\n*[[Internet of things|Internet]]\\n*[[Telematics]]}}', 'num_employees': '135,400 (2020)', 'company_name': 'Verizon Communications'}, {'founder': '[[Bernard Kroger]]', 'location_country': 'U.S.', 'revenue': '{{increase}} {{US$|121.16 billion|link|=|yes}} (2019)', 'operating_income': '{{increase}} {{US$|2.67 billion}} (2019)', 'net_income': '{{increase}} {{US$|3.11 billion}} (2019)', 'assets': '{{increase}} {{US$|38.11 billion}} (2019)', 'equity': '{{increase}} {{US$|7.88 billion}} (2019)', 'type': '[[Public company|Public]]', 'industry': '[[Retail]]', 'products': '[[Supercenter]]/[[superstore]],<br>Other specialty, [[supermarket]]', 'num_employees': '453,000 (2019)', 'company_name': 'Kroger'}, {'founder': '', 'location_country': '', 'revenue': '{{nowrap|Increase| [[US$]] 121.615 billion |small|(2018)}} {{Increase}} [[US$]] 121.615 billion {{small|(2018)}}', 'operating_income': '{{nowrap|Decrease| US$ |color|red|&minus;20.717| billion |small|(2018)}} {{Decrease}} US$ {{color|red|&minus;20.717}} billion {{small|(2018)}}', 'net_income': '{{nowrap|Decrease| US$ |color|red|&minus;22.355| billion |small|(2018)}} {{Decrease}} US$ {{color|red|&minus;22.355}} billion {{small|(2018)}}', 'assets': '{{nowrap|Decrease| US$ 309.129 billion |small|(2018)}} {{Decrease}} US$ 309.129 billion {{small|(2018)}}', 'equity': '{{nowrap|Decrease| US$ 30.981 billion |small|(2018)}} {{Decrease}} US$ 30.981 billion {{small|(2018)}}', 'type': '[[Public company|Public]]', 'industry': '[[Conglomerate (company)|Conglomerate]]', 'products': '{{hlist|[[Aircraft engine]]s|[[Electric power distribution|Electrical distribution]]|[[Electric motor]]s|[[Energy]]|[[Finance]]|[[Health care]]|[[Lighting]]|[[Software]]|[[Wind turbine]]s}}', 'num_employees': '283,000 {{small|(2018)}}', 'company_name': 'General Electric'}, {'founder': '', 'location_country': '', 'revenue': \"{{nowrap|increase| |US$|136.9 billion|link|=|yes|ref| name='10-K'|{{cite web|url=https://sec.report/Document/0001618921-19-000069/ |title=Walgreens Boots Alliance Annual Report (Form 10-K) |publisher=[[U.S. Securities and Exchange Commission]]}}|</ref>}} {{increase}} {{US$|136.9 billion|link|=|yes}}\", 'operating_income': '{{decrease}} {{US$|4.9 billion}}', 'net_income': '{{decrease}} {{US$|3.9 billion}}', 'assets': '{{decrease}} {{US$|67.59 billion}}', 'equity': '{{decrease}} {{US$|24.15 billion}}', 'type': '[[Public company|Public]]', 'industry': '[[Pharmaceutical]]<br>[[Retail]]', 'products': '[[Drug store]]<br>[[Pharmacy]]', 'num_employees': '440,000', 'company_name': 'Walgreens Boots Alliance'}, {'founder': '[[Aaron Burr]] (Bank of the Manhattan Company)<br>[[J. P. Morgan|John Pierpont Morgan]]', 'location_country': '', 'revenue': '{{increase}} [[United States dollar|US$]]115.627 [[Billion (short scale)|billion]]', 'operating_income': '{{increase}} [[United States dollar|US$]]44.545 billion', 'net_income': '{{increase}} [[United States dollar|US$]]36.431 billion', 'assets': '{{increase}} [[United States dollar|US$]]2.687 [[trillion]]', 'equity': '{{increase}} [[United States dollar|US$]]261.330 billion', 'type': '[[Public company|Public]]', 'industry': '[[Bank]]ing<br>[[Financial services]]', 'products': '[[Alternative financial service]]s, [[American depositary receipt]]s, [[asset allocation]], [[asset management]], [[Bond (finance)|bond]] trading, [[broker]] services, [[capital market]] services, [[collateralized debt obligation]]s, [[commercial banking]], [[commodity market|commodities]] trading, [[commercial bank]]ing, [[credit card]]s, [[credit default swap]], [[credit derivative]] trading, [[currency exchange]], [[custodian bank]]ing, [[debt settlement]], [[digital banking]], [[estate planning]], [[exchange-traded fund]]s, [[financial analysis]], [[financial market]]s, [[foreign exchange market]], [[futures exchange]], [[hedge fund]]s, [[index fund]]s, [[information processing]], [[institutional investor|institutional investing]], [[insurance]], [[investment bank]]ing, [[Financial capital|investment capital]], [[investment management]], investment [[Portfolio (finance)|portfolios]], [[loan servicing]], [[merchant services]], [[mobile banking]], [[money market]] trading, [[mortgage brokers|mortgage broker]]ing, [[mortgage loan]]s, [[Mortgage-backed security|mortgage\u2013backed securities]], [[mutual fund]]s, [[pension fund]]s, [[prime brokerage]], [[private banking]], [[private equity]], [[remittance]], [[retail banking]], retail [[broker]]age, [[risk management]], [[securities lending]], [[Security (finance)|security]] services, [[stock trader|stock trading]], [[subprime lending]], [[treasury services]], [[trustee]] services, [[underwriting]], [[venture capital]], [[wealth management]], [[wholesale funding]], [[Wholesale mortgage lenders|wholesale mortgage lending]], [[wire transfer]]s', 'num_employees': '{{increase}} 256,981', 'company_name': 'JPMorgan Chase'}] Finally, let's export all the scapped infoboxes as a single JSON file to a convenient location as follows, 1 2 with open ( '../data/infoboxes.json' , 'w' ) as file : json . dump ( wiki_data , file ) Import : 1 2 with open ( '../data/infoboxes.json' , 'r' ) as file : wiki_data = json . load ( file )","title":"wptools"},{"location":"section-3-API-based-scraping/#references","text":"https://phpenthusiast.com/blog/what-is-rest-api 1","title":"References"},{"location":"section-4-wrangling-and-analysis/","text":"Wrangling and Analysis \u00b6 For this excercise, we will primarily focus on product industry assets What type of products are sold by the top 20 companies? \u00b6 1 companies 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ['Walmart', 'ExxonMobil', 'Berkshire Hathaway', 'Apple Inc.', 'UnitedHealth Group', 'McKesson Corporation', 'CVS Health', 'Amazon (company)', 'AT&T', 'General Motors', 'Ford Motor Company', 'AmerisourceBergen', 'Chevron Corporation', 'Cardinal Health', 'Costco', 'Verizon Communications', 'Kroger', 'General Electric', 'Walgreens Boots Alliance', 'JPMorgan Chase'] 1 2 3 4 5 6 7 8 9 10 11 12 regex1 = re . compile ( '[\\{\\[]+(.*?)[\\]\\}]' ) regex2 = re . compile ( '[^a-zA-Z\\- ]' ) products = [] data = [] for x in wiki_data : y = x [ 'products' ] # get products z = regex1 . findall ( y ) # extract all products z = [ d . lower () . split ( '|' ) for d in z ] # get a list m = list ( itertools . chain ( * z )) # flatten the list; add alternative vanilla python m = [ regex2 . sub ( '' , t ) for t in m if t != 'hlist' ] # remove hlist (a rogue token) data . append ({ 'wiki_title' : x [ 'company_name' ], 'product' : '|' . join ( m )}) products . extend ( m ) 1 print ( products ) 1 ['electronics', 'movies and music', 'home and furniture', 'home improvement', 'clothing', 'footwear', 'jewelry', 'toys', 'health and beauty', 'pet supplies', 'sporting goods and fitness', 'auto', 'photo finishing', 'craft supplies', 'party supplies', 'grocery', 'crude oil', 'oil products', 'natural gas', 'petrochemical', 'power generation', 'investment', 'diversified investments', 'insurancetypes', 'property casualty insurance', 'public utility', 'utilities', 'restaurants', 'food processing', 'aerospace', 'toys', 'mass media', 'media', 'automotive industry', 'automotive', 'sports equipment', 'sporting goods', 'final good', 'consumer products', 'internet', 'real estate', 'macintosh', 'ipod', 'iphone', 'ipad', 'apple watch', 'apple tv', 'homepod', 'macos', 'ios', 'ipados', 'watchos', 'tvos', 'ilife', 'iwork', 'final cut pro', 'logic pro', 'garageband', 'shazam application', 'shazam', 'siri', 'uniprise', 'health care', 'service economics', 'services', 'ingenix', 'pharmaceuticals', 'medical technology', 'health care services', 'amazon echo', 'amazon fire tablet', 'amazon fire', 'amazon fire tv', 'fire os', 'amazon fire os', 'amazon kindle', 'satellite television', 'landline', 'fixed-line telephones', 'mobile phone', 'mobile telephones', 'internet service provider', 'internet services', 'broadband', 'digital television', 'home security', 'iptv', 'over-the-top media services', 'ott services', 'network security', 'filmmaking', 'film production', 'television production', 'cable television', 'pay television', 'publishing', 'podcast', 'sports management', 'news agency', 'video game', 'car', 'automobiles', 'commercial vehicle', 'car', 'automobiles', 'luxury car', 'luxury vehicles', 'commercial vehicle', 'commercial vehicles', 'list of auto parts', 'automotive parts', 'pickup trucks', 'suvs', 'pharmaceutical', 'pharmacy', 'petroleum', 'natural gas', 'petrochemical', 'marketing brands', 'see chevron products', 'cable television', 'landline', 'mobile phone', 'broadband', 'digital television', 'iptv', 'digital media', 'internet of things', 'internet', 'telematics', 'supercenter', 'superstore', 'supermarket', 'aircraft engine', 'electric power distribution', 'electrical distribution', 'electric motor', 'energy', 'finance', 'health care', 'lighting', 'software', 'wind turbine', 'drug store', 'pharmacy', 'alternative financial service', 'american depositary receipt', 'asset allocation', 'asset management', 'bond finance', 'bond', 'broker', 'capital market', 'collateralized debt obligation', 'commercial banking', 'commodity market', 'commodities', 'commercial bank', 'credit card', 'credit default swap', 'credit derivative', 'currency exchange', 'custodian bank', 'debt settlement', 'digital banking', 'estate planning', 'exchange-traded fund', 'financial analysis', 'financial market', 'foreign exchange market', 'futures exchange', 'hedge fund', 'index fund', 'information processing', 'institutional investor', 'institutional investing', 'insurance', 'investment bank', 'financial capital', 'investment capital', 'investment management', 'portfolio finance', 'portfolios', 'loan servicing', 'merchant services', 'mobile banking', 'money market', 'mortgage brokers', 'mortgage broker', 'mortgage loan', 'mortgage-backed security', 'mortgagebacked securities', 'mutual fund', 'pension fund', 'prime brokerage', 'private banking', 'private equity', 'remittance', 'retail banking', 'broker', 'risk management', 'securities lending', 'security finance', 'security', 'stock trader', 'stock trading', 'subprime lending', 'treasury services', 'trustee', 'underwriting', 'venture capital', 'wealth management', 'wholesale funding', 'wholesale mortgage lenders', 'wholesale mortgage lending', 'wire transfer'] To create wordclouds, 1 2 3 4 5 6 7 8 9 10 11 def create_wordcloud ( items , stopwords = []): # Create the wordcloud object text = ' ' . join ( items ) wordcloud = WordCloud ( width = 1000 , height = 800 , margin = 0 , stopwords = stopwords ) . generate ( text ) # max_words=20 # Display the generated image: plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . axis ( \"off\" ) plt . margins ( x = 0 , y = 0 ) plt . show () 1 create_wordcloud ( products , [ 'and' ]) What type of industries do the top 20 company belong from? \u00b6 1 2 3 4 5 6 7 8 9 regex = re . compile ( '[\\[]+(.*?)[\\]]' ) industries = [] for i , x in enumerate ( wiki_data ): y = x [ 'industry' ] # get industries z = regex . findall ( y ) # extract industries z = [ d . lower () . split ( '|' ) for d in z ] # get a list m = list ( itertools . chain ( * z )) # flatten data [ i ][ 'industry' ] = '|' . join ( m ) industries . extend ( m ) 1 create_wordcloud ( industries , [ 'industry' , 'and' ]) What the assets of the top 20 companies look like? \u00b6 1 2 3 4 5 6 7 8 9 10 regex = re . compile ( '([\\d\\.]+)(?!billion|million|trillion)' ) assets = [] for i , x in enumerate ( wiki_data ): y = x [ 'assets' ] # get assets z = regex . findall ( y ) # extract assets u = re . findall ( '(billion|million|trillion)' , y ) # extract the unit asset = float ( z [ 0 ]) # get the numeric value unit = u [ 0 ] data [ i ][ 'assets' ] = str ( asset ) + ' ' + unit assets . append ({ x [ 'company_name' ] : ( asset , unit )}) 1 assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [{'Walmart': (219.295, 'billion')}, {'ExxonMobil': (346.2, 'billion')}, {'Berkshire Hathaway': (707.8, 'billion')}, {'Apple Inc.': (338.516, 'billion')}, {'UnitedHealth Group': (173.889, 'billion')}, {'McKesson Corporation': (60.381, 'billion')}, {'CVS Health': (196.456, 'billion')}, {'Amazon (company)': (162.648, 'billion')}, {'AT&T': (531.0, 'billion')}, {'General Motors': (227.339, 'billion')}, {'Ford Motor Company': (256.54, 'billion')}, {'AmerisourceBergen': (37.66, 'billion')}, {'Chevron Corporation': (253.9, 'billion')}, {'Cardinal Health': (39.95, 'billion')}, {'Costco': (45.4, 'billion')}, {'Verizon Communications': (264.82, 'billion')}, {'Kroger': (38.11, 'billion')}, {'General Electric': (309.129, 'billion')}, {'Walgreens Boots Alliance': (67.59, 'billion')}, {'JPMorgan Chase': (2.687, 'trillion')}] Normalize all the values/units, 1 2 3 4 for i , asset in enumerate ( assets ): for k , v in asset . items (): if v [ 1 ] == 'trillion' : assets [ i ][ k ] = ( v [ 0 ] * 1000 , 'billion' ) 1 assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [{'Walmart': (219.295, 'billion')}, {'ExxonMobil': (346.2, 'billion')}, {'Berkshire Hathaway': (707.8, 'billion')}, {'Apple Inc.': (338.516, 'billion')}, {'UnitedHealth Group': (173.889, 'billion')}, {'McKesson Corporation': (60.381, 'billion')}, {'CVS Health': (196.456, 'billion')}, {'Amazon (company)': (162.648, 'billion')}, {'AT&T': (531.0, 'billion')}, {'General Motors': (227.339, 'billion')}, {'Ford Motor Company': (256.54, 'billion')}, {'AmerisourceBergen': (37.66, 'billion')}, {'Chevron Corporation': (253.9, 'billion')}, {'Cardinal Health': (39.95, 'billion')}, {'Costco': (45.4, 'billion')}, {'Verizon Communications': (264.82, 'billion')}, {'Kroger': (38.11, 'billion')}, {'General Electric': (309.129, 'billion')}, {'Walgreens Boots Alliance': (67.59, 'billion')}, {'JPMorgan Chase': (2687.0, 'billion')}] 1 2 3 4 5 6 7 8 9 10 x = [ list ( a . keys ())[ 0 ] for a in assets ] energy = [ list ( a . values ())[ 0 ][ 0 ] for a in assets ] x_pos = [ i for i , _ in enumerate ( x )] plt . bar ( x_pos , energy ) plt . ylabel ( \"Assets (in Billions)\" ) plt . xlabel ( \"Company Name\" ) plt . title ( \"Assets from the Top 20 Companies on Fortune 500\" ) plt . xticks ( x_pos , x , rotation = 90 ) plt . show () 1 data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 [{'wiki_title': 'Walmart', 'product': 'electronics|movies and music|home and furniture|home improvement|clothing|footwear|jewelry|toys|health and beauty|pet supplies|sporting goods and fitness|auto|photo finishing|craft supplies|party supplies|grocery', 'industry': 'retail', 'assets': '219.295 billion'}, {'wiki_title': 'ExxonMobil', 'product': 'crude oil|oil products|natural gas|petrochemical|power generation', 'industry': 'energy industry|energy|oil and gas industry|oil and gas', 'assets': '346.2 billion'}, {'wiki_title': 'Berkshire Hathaway', 'product': 'investment|diversified investments|insurancetypes|property casualty insurance|public utility|utilities|restaurants|food processing|aerospace|toys|mass media|media|automotive industry|automotive|sports equipment|sporting goods|final good|consumer products|internet|real estate', 'industry': 'conglomerate (company)|conglomerate', 'assets': '707.8 billion'}, {'wiki_title': 'Apple Inc.', 'product': 'macintosh|ipod|iphone|ipad|apple watch|apple tv|homepod|macos|ios|ipados|watchos|tvos|ilife|iwork|final cut pro|logic pro|garageband|shazam application|shazam|siri', 'industry': 'computer hardware|computer software|consumer electronics|cloud computing|digital distribution|fabless manufacturing|fabless silicon design|semiconductors|financial technology|artificial intelligence', 'assets': '338.516 billion'}, {'wiki_title': 'UnitedHealth Group', 'product': 'uniprise|health care|service economics|services|ingenix', 'industry': 'managed health care', 'assets': '173.889 billion'}, {'wiki_title': 'McKesson Corporation', 'product': 'pharmaceuticals|medical technology|health care services', 'industry': 'healthcare', 'assets': '60.381 billion'}, {'wiki_title': 'CVS Health', 'product': '', 'industry': 'retail|health care', 'assets': '196.456 billion'}, {'wiki_title': 'Amazon (company)', 'product': 'amazon echo|amazon fire tablet|amazon fire|amazon fire tv|fire os|amazon fire os|amazon kindle', 'industry': 'cloud computing|e-commerce|artificial intelligence|consumer electronics|digital distribution|grocery stores', 'assets': '162.648 billion'}, {'wiki_title': 'AT&T', 'product': 'satellite television|landline|fixed-line telephones|mobile phone|mobile telephones|internet service provider|internet services|broadband|digital television|home security|iptv|over-the-top media services|ott services|network security|filmmaking|film production|television production|cable television|pay television|publishing|podcast|sports management|news agency|video game', 'industry': 'telecommunications industry|telecommunications|technology company|technology|mass media|entertainment', 'assets': '531.0 billion'}, {'wiki_title': 'General Motors', 'product': 'car|automobiles|commercial vehicle', 'industry': 'automotive industry|automotive', 'assets': '227.339 billion'}, {'wiki_title': 'Ford Motor Company', 'product': 'car|automobiles|luxury car|luxury vehicles|commercial vehicle|commercial vehicles|list of auto parts|automotive parts|pickup trucks|suvs', 'industry': 'automotive industry|automotive', 'assets': '256.54 billion'}, {'wiki_title': 'AmerisourceBergen', 'product': 'pharmaceutical|pharmacy', 'industry': 'pharmaceutical', 'assets': '37.66 billion'}, {'wiki_title': 'Chevron Corporation', 'product': 'petroleum|natural gas|petrochemical|marketing brands|see chevron products', 'industry': 'oil and gas industry|oil and gas', 'assets': '253.9 billion'}, {'wiki_title': 'Cardinal Health', 'product': '', 'industry': 'pharmaceuticals', 'assets': '39.95 billion'}, {'wiki_title': 'Costco', 'product': '', 'industry': 'retail', 'assets': '45.4 billion'}, {'wiki_title': 'Verizon Communications', 'product': 'cable television|landline|mobile phone|broadband|digital television|iptv|digital media|internet of things|internet|telematics', 'industry': 'telecommunications industry|telecommunications|mass media', 'assets': '264.82 billion'}, {'wiki_title': 'Kroger', 'product': 'supercenter|superstore|supermarket', 'industry': 'retail', 'assets': '38.11 billion'}, {'wiki_title': 'General Electric', 'product': 'aircraft engine|electric power distribution|electrical distribution|electric motor|energy|finance|health care|lighting|software|wind turbine', 'industry': 'conglomerate (company)|conglomerate', 'assets': '309.129 billion'}, {'wiki_title': 'Walgreens Boots Alliance', 'product': 'drug store|pharmacy', 'industry': 'pharmaceutical|retail', 'assets': '67.59 billion'}, {'wiki_title': 'JPMorgan Chase', 'product': 'alternative financial service|american depositary receipt|asset allocation|asset management|bond finance|bond|broker|capital market|collateralized debt obligation|commercial banking|commodity market|commodities|commercial bank|credit card|credit default swap|credit derivative|currency exchange|custodian bank|debt settlement|digital banking|estate planning|exchange-traded fund|financial analysis|financial market|foreign exchange market|futures exchange|hedge fund|index fund|information processing|institutional investor|institutional investing|insurance|investment bank|financial capital|investment capital|investment management|portfolio finance|portfolios|loan servicing|merchant services|mobile banking|money market|mortgage brokers|mortgage broker|mortgage loan|mortgage-backed security|mortgagebacked securities|mutual fund|pension fund|prime brokerage|private banking|private equity|remittance|retail banking|broker|risk management|securities lending|security finance|security|stock trader|stock trading|subprime lending|treasury services|trustee|underwriting|venture capital|wealth management|wholesale funding|wholesale mortgage lenders|wholesale mortgage lending|wire transfer', 'industry': 'bank|financial services', 'assets': '2.687 trillion'}] 1 2 df_new = pd . DataFrame ( data ) df_new . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } wiki_title product industry assets 0 Walmart electronics|movies and music|home and furnitur... retail 219.295 billion 1 ExxonMobil crude oil|oil products|natural gas|petrochemic... energy industry|energy|oil and gas industry|oi... 346.2 billion 2 Berkshire Hathaway investment|diversified investments|insurancety... conglomerate (company)|conglomerate 707.8 billion 3 Apple Inc. macintosh|ipod|iphone|ipad|apple watch|apple t... computer hardware|computer software|consumer e... 338.516 billion 4 UnitedHealth Group uniprise|health care|service economics|service... managed health care 173.889 billion 1 df = pd . concat ([ df_sub , df_new ], axis = 1 ) 1 df . to_csv ( './data/top_20_companies.csv' , index = False ) 1","title":"Wrangling and Analysis"},{"location":"section-4-wrangling-and-analysis/#wrangling-and-analysis","text":"For this excercise, we will primarily focus on product industry assets","title":"Wrangling and Analysis"},{"location":"section-4-wrangling-and-analysis/#what-type-of-products-are-sold-by-the-top-20-companies","text":"1 companies 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ['Walmart', 'ExxonMobil', 'Berkshire Hathaway', 'Apple Inc.', 'UnitedHealth Group', 'McKesson Corporation', 'CVS Health', 'Amazon (company)', 'AT&T', 'General Motors', 'Ford Motor Company', 'AmerisourceBergen', 'Chevron Corporation', 'Cardinal Health', 'Costco', 'Verizon Communications', 'Kroger', 'General Electric', 'Walgreens Boots Alliance', 'JPMorgan Chase'] 1 2 3 4 5 6 7 8 9 10 11 12 regex1 = re . compile ( '[\\{\\[]+(.*?)[\\]\\}]' ) regex2 = re . compile ( '[^a-zA-Z\\- ]' ) products = [] data = [] for x in wiki_data : y = x [ 'products' ] # get products z = regex1 . findall ( y ) # extract all products z = [ d . lower () . split ( '|' ) for d in z ] # get a list m = list ( itertools . chain ( * z )) # flatten the list; add alternative vanilla python m = [ regex2 . sub ( '' , t ) for t in m if t != 'hlist' ] # remove hlist (a rogue token) data . append ({ 'wiki_title' : x [ 'company_name' ], 'product' : '|' . join ( m )}) products . extend ( m ) 1 print ( products ) 1 ['electronics', 'movies and music', 'home and furniture', 'home improvement', 'clothing', 'footwear', 'jewelry', 'toys', 'health and beauty', 'pet supplies', 'sporting goods and fitness', 'auto', 'photo finishing', 'craft supplies', 'party supplies', 'grocery', 'crude oil', 'oil products', 'natural gas', 'petrochemical', 'power generation', 'investment', 'diversified investments', 'insurancetypes', 'property casualty insurance', 'public utility', 'utilities', 'restaurants', 'food processing', 'aerospace', 'toys', 'mass media', 'media', 'automotive industry', 'automotive', 'sports equipment', 'sporting goods', 'final good', 'consumer products', 'internet', 'real estate', 'macintosh', 'ipod', 'iphone', 'ipad', 'apple watch', 'apple tv', 'homepod', 'macos', 'ios', 'ipados', 'watchos', 'tvos', 'ilife', 'iwork', 'final cut pro', 'logic pro', 'garageband', 'shazam application', 'shazam', 'siri', 'uniprise', 'health care', 'service economics', 'services', 'ingenix', 'pharmaceuticals', 'medical technology', 'health care services', 'amazon echo', 'amazon fire tablet', 'amazon fire', 'amazon fire tv', 'fire os', 'amazon fire os', 'amazon kindle', 'satellite television', 'landline', 'fixed-line telephones', 'mobile phone', 'mobile telephones', 'internet service provider', 'internet services', 'broadband', 'digital television', 'home security', 'iptv', 'over-the-top media services', 'ott services', 'network security', 'filmmaking', 'film production', 'television production', 'cable television', 'pay television', 'publishing', 'podcast', 'sports management', 'news agency', 'video game', 'car', 'automobiles', 'commercial vehicle', 'car', 'automobiles', 'luxury car', 'luxury vehicles', 'commercial vehicle', 'commercial vehicles', 'list of auto parts', 'automotive parts', 'pickup trucks', 'suvs', 'pharmaceutical', 'pharmacy', 'petroleum', 'natural gas', 'petrochemical', 'marketing brands', 'see chevron products', 'cable television', 'landline', 'mobile phone', 'broadband', 'digital television', 'iptv', 'digital media', 'internet of things', 'internet', 'telematics', 'supercenter', 'superstore', 'supermarket', 'aircraft engine', 'electric power distribution', 'electrical distribution', 'electric motor', 'energy', 'finance', 'health care', 'lighting', 'software', 'wind turbine', 'drug store', 'pharmacy', 'alternative financial service', 'american depositary receipt', 'asset allocation', 'asset management', 'bond finance', 'bond', 'broker', 'capital market', 'collateralized debt obligation', 'commercial banking', 'commodity market', 'commodities', 'commercial bank', 'credit card', 'credit default swap', 'credit derivative', 'currency exchange', 'custodian bank', 'debt settlement', 'digital banking', 'estate planning', 'exchange-traded fund', 'financial analysis', 'financial market', 'foreign exchange market', 'futures exchange', 'hedge fund', 'index fund', 'information processing', 'institutional investor', 'institutional investing', 'insurance', 'investment bank', 'financial capital', 'investment capital', 'investment management', 'portfolio finance', 'portfolios', 'loan servicing', 'merchant services', 'mobile banking', 'money market', 'mortgage brokers', 'mortgage broker', 'mortgage loan', 'mortgage-backed security', 'mortgagebacked securities', 'mutual fund', 'pension fund', 'prime brokerage', 'private banking', 'private equity', 'remittance', 'retail banking', 'broker', 'risk management', 'securities lending', 'security finance', 'security', 'stock trader', 'stock trading', 'subprime lending', 'treasury services', 'trustee', 'underwriting', 'venture capital', 'wealth management', 'wholesale funding', 'wholesale mortgage lenders', 'wholesale mortgage lending', 'wire transfer'] To create wordclouds, 1 2 3 4 5 6 7 8 9 10 11 def create_wordcloud ( items , stopwords = []): # Create the wordcloud object text = ' ' . join ( items ) wordcloud = WordCloud ( width = 1000 , height = 800 , margin = 0 , stopwords = stopwords ) . generate ( text ) # max_words=20 # Display the generated image: plt . imshow ( wordcloud , interpolation = 'bilinear' ) plt . axis ( \"off\" ) plt . margins ( x = 0 , y = 0 ) plt . show () 1 create_wordcloud ( products , [ 'and' ])","title":"What type of products are sold by the top 20 companies?"},{"location":"section-4-wrangling-and-analysis/#what-type-of-industries-do-the-top-20-company-belong-from","text":"1 2 3 4 5 6 7 8 9 regex = re . compile ( '[\\[]+(.*?)[\\]]' ) industries = [] for i , x in enumerate ( wiki_data ): y = x [ 'industry' ] # get industries z = regex . findall ( y ) # extract industries z = [ d . lower () . split ( '|' ) for d in z ] # get a list m = list ( itertools . chain ( * z )) # flatten data [ i ][ 'industry' ] = '|' . join ( m ) industries . extend ( m ) 1 create_wordcloud ( industries , [ 'industry' , 'and' ])","title":"What type of industries do the top 20 company belong from?"},{"location":"section-4-wrangling-and-analysis/#what-the-assets-of-the-top-20-companies-look-like","text":"1 2 3 4 5 6 7 8 9 10 regex = re . compile ( '([\\d\\.]+)(?!billion|million|trillion)' ) assets = [] for i , x in enumerate ( wiki_data ): y = x [ 'assets' ] # get assets z = regex . findall ( y ) # extract assets u = re . findall ( '(billion|million|trillion)' , y ) # extract the unit asset = float ( z [ 0 ]) # get the numeric value unit = u [ 0 ] data [ i ][ 'assets' ] = str ( asset ) + ' ' + unit assets . append ({ x [ 'company_name' ] : ( asset , unit )}) 1 assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [{'Walmart': (219.295, 'billion')}, {'ExxonMobil': (346.2, 'billion')}, {'Berkshire Hathaway': (707.8, 'billion')}, {'Apple Inc.': (338.516, 'billion')}, {'UnitedHealth Group': (173.889, 'billion')}, {'McKesson Corporation': (60.381, 'billion')}, {'CVS Health': (196.456, 'billion')}, {'Amazon (company)': (162.648, 'billion')}, {'AT&T': (531.0, 'billion')}, {'General Motors': (227.339, 'billion')}, {'Ford Motor Company': (256.54, 'billion')}, {'AmerisourceBergen': (37.66, 'billion')}, {'Chevron Corporation': (253.9, 'billion')}, {'Cardinal Health': (39.95, 'billion')}, {'Costco': (45.4, 'billion')}, {'Verizon Communications': (264.82, 'billion')}, {'Kroger': (38.11, 'billion')}, {'General Electric': (309.129, 'billion')}, {'Walgreens Boots Alliance': (67.59, 'billion')}, {'JPMorgan Chase': (2.687, 'trillion')}] Normalize all the values/units, 1 2 3 4 for i , asset in enumerate ( assets ): for k , v in asset . items (): if v [ 1 ] == 'trillion' : assets [ i ][ k ] = ( v [ 0 ] * 1000 , 'billion' ) 1 assets 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [{'Walmart': (219.295, 'billion')}, {'ExxonMobil': (346.2, 'billion')}, {'Berkshire Hathaway': (707.8, 'billion')}, {'Apple Inc.': (338.516, 'billion')}, {'UnitedHealth Group': (173.889, 'billion')}, {'McKesson Corporation': (60.381, 'billion')}, {'CVS Health': (196.456, 'billion')}, {'Amazon (company)': (162.648, 'billion')}, {'AT&T': (531.0, 'billion')}, {'General Motors': (227.339, 'billion')}, {'Ford Motor Company': (256.54, 'billion')}, {'AmerisourceBergen': (37.66, 'billion')}, {'Chevron Corporation': (253.9, 'billion')}, {'Cardinal Health': (39.95, 'billion')}, {'Costco': (45.4, 'billion')}, {'Verizon Communications': (264.82, 'billion')}, {'Kroger': (38.11, 'billion')}, {'General Electric': (309.129, 'billion')}, {'Walgreens Boots Alliance': (67.59, 'billion')}, {'JPMorgan Chase': (2687.0, 'billion')}] 1 2 3 4 5 6 7 8 9 10 x = [ list ( a . keys ())[ 0 ] for a in assets ] energy = [ list ( a . values ())[ 0 ][ 0 ] for a in assets ] x_pos = [ i for i , _ in enumerate ( x )] plt . bar ( x_pos , energy ) plt . ylabel ( \"Assets (in Billions)\" ) plt . xlabel ( \"Company Name\" ) plt . title ( \"Assets from the Top 20 Companies on Fortune 500\" ) plt . xticks ( x_pos , x , rotation = 90 ) plt . show () 1 data 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 [{'wiki_title': 'Walmart', 'product': 'electronics|movies and music|home and furniture|home improvement|clothing|footwear|jewelry|toys|health and beauty|pet supplies|sporting goods and fitness|auto|photo finishing|craft supplies|party supplies|grocery', 'industry': 'retail', 'assets': '219.295 billion'}, {'wiki_title': 'ExxonMobil', 'product': 'crude oil|oil products|natural gas|petrochemical|power generation', 'industry': 'energy industry|energy|oil and gas industry|oil and gas', 'assets': '346.2 billion'}, {'wiki_title': 'Berkshire Hathaway', 'product': 'investment|diversified investments|insurancetypes|property casualty insurance|public utility|utilities|restaurants|food processing|aerospace|toys|mass media|media|automotive industry|automotive|sports equipment|sporting goods|final good|consumer products|internet|real estate', 'industry': 'conglomerate (company)|conglomerate', 'assets': '707.8 billion'}, {'wiki_title': 'Apple Inc.', 'product': 'macintosh|ipod|iphone|ipad|apple watch|apple tv|homepod|macos|ios|ipados|watchos|tvos|ilife|iwork|final cut pro|logic pro|garageband|shazam application|shazam|siri', 'industry': 'computer hardware|computer software|consumer electronics|cloud computing|digital distribution|fabless manufacturing|fabless silicon design|semiconductors|financial technology|artificial intelligence', 'assets': '338.516 billion'}, {'wiki_title': 'UnitedHealth Group', 'product': 'uniprise|health care|service economics|services|ingenix', 'industry': 'managed health care', 'assets': '173.889 billion'}, {'wiki_title': 'McKesson Corporation', 'product': 'pharmaceuticals|medical technology|health care services', 'industry': 'healthcare', 'assets': '60.381 billion'}, {'wiki_title': 'CVS Health', 'product': '', 'industry': 'retail|health care', 'assets': '196.456 billion'}, {'wiki_title': 'Amazon (company)', 'product': 'amazon echo|amazon fire tablet|amazon fire|amazon fire tv|fire os|amazon fire os|amazon kindle', 'industry': 'cloud computing|e-commerce|artificial intelligence|consumer electronics|digital distribution|grocery stores', 'assets': '162.648 billion'}, {'wiki_title': 'AT&T', 'product': 'satellite television|landline|fixed-line telephones|mobile phone|mobile telephones|internet service provider|internet services|broadband|digital television|home security|iptv|over-the-top media services|ott services|network security|filmmaking|film production|television production|cable television|pay television|publishing|podcast|sports management|news agency|video game', 'industry': 'telecommunications industry|telecommunications|technology company|technology|mass media|entertainment', 'assets': '531.0 billion'}, {'wiki_title': 'General Motors', 'product': 'car|automobiles|commercial vehicle', 'industry': 'automotive industry|automotive', 'assets': '227.339 billion'}, {'wiki_title': 'Ford Motor Company', 'product': 'car|automobiles|luxury car|luxury vehicles|commercial vehicle|commercial vehicles|list of auto parts|automotive parts|pickup trucks|suvs', 'industry': 'automotive industry|automotive', 'assets': '256.54 billion'}, {'wiki_title': 'AmerisourceBergen', 'product': 'pharmaceutical|pharmacy', 'industry': 'pharmaceutical', 'assets': '37.66 billion'}, {'wiki_title': 'Chevron Corporation', 'product': 'petroleum|natural gas|petrochemical|marketing brands|see chevron products', 'industry': 'oil and gas industry|oil and gas', 'assets': '253.9 billion'}, {'wiki_title': 'Cardinal Health', 'product': '', 'industry': 'pharmaceuticals', 'assets': '39.95 billion'}, {'wiki_title': 'Costco', 'product': '', 'industry': 'retail', 'assets': '45.4 billion'}, {'wiki_title': 'Verizon Communications', 'product': 'cable television|landline|mobile phone|broadband|digital television|iptv|digital media|internet of things|internet|telematics', 'industry': 'telecommunications industry|telecommunications|mass media', 'assets': '264.82 billion'}, {'wiki_title': 'Kroger', 'product': 'supercenter|superstore|supermarket', 'industry': 'retail', 'assets': '38.11 billion'}, {'wiki_title': 'General Electric', 'product': 'aircraft engine|electric power distribution|electrical distribution|electric motor|energy|finance|health care|lighting|software|wind turbine', 'industry': 'conglomerate (company)|conglomerate', 'assets': '309.129 billion'}, {'wiki_title': 'Walgreens Boots Alliance', 'product': 'drug store|pharmacy', 'industry': 'pharmaceutical|retail', 'assets': '67.59 billion'}, {'wiki_title': 'JPMorgan Chase', 'product': 'alternative financial service|american depositary receipt|asset allocation|asset management|bond finance|bond|broker|capital market|collateralized debt obligation|commercial banking|commodity market|commodities|commercial bank|credit card|credit default swap|credit derivative|currency exchange|custodian bank|debt settlement|digital banking|estate planning|exchange-traded fund|financial analysis|financial market|foreign exchange market|futures exchange|hedge fund|index fund|information processing|institutional investor|institutional investing|insurance|investment bank|financial capital|investment capital|investment management|portfolio finance|portfolios|loan servicing|merchant services|mobile banking|money market|mortgage brokers|mortgage broker|mortgage loan|mortgage-backed security|mortgagebacked securities|mutual fund|pension fund|prime brokerage|private banking|private equity|remittance|retail banking|broker|risk management|securities lending|security finance|security|stock trader|stock trading|subprime lending|treasury services|trustee|underwriting|venture capital|wealth management|wholesale funding|wholesale mortgage lenders|wholesale mortgage lending|wire transfer', 'industry': 'bank|financial services', 'assets': '2.687 trillion'}] 1 2 df_new = pd . DataFrame ( data ) df_new . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } wiki_title product industry assets 0 Walmart electronics|movies and music|home and furnitur... retail 219.295 billion 1 ExxonMobil crude oil|oil products|natural gas|petrochemic... energy industry|energy|oil and gas industry|oi... 346.2 billion 2 Berkshire Hathaway investment|diversified investments|insurancety... conglomerate (company)|conglomerate 707.8 billion 3 Apple Inc. macintosh|ipod|iphone|ipad|apple watch|apple t... computer hardware|computer software|consumer e... 338.516 billion 4 UnitedHealth Group uniprise|health care|service economics|service... managed health care 173.889 billion 1 df = pd . concat ([ df_sub , df_new ], axis = 1 ) 1 df . to_csv ( './data/top_20_companies.csv' , index = False ) 1","title":"What the assets of the top 20 companies look like?"},{"location":"section-5-legal-and-ethical-considerations/","text":"Legal and Ethical COnsiderations \u00b6 Now that we have seen several different ways to scrape data from websites and are ready to start working on potentially larger projects, we may ask ourselves whether there are any legal implications of writing a piece of computer code that downloads information from the Internet. In this section, we will be discussing some of the issues to be aware of when scraping websites, and we will establish a code of conduct (below) to guide our web scraping projects. This section does not constitute legal advice \u00b6 Please note that the information provided on this page is for information purposes only and does not constitute professional legal advice on the practice of web scraping. If you are concerned about the legal implications of using web scraping on a project you are working on, it is probably a good idea to seek advice from a professional, preferably someone who has knowledge of the intellectual property (copyright) legislation in effect in your country. Don't break the web: Denial of Service attacks \u00b6 The first and most important thing to be careful about when writing a web scraper is that it typically involves querying a website repeatedly and accessing a potentially large number of pages. For each of these pages, a request will be sent to the web server that is hosting the site, and the server will have to process the request and send a response back to the computer that is running our code. Each of these requests will consume resources on the server, during which it will not be doing something else, like for example responding to someone else trying to access the same site. If we send too many such requests over a short span of time, we can prevent other \u201cnormal\u201d users from accessing the site during that time, or even cause the server to run out of resources and crash. In fact, this is such an efficient way to disrupt a web site that hackers are often doing it on purpose. This is called a Denial of Service (DoS) attack . Since DoS attacks are unfortunately a common occurence on the Internet, modern web servers include measures to ward off such illegitimate use of their resources. They are watchful for large amounts of requests appearing to come from a single computer or IP address, and their first line of defense often involves refusing any further requests coming from this IP address. A web scraper, even one with legitimate purposes and no intent to bring a website down, can exhibit similar behaviour and, if we are not careful, result in our computer being banned from accessing a website. Don't steal: Copyright and fair use \u00b6 It is important to recognize that in certain circumstances web scraping can be illegal. If the terms and conditions of the web site we are scraping specifically prohibit downloading and copying its content, then we could be in trouble for scraping it. In practice, however, web scraping is a tolerated practice, provided reasonable care is taken not to disrupt the \u201cregular\u201d use of a web site, as we have seen above. In a sense, web scraping is no different than using a web browser to visit a web page, in that it amounts to using computer software (a browser vs a scraper) to acccess data that is publicly available on the web. In general, if data is publicly available (the content that is being scraped is not behind a password-protected authentication system), then it is OK to scrape it, provided we don\u2019t break the web site doing so. What is potentially problematic is if the scraped data will be shared further. For example, downloading content off one website and posting it on another website (as our own), unless explicitely permitted, would constitute copyright violation and be illegal. However, most copyright legislations recognize cases in which reusing some, possibly copyrighted, information in an aggregate or derivative format is considered \u201cfair use\u201d. In general, unless the intent is to pass off data as our own, copy it word for word or trying to make money out of it, reusing publicly available content scraped off the internet is OK. Better be safe than sorry \u00b6 Be aware that copyright and data privacy legislation typically differs from country to country. Be sure to check the laws that apply in your context. For example, in Australia, it can be illegal to scrape and store personal information such as names, phone numbers and email addresses, even if they are publicly available. If you are looking to scrape data for your own personal use, then the above guidelines should probably be all that you need to worry about. However, if you plan to start harvesting a large amount of data for research or commercial purposes, you should probably seek legal advice first. If you work in a university, chances are it has a copyright office that will help you sort out the legal aspects of your project. The university library is often the best place to start looking for help on copyright. Be nice: ask and share \u00b6 Depending on the scope of your project, it might be worthwhile to consider asking the owners or curators of the data you are planning to scrape if they have it already available in a structured format that could suit your project. If your aim is do use their data for research, or to use it in a way that could potentially interest them, not only it could save you the trouble of writing a web scraper, but it could also help clarify straight away what you can and cannot do with the data. On the other hand, when you are publishing your own data, as part of a research project, documentation or a public website, you might want to think about whether someone might be interested in getting your data for their own project. If you can, try to provide others with a way to download your raw data in a structured format, and thus save them the trouble to try and scrape your own pages! Web scraping code of conduct \u00b6 This all being said, if you adhere to the following simple rules, you will probably be fine. Ask nicely. If your project requires data from a particular organisation, for example, you can try asking them directly if they could provide you what you are looking for. With some luck, they will have the primary data that they used on their website in a structured format, saving you the trouble. Don't download copies of documents that are clearly not public. For example, academic journal publishers often have very strict rules about what you can and what you cannot do with their databases. Mass downloading article PDFs is probably prohibited and can put you (or at the very least your friendly university librarian) in trouble. If your project requires local copies of documents (e.g. for text mining projects), special agreements can be reached with the publisher. The library is a good place to start investigating something like that. Check your local legislation. For example, certain countries have laws protecting personal information such as email addresses and phone numbers. Scraping such information, even from publicly avaialable web sites, can be illegal (e.g. in Australia). Don't share downloaded content illegally. Scraping for personal purposes is usually OK, even if it is copyrighted information, as it could fall under the fair use provision of the intellectual property legislation. However, sharing data for which you don\u2019t hold the right to share is illegal. Share what you can. If the data you scraped is in the public domain or you got permission to share it, then put it out there for other people to reuse it (e.g. on datahub.io). If you wrote a web scraper to access it, share its code (e.g. on GitHub) so that others can benefit from it. Don't break the Internet. Not all web sites are designed to withstand thousands of requests per second. If you are writing a recursive scraper (i.e. that follows hyperlinks), test it on a smaller dataset first to make sure it does what it is supposed to do. Adjust the settings of your scraper to allow for a delay between requests. By default, Scrapy uses conservative settings that should minimize this risk. Publish your own data in a reusable way. Don\u2019t force others to write their own scrapers to get at your data. Use open and software-agnostic formats (e.g. JSON, XML), provide metadata (data about your data: where it came from, what it represents, how to use it, etc.) and make sure it can be indexed by search engines so that people can find it. Happy scraping! References \u00b6 The Web scraping Wikipedia page has a concise definition of many concepts discussed here. This case study is a great example of what can be done using web scraping and how to achieve it. This recent case about Linkedin data is a good read. Commencing 25 May 2018, Monash University will also become subject to the European Union\u2019s General Data Protection Regulation ( GDPR ). Software Carpentry is a non-profit organisation that runs learn-to-code workshops worldwide. All lessons are publicly available and can be followed indepentently. This lesson is heavily inspired by Software Carpentry. Data Carpentry is a sister organisation of Software Carpentry focused on the fundamental data management skills required to conduct research. Library Carpentry is another Software Carpentry spinoff focused on software skills for librarians.","title":"Legal and Ethical considerations"},{"location":"section-5-legal-and-ethical-considerations/#legal-and-ethical-considerations","text":"Now that we have seen several different ways to scrape data from websites and are ready to start working on potentially larger projects, we may ask ourselves whether there are any legal implications of writing a piece of computer code that downloads information from the Internet. In this section, we will be discussing some of the issues to be aware of when scraping websites, and we will establish a code of conduct (below) to guide our web scraping projects.","title":"Legal and Ethical COnsiderations"},{"location":"section-5-legal-and-ethical-considerations/#this-section-does-not-constitute-legal-advice","text":"Please note that the information provided on this page is for information purposes only and does not constitute professional legal advice on the practice of web scraping. If you are concerned about the legal implications of using web scraping on a project you are working on, it is probably a good idea to seek advice from a professional, preferably someone who has knowledge of the intellectual property (copyright) legislation in effect in your country.","title":"This section does not constitute legal advice"},{"location":"section-5-legal-and-ethical-considerations/#dont-break-the-web-denial-of-service-attacks","text":"The first and most important thing to be careful about when writing a web scraper is that it typically involves querying a website repeatedly and accessing a potentially large number of pages. For each of these pages, a request will be sent to the web server that is hosting the site, and the server will have to process the request and send a response back to the computer that is running our code. Each of these requests will consume resources on the server, during which it will not be doing something else, like for example responding to someone else trying to access the same site. If we send too many such requests over a short span of time, we can prevent other \u201cnormal\u201d users from accessing the site during that time, or even cause the server to run out of resources and crash. In fact, this is such an efficient way to disrupt a web site that hackers are often doing it on purpose. This is called a Denial of Service (DoS) attack . Since DoS attacks are unfortunately a common occurence on the Internet, modern web servers include measures to ward off such illegitimate use of their resources. They are watchful for large amounts of requests appearing to come from a single computer or IP address, and their first line of defense often involves refusing any further requests coming from this IP address. A web scraper, even one with legitimate purposes and no intent to bring a website down, can exhibit similar behaviour and, if we are not careful, result in our computer being banned from accessing a website.","title":"Don't break the web: Denial of Service attacks"},{"location":"section-5-legal-and-ethical-considerations/#dont-steal-copyright-and-fair-use","text":"It is important to recognize that in certain circumstances web scraping can be illegal. If the terms and conditions of the web site we are scraping specifically prohibit downloading and copying its content, then we could be in trouble for scraping it. In practice, however, web scraping is a tolerated practice, provided reasonable care is taken not to disrupt the \u201cregular\u201d use of a web site, as we have seen above. In a sense, web scraping is no different than using a web browser to visit a web page, in that it amounts to using computer software (a browser vs a scraper) to acccess data that is publicly available on the web. In general, if data is publicly available (the content that is being scraped is not behind a password-protected authentication system), then it is OK to scrape it, provided we don\u2019t break the web site doing so. What is potentially problematic is if the scraped data will be shared further. For example, downloading content off one website and posting it on another website (as our own), unless explicitely permitted, would constitute copyright violation and be illegal. However, most copyright legislations recognize cases in which reusing some, possibly copyrighted, information in an aggregate or derivative format is considered \u201cfair use\u201d. In general, unless the intent is to pass off data as our own, copy it word for word or trying to make money out of it, reusing publicly available content scraped off the internet is OK.","title":"Don't steal: Copyright and fair use"},{"location":"section-5-legal-and-ethical-considerations/#better-be-safe-than-sorry","text":"Be aware that copyright and data privacy legislation typically differs from country to country. Be sure to check the laws that apply in your context. For example, in Australia, it can be illegal to scrape and store personal information such as names, phone numbers and email addresses, even if they are publicly available. If you are looking to scrape data for your own personal use, then the above guidelines should probably be all that you need to worry about. However, if you plan to start harvesting a large amount of data for research or commercial purposes, you should probably seek legal advice first. If you work in a university, chances are it has a copyright office that will help you sort out the legal aspects of your project. The university library is often the best place to start looking for help on copyright.","title":"Better be safe than sorry"},{"location":"section-5-legal-and-ethical-considerations/#be-nice-ask-and-share","text":"Depending on the scope of your project, it might be worthwhile to consider asking the owners or curators of the data you are planning to scrape if they have it already available in a structured format that could suit your project. If your aim is do use their data for research, or to use it in a way that could potentially interest them, not only it could save you the trouble of writing a web scraper, but it could also help clarify straight away what you can and cannot do with the data. On the other hand, when you are publishing your own data, as part of a research project, documentation or a public website, you might want to think about whether someone might be interested in getting your data for their own project. If you can, try to provide others with a way to download your raw data in a structured format, and thus save them the trouble to try and scrape your own pages!","title":"Be nice: ask and share"},{"location":"section-5-legal-and-ethical-considerations/#web-scraping-code-of-conduct","text":"This all being said, if you adhere to the following simple rules, you will probably be fine. Ask nicely. If your project requires data from a particular organisation, for example, you can try asking them directly if they could provide you what you are looking for. With some luck, they will have the primary data that they used on their website in a structured format, saving you the trouble. Don't download copies of documents that are clearly not public. For example, academic journal publishers often have very strict rules about what you can and what you cannot do with their databases. Mass downloading article PDFs is probably prohibited and can put you (or at the very least your friendly university librarian) in trouble. If your project requires local copies of documents (e.g. for text mining projects), special agreements can be reached with the publisher. The library is a good place to start investigating something like that. Check your local legislation. For example, certain countries have laws protecting personal information such as email addresses and phone numbers. Scraping such information, even from publicly avaialable web sites, can be illegal (e.g. in Australia). Don't share downloaded content illegally. Scraping for personal purposes is usually OK, even if it is copyrighted information, as it could fall under the fair use provision of the intellectual property legislation. However, sharing data for which you don\u2019t hold the right to share is illegal. Share what you can. If the data you scraped is in the public domain or you got permission to share it, then put it out there for other people to reuse it (e.g. on datahub.io). If you wrote a web scraper to access it, share its code (e.g. on GitHub) so that others can benefit from it. Don't break the Internet. Not all web sites are designed to withstand thousands of requests per second. If you are writing a recursive scraper (i.e. that follows hyperlinks), test it on a smaller dataset first to make sure it does what it is supposed to do. Adjust the settings of your scraper to allow for a delay between requests. By default, Scrapy uses conservative settings that should minimize this risk. Publish your own data in a reusable way. Don\u2019t force others to write their own scrapers to get at your data. Use open and software-agnostic formats (e.g. JSON, XML), provide metadata (data about your data: where it came from, what it represents, how to use it, etc.) and make sure it can be indexed by search engines so that people can find it. Happy scraping!","title":"Web scraping code of conduct"},{"location":"section-5-legal-and-ethical-considerations/#references","text":"The Web scraping Wikipedia page has a concise definition of many concepts discussed here. This case study is a great example of what can be done using web scraping and how to achieve it. This recent case about Linkedin data is a good read. Commencing 25 May 2018, Monash University will also become subject to the European Union\u2019s General Data Protection Regulation ( GDPR ). Software Carpentry is a non-profit organisation that runs learn-to-code workshops worldwide. All lessons are publicly available and can be followed indepentently. This lesson is heavily inspired by Software Carpentry. Data Carpentry is a sister organisation of Software Carpentry focused on the fundamental data management skills required to conduct research. Library Carpentry is another Software Carpentry spinoff focused on software skills for librarians.","title":"References"},{"location":"section-6-advanced-topics/","text":"Variable arguments: \u00b6 1 2 3 4 5 def plus ( * args ): return sum ( args ) # Calculate the sum plus ( 1 , 4 , 5 ) 1 10 Variable keyword arguments: \u00b6 1 2 3 4 5 6 7 8 def concatenate ( ** kwargs ): result = \"\" # Iterating over the Python kwargs dictionary for arg in kwargs . values (): result = result + arg + \" \" return result print ( concatenate ( a = \"Real\" , b = \"Python\" , c = \"Is\" , d = \"Great\" , e = \"!\" )) 1 Real Python Is Great ! the correct order for your parameters is: 1 2 3 Standard arguments *args arguments **kwargs arguments 1","title":"Advanced topics"},{"location":"section-6-advanced-topics/#variable-arguments","text":"1 2 3 4 5 def plus ( * args ): return sum ( args ) # Calculate the sum plus ( 1 , 4 , 5 ) 1 10","title":"Variable arguments:"},{"location":"section-6-advanced-topics/#variable-keyword-arguments","text":"1 2 3 4 5 6 7 8 def concatenate ( ** kwargs ): result = \"\" # Iterating over the Python kwargs dictionary for arg in kwargs . values (): result = result + arg + \" \" return result print ( concatenate ( a = \"Real\" , b = \"Python\" , c = \"Is\" , d = \"Great\" , e = \"!\" )) 1 Real Python Is Great ! the correct order for your parameters is: 1 2 3 Standard arguments *args arguments **kwargs arguments 1","title":"Variable keyword arguments:"},{"location":"section-7-references/","text":"References \u00b6 Library Carpentry is another Software Carpentry spinoff focused on software skills for librarians. https://www.w3schools.com/python/default.asp https://www.tutorialspoint.com/python/index.htm","title":"References"},{"location":"section-7-references/#references","text":"Library Carpentry is another Software Carpentry spinoff focused on software skills for librarians. https://www.w3schools.com/python/default.asp https://www.tutorialspoint.com/python/index.htm","title":"References"}]}